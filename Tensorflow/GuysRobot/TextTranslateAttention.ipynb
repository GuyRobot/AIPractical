{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import io\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "path_data_file = \"../Jupyter/text_data/rus-eng/rus.txt\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "'Оно там?'"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def unicode_to_ascii(w):\n",
    "    return ''.join(unicodedata.normalize(\"NFD\", c) for c in w if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "word = \"Оно там?\"\n",
    "unicode_to_ascii(word)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "'<start> Оно там ? <end>'"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_word(w):\n",
    "    w = unicode_to_ascii(w)\n",
    "\n",
    "    w = re.sub(r\"([.!?,])\", r\" \\1\", w)\n",
    "    w = re.sub(r'[\" ]', \" \", w)\n",
    "\n",
    "    w = w.strip()\n",
    "    return \"<start> %s <end>\" % w\n",
    "\n",
    "preprocess_word(word)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "(('<start> Go . <end>', '<start> Go . <end>', '<start> Go . <end>'),\n ('<start> Марш ! <end>', '<start> Иди . <end>', '<start> Идите . <end>'))"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_dataset(path, num_instance):\n",
    "    lines = io.open(path, encoding=\"utf-8\").read().strip().split(\"\\n\")\n",
    "    return zip(*[[preprocess_word(w) for w in line.split(\"\\t\")[:2]] for line in lines[:num_instance]])\n",
    "\n",
    "a, b = create_dataset(path_data_file, 3)\n",
    "a, b"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def tokenize(texts):\n",
    "    \"\"\"\n",
    "\n",
    "    :param texts: the text to tokenize\n",
    "    :return: the tensors and tokenizer of the texts\n",
    "    \"\"\"\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "\n",
    "    tensor = tokenizer.texts_to_sequences(texts)\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding=\"post\")\n",
    "\n",
    "    return tensor, tokenizer\n",
    "\n",
    "\n",
    "def load_dataset(path, num_instance):\n",
    "    tar, inp = create_dataset(path, num_instance)\n",
    "\n",
    "    tar_tensor, tar_tokenizer = tokenize(tar)\n",
    "    inp_tensor, inp_tokenizer = tokenize(inp)\n",
    "\n",
    "    return tar_tensor, inp_tensor, tar_tokenizer, inp_tokenizer\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "(8, 14)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_EXAMPLES = 10000\n",
    "\n",
    "tar_tensor, inp_tensor, tar_tokenizer, inp_tokenizer = load_dataset(path_data_file, NUM_EXAMPLES)\n",
    "\n",
    "max_len_tar = tar_tensor.shape[1]\n",
    "max_len_inp = inp_tensor.shape[1]\n",
    "\n",
    "max_len_tar, max_len_inp"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "(7500, 2500)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_tensor_train, inp_tensor_val, tar_tensor_train, tar_tensor_val = train_test_split(inp_tensor, tar_tensor)\n",
    "\n",
    "len(inp_tensor_train), len(inp_tensor_val)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t--->\t<start>\n",
      "102\t--->\tвозьми\n",
      "16\t--->\tего\n",
      "3\t--->\t.\n",
      "2\t--->\t<end>\n"
     ]
    }
   ],
   "source": [
    "def print_convert(tokenizer, tensor):\n",
    "    for t in tensor:\n",
    "        if t != 0:\n",
    "            print(\"%d\\t--->\\t%s\" % (t, tokenizer.index_word[t]))\n",
    "\n",
    "print_convert(inp_tokenizer, inp_tensor_train[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "<PrefetchDataset shapes: ((64, 14), (64, 8)), types: (tf.int32, tf.int32)>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((inp_tensor_train, tar_tensor_train))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).cache().batch(BATCH_SIZE, drop_remainder=True).prefetch(1)\n",
    "dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 256\n",
    "ENC_UNITS = 1024\n",
    "DEC_UNITS = 1024\n",
    "\n",
    "vocab_inp_size = len(inp_tokenizer.word_index) + 1\n",
    "vocab_tar_size = len(tar_tokenizer.word_index) + 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 14) (64, 8)\n"
     ]
    }
   ],
   "source": [
    "for exam_inp, exam_tar in dataset.take(1):\n",
    "    print(exam_inp.shape, exam_tar.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "('Encoder Output Shape: ',\n TensorShape([64, 14, 1024]),\n 'Encoder Hidden shape: ',\n TensorShape([64, 1024]))"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def get_config(self):\n",
    "        pass\n",
    "    def __init__(self, vocab_size, embedding_dim, encoder_units, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "        self.encoder_units = encoder_units\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(encoder_units, return_sequences=True, return_state=True,\n",
    "                                       recurrent_initializer=\"glorot_uniform\")\n",
    "\n",
    "    def call(self, inputs, *args):\n",
    "        x, hidden = inputs\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state=hidden)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden(self):\n",
    "        return tf.zeros((self.batch_size, self.encoder_units))\n",
    "\n",
    "\n",
    "exam_inp, exam_tar = next(iter(dataset))\n",
    "encoder_test = Encoder(vocab_inp_size, EMBEDDING_DIM, ENC_UNITS, batch_size=BATCH_SIZE)\n",
    "\n",
    "init_hidden = encoder_test.initialize_hidden()\n",
    "sample_output, sample_hidden = encoder_test((exam_inp, init_hidden))\n",
    "\n",
    "\"Encoder Output Shape: \", sample_output.shape, \"Encoder Hidden shape: \", sample_hidden.shape\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "('Attention context shape: ',\n TensorShape([64, 1024]),\n 'attention weights shape: ',\n TensorShape([64, 14, 1]))"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, fc_units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.fc1 = tf.keras.layers.Dense(fc_units)\n",
    "        self.fc2 = tf.keras.layers.Dense(fc_units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values, *args):\n",
    "        \"\"\"\n",
    "\n",
    "        :param query: The hidden from encoder (batch_size, enc_hidden)\n",
    "        :param values: encoder output (batch_size, seq_len, enc_hidden)\n",
    "        :param args:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # query hidden state shape == (batch_size, hidden size)\n",
    "        # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # values shape == (batch_size, max_len, hidden size)\n",
    "        # expand dim to broadcast addition along the time axis to calculate the score\n",
    "        query_time_with_axis = tf.expand_dims(query, axis=1)\n",
    "        # fc2 --> (batch_size, ..., units)\n",
    "        # fc1 --> (batch_size, ..., units)\n",
    "        # fc1 + fc2 --> (batch_size, ..., units)\n",
    "        # V --> (batch_size, ..., 1) (score shape)\n",
    "        score = self.V(tf.tanh(self.fc2(query_time_with_axis) + self.fc1(values)))\n",
    "        # attention_weights = tf.keras.layers.Softmax(axis=1)(score)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        # Point wise element multi (not dot product)\n",
    "        context_vector = tf.reduce_sum(attention_weights * values, axis=1)\n",
    "        # context_vector = tf.reduce_sum(tf.matmul(attention_weights, encoder_out), axis=1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "    # def call(self, inputs, training=None, mask=None):\n",
    "    #     encoder_out, hidden = inputs\n",
    "    #     score = self.V(tf.tanh(self.fc2(encoder_out) + self.fc1(hidden)))\n",
    "    #     # attention_weights = tf.keras.layers.Softmax(axis=1)(score)\n",
    "    #     attention_weights = tf.nn.softmax(score, axis=1)\n",
    "    #     context_vector = tf.reduce_sum(attention_weights * encoder_out, axis=1)\n",
    "    #     # context_vector = tf.reduce_sum(tf.matmul(attention_weights, encoder_out), axis=1)\n",
    "    #\n",
    "    #     return context_vector, attention_weights\n",
    "\n",
    "attention = BahdanauAttention(10)\n",
    "attention_context, attention_weights = attention(sample_hidden, sample_output)\n",
    "\n",
    "\"Attention context shape: \", attention_context.shape, \"attention weights shape: \", attention_weights.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n",
      "WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n",
      "WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n",
      "WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n",
      "WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n",
      "WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n"
     ]
    },
    {
     "data": {
      "text/plain": "TensorShape([64, 1748])"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(units=dec_units, return_state=True, return_sequences=True,\n",
    "                                       recurrent_initializer=\"glorot_uniform\")\n",
    "\n",
    "        self.final_fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(dec_units)\n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        \"\"\"\n",
    "        :param x: inputs\n",
    "        :param hidden: hidden from encoder\n",
    "        :param enc_output: output of encoder\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # (batch_size, hidden_units), (batch_size, seq_len, hidden_units)\n",
    "        # embedded (batch_size, ..., embedding_dim)\n",
    "        # concat (batch_size, ..., embedding_dim + units)\n",
    "        # gru --> out (batch_size, ..., units), state (batch_size, units)\n",
    "        # reshape --> merge batch_size and ... --> (batch_size, units)\n",
    "        # fc --> (batch_size, vocab_size)\n",
    "        context_vector, context_weights = self.attention(hidden, enc_output)\n",
    "        x = self.embedding(x)\n",
    "        x = tf.concat([x, tf.expand_dims(context_vector, axis=1)], axis=-1)\n",
    "\n",
    "        out, state = self.gru(x)\n",
    "\n",
    "        out = tf.reshape(out, shape=(-1, out.shape[2]))\n",
    "\n",
    "        x = self.final_fc(out)\n",
    "\n",
    "        return x, state, attention_weights\n",
    "\n",
    "\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim=EMBEDDING_DIM, dec_units=DEC_UNITS)\n",
    "\n",
    "sample_decode_output, _, _ = decoder(tf.random.uniform(shape=(BATCH_SIZE, 1)), sample_hidden, sample_output)\n",
    "\n",
    "sample_decode_output.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "sparse_loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_func(real, pred):\n",
    "    # Mask for target (0 is mask, 1 is real target)\n",
    "    mask = tf.math.logical_not(tf.equal(real, 0))\n",
    "    loss = sparse_loss(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    # ignore for mask loss\n",
    "    loss *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_size=vocab_inp_size, embedding_dim=EMBEDDING_DIM, encoder_units=ENC_UNITS, batch_size=BATCH_SIZE)\n",
    "decoder = Decoder(vocab_size=vocab_tar_size, embedding_dim=EMBEDDING_DIM, dec_units=DEC_UNITS)\n",
    "check_point_dir = \"./text_translate\"\n",
    "prefix = \"ckpt\"\n",
    "check_point_prefix = os.path.join(check_point_dir, prefix)\n",
    "checkpoint = tf.train.Checkpoint(encoder=encoder, decoder=decoder)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, tar, enc_hidden):\n",
    "    \"\"\"\n",
    "    Pass the input through the encoder which return encoder output and the encoder hidden state.\n",
    "    The encoder output, encoder hidden state and the decoder input (which is the start token) is passed to the decoder.\n",
    "    The decoder returns the predictions and the decoder hidden state.\n",
    "    The decoder hidden state is then passed back into the model and the predictions are used to calculate the loss.\n",
    "    Use teacher forcing to decide the next input to the decoder.\n",
    "    Teacher forcing is the technique where the target word is passed as the next input to the decoder.\n",
    "    The final step is to calculate the gradients and apply it to the optimizer and backpropagate\n",
    "    :param inp:\n",
    "    :param tar:\n",
    "    :param enc_hidden:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    loss = 0.0\n",
    "\n",
    "    with tf.GradientTape() as g:\n",
    "\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "        dec_input = tf.expand_dims([tar_tokenizer.word_index['<start>']] * BATCH_SIZE, axis=1)\n",
    "        dec_hidden = enc_hidden\n",
    "        for t in tar:\n",
    "            predictions, state, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "            loss += loss_func(tar[:, t], predictions)\n",
    "\n",
    "            dec_input = tf.expand_dims(tar[:, t], axis=1)\n",
    "\n",
    "    batch_loss = (loss / int(tar.shape[1]))\n",
    "\n",
    "    train_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = g.gradient(loss, train_variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, train_variables))\n",
    "\n",
    "    return batch_loss\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "steps_per_epoch = len(inp_tensor_train) // BATCH_SIZE\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    enc_hidden = encoder.initialize_hidden()\n",
    "\n",
    "    for (batch, (inp, tar)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        loss = train_step(inp, tar, enc_hidden)\n",
    "        total_loss += loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch %d Batch %d Loss %.4f' % (epoch + 1, batch, loss.numpy()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}