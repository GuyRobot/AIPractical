{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "filenames = [format(i, '03d') + '.txt' for i in range(1, 101)]\n",
    "dir_name = 'stories'\n",
    "\n",
    "def read_data(filename):\n",
    "    with open(filename) as f:\n",
    "        data =  tf.compat.as_str(f.read())\n",
    "        data = data.lower()\n",
    "        data = list(data)\n",
    "    return data\n",
    "\n",
    "global documents\n",
    "documents = []\n",
    "num_files = 100\n",
    "for i in range(num_files):\n",
    "    print(\"processing file %s\" % os.path.join(dir_name, filenames[i]))\n",
    "    chars = read_data(os.path.join(dir_name, filenames[i]))\n",
    "\n",
    "    # break into bigrams\n",
    "    two_grams = [''.join(chars[ch_i:ch_i+2]) for ch_i in range(0, len(chars)-2, 2)]\n",
    "    # Create document\n",
    "    documents.append(two_grams)\n",
    "    print(\"Data size (chars) (document %d) %d\" % (i, len(two_grams)))\n",
    "    print(\"Sample string %s\\n\" % (two_grams[:50]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def build_dataset(documents):\n",
    "    chars = []\n",
    "    # list of lists\n",
    "    data_list = []\n",
    "\n",
    "    for d in documents:\n",
    "        chars.extend(d)\n",
    "    print('%d character found.' % len(chars))\n",
    "\n",
    "    count = []\n",
    "    # bigrams sorted by their frequency\n",
    "    count.extend(collections.Counter(chars).most_common())\n",
    "\n",
    "    # Create dict map word to id by given the current length of the dictionary\n",
    "    # UNK is for two rare word\n",
    "    dictionary = dict({'UNK': 0})\n",
    "    for char, c in count:\n",
    "        # Only add if its frequency is more than 10\n",
    "        if c > 10:\n",
    "            dictionary[char] = len(dictionary)\n",
    "    unk_count = 0\n",
    "    # replace word with id of word\n",
    "    for d in documents:\n",
    "        data = list()\n",
    "        for char in d:\n",
    "            # if word in dictionary use the id of word\n",
    "            # otherwise use id of UNK\n",
    "            if char in dictionary:\n",
    "                index = dictionary[char]\n",
    "            else:\n",
    "                index = dictionary['UNK']\n",
    "                unk_count += 1\n",
    "            data.append(index)\n",
    "        data_list.append(data)\n",
    "\n",
    "    # dict map id to word\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data_list, count, dictionary, reverse_dictionary\n",
    "\n",
    "data_list, count, dictionary, reverse_dictionary = build_dataset(documents)\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Least common words (+UNK)', count[-15:])\n",
    "print('Sample data', data_list[0][:10])\n",
    "print('Sample data', data_list[1][:10])\n",
    "print('Vocabulary: ',len(dictionary))\n",
    "vocabulary_size = len(dictionary)\n",
    "del documents  # To reduce memory."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_nodes = 128\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# the number os time steps used in truncated BPTT\n",
    "num_unrolling = 50\n",
    "\n",
    "# regular\n",
    "dropout = .0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "train_inputs, train_labels = [], []\n",
    "# Unroll training inputs\n",
    "for ui in range(num_unrolling):\n",
    "    train_inputs.append(tf.compat.v1.placeholder(tf.float32, shape=(batch_size, vocabulary_size),\n",
    "                                                 name='train_inputs_%d' % ui))\n",
    "    train_labels.append(tf.compat.v1.placeholder(tf.float32, shape=(batch_size, vocabulary_size),\n",
    "                                                 name='train_label_%d' % ui))\n",
    "\n",
    "# Validation data\n",
    "valid_inputs = tf.compat.v1.placeholder(tf.float32, shape=(1, vocabulary_size), name='valid_inputs')\n",
    "valid_labels = tf.compat.v1.placeholder(tf.float32, shape=(1, vocabulary_size), name='valid_labels')\n",
    "\n",
    "# Test data\n",
    "test_input = tf.compat.v1.placeholder(tf.float32, shape=(1, vocabulary_size), name='test_input')\n",
    "\n",
    "\n",
    "# Input gate - How much memory to write to cell state\n",
    "# connects current input to the input gate\n",
    "ix = tf.Variable(tf.compat.v1.truncated_normal(shape=(vocabulary_size, num_nodes), stddev=.02))\n",
    "# connects the previous hidden state to the input gate\n",
    "im = tf.Variable(tf.compat.v1.truncated_normal((num_nodes, num_nodes), stddev=.02))\n",
    "# bias of input gate\n",
    "ib = tf.Variable(tf.compat.v1.random_uniform((1, num_nodes),-0.02, 0.02))\n",
    "\n",
    "# Forget gate - how much memory to discard from cell state\n",
    "# connect current input to he forget gate\n",
    "fx = tf.Variable(tf.compat.v1.truncated_normal((vocabulary_size, num_nodes), stddev=.02))\n",
    "# connect the previous hidden state to the forget gate\n",
    "fm = tf.Variable(tf.random.truncated_normal((num_nodes, num_nodes), stddev=0.02))\n",
    "# bias of forget gate\n",
    "fb = tf.Variable(tf.random.uniform(shape=(1, num_nodes), minval=-0.02, maxval=0.02))\n",
    "\n",
    "# Candidate - compute the current cell state\n",
    "# connect current input to candidate\n",
    "cx = tf.Variable(tf.random.truncated_normal((vocabulary_size, num_nodes), stddev=0.02))\n",
    "# connect previous hidden state to the candidate\n",
    "cm = tf.Variable(tf.random.truncated_normal((num_nodes, num_nodes), stddev=0.02))\n",
    "# bias of candidate\n",
    "cb = tf.Variable(tf.random.uniform((1, num_nodes), minval=-0.02, maxval=0.02))\n",
    "\n",
    "# Output gate - how much memory to output from cell state\n",
    "ox = tf.Variable(tf.random.truncated_normal((vocabulary_size, num_nodes), stddev=.02))\n",
    "om = tf.Variable(tf.random.truncated_normal((num_nodes, num_nodes), stddev=.02))\n",
    "ob = tf.Variable(tf.random.uniform((1, num_nodes), minval=-0.02, maxval=0.02))\n",
    "\n",
    "# Softmax classifier weights and biases\n",
    "w = tf.Variable(tf.random.truncated_normal((num_nodes, vocabulary_size), stddev=0.02))\n",
    "b = tf.Variable(tf.random.uniform((vocabulary_size,), minval=-0.02, maxval=0.02))\n",
    "\n",
    "saved_output = tf.Variable(tf.zeros((batch_size, num_nodes)), trainable=False, name=\"train_hidden\")\n",
    "# cell state\n",
    "saved_state = tf.Variable(tf.zeros((batch_size, num_nodes)), trainable=False, name=\"train_cell\")\n",
    "\n",
    "# Variables for validation\n",
    "saved_valid_output = tf.Variable(tf.zeros((1, num_nodes)), trainable=False, name='valid_hidden')\n",
    "saved_valid_state = tf.Variable(tf.zeros((1, num_nodes)), trainable=False, name='valid_cell')\n",
    "\n",
    "# Variables for testing\n",
    "saved_test_output = tf.Variable(tf.zeros((1, num_nodes)), trainable=False, name=\"test_hidden\")\n",
    "saved_test_state = tf.Variable(tf.zeros((1, num_nodes)), trainable=False, name='test_cell')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def lstm_cell(i, o, state):\n",
    "    \"\"\"\n",
    "\n",
    "    :param i: input\n",
    "    :param o: output from previous cell or hidden state\n",
    "    :param state: the previous cell state\n",
    "\n",
    "    forget_gate = σ(Wf · concat(h_t-1, x_t) + bf)\n",
    "    input_gate = σ(Wi · concat(h_t-1, x_t) + bi)\n",
    "    candidate = tanh(Wc · concat(h_t-1, x_t) + bc)\n",
    "    cell_state = forget_gate * previous_cell_state + input_gate * candidate\n",
    "    output_gate = σ(Wo · concat(h_t-1, x_t) + bo)\n",
    "    hidden_state = output_gate * tanh(cell_state)\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    candidate_update = tf.tanh(tf.matmul(i, cx) + tf.matmul(o, cm) + cb)\n",
    "    state = forget_gate * state + input_gate * candidate_update\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_output, test_state = lstm_cell(\n",
    "    test_input, saved_test_output, saved_test_state\n",
    ")\n",
    "# Make sure state are updated\n",
    "with tf.control_dependencies([saved_test_output.assign(test_output),\n",
    "                              saved_test_state.assign(test_state)]):\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(test_output, w) + b)\n",
    "\n",
    "reset_test_state = tf.group(saved_test_output.assign((tf.random.normal((1, num_nodes), stddev=.05))),\n",
    "                            saved_test_state.assign((tf.random.normal((1, num_nodes), stddev=.05))))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def sample(distribution):\n",
    "    \"\"\"\n",
    "    Greedy Sampling\n",
    "    Pick the three best prob given by LSTM and sample one\n",
    "    of them with very high prob of pick the best one\n",
    "    :param distribution:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    best_indices = np.argsort(distribution)[-3:]\n",
    "    best_probs = distribution[best_indices] / np.sum(distribution[best_indices])\n",
    "    best_idx = np.random.choice(best_indices, p=best_probs)\n",
    "\n",
    "    return best_idx\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "saver = tf.compat.v1.train.Saver()\n",
    "\n",
    "segments_to_generate = 1\n",
    "chars_in_segment = 500\n",
    "new_session = tf.compat.v1.InteractiveSession()\n",
    "\n",
    "saver.restore(new_session, \"././my_model/my_saved_variable-20\")\n",
    "\n",
    "for _ in range(segments_to_generate):\n",
    "            print(\"=\"*24 + \"New text\" + \"=\"*24 )\n",
    "\n",
    "            # start with random word\n",
    "            test_word = np.zeros((1, vocabulary_size), dtype=np.float32)\n",
    "            rand_doc = data_list[np.random.randint(0, num_files)]\n",
    "            test_word[0, rand_doc[np.random.randint(0, len(rand_doc))]] = 1.0\n",
    "            print(reverse_dictionary[np.argmax(test_word[0])], end='')\n",
    "\n",
    "            # Generating words by feeding the previous prediction\n",
    "            # as current input in a recursive manner\n",
    "            for _ in range(chars_in_segment):\n",
    "                sample_pred = new_session.run(test_prediction, feed_dict={test_input: test_word})\n",
    "                next_ind = sample(sample_pred.ravel())\n",
    "                test_word = np.zeros((1, vocabulary_size), dtype=np.float32)\n",
    "                test_word[0, next_ind] = 1.0\n",
    "                print(reverse_dictionary[next_ind], end='')\n",
    "\n",
    "            # Reset train state\n",
    "            new_session.run(reset_test_state)\n",
    "            print(\"-\" * 28)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}