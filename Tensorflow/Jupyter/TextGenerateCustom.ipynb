{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "text = open(path_to_file, 'rb').read().decode('utf-8')\n",
    "print('Length of text: {} characters'.format(len(text)))\n",
    "print(text[:100])\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "65"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unique character in files (a, b, c...)\n",
    "vocab = sorted(set(text))\n",
    "len(vocab)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "(array(['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?',\n        'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M',\n        'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z',\n        'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm',\n        'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'],\n       dtype='<U1'), array([18, 47, 56, ..., 45,  8,  0]))"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vectorize\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "# Convert all character to int base on char2idx dict\n",
    "text_as_int = np.array([char2idx[c] for c in text])\n",
    "idx2char, text_as_int\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F\n",
      "i\n",
      "r\n",
      "s\n",
      "t\n"
     ]
    }
   ],
   "source": [
    "# Training examples  and targets\n",
    "# Divide text into example sequences, each input sequence will\n",
    "# contain seq_length characters from the text\n",
    "# Each sequence, the targets contain the same seq_length of text, but shifted one character to right\n",
    "\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text) // (seq_length + 1)\n",
    "\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(5):\n",
    "    print(idx2char[i.numpy()])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
      "\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
      "\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
      "'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
     ]
    }
   ],
   "source": [
    "sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(5):\n",
    "    print(repr(''.join(idx2char[item.numpy()])))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "<MapDataset shapes: ((100,), (100,)), types: (tf.int32, tf.int32)>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map text to input and target (both input and target have\n",
    "# the same seq_length but target is shifted to right one character)\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1] # take all except the last character\n",
    "    target_text = chunk[1:] # take all except the first character\n",
    "\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)\n",
    "dataset\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data 'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
      "Target data 'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
     ]
    }
   ],
   "source": [
    "for input_exp, target_exp in dataset.take(1):\n",
    "    print('Input data', repr(''.join(idx2char[input_exp.numpy()])))\n",
    "    print(\"Target data\", repr(''.join(idx2char[target_exp.numpy()])))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    0\n",
      "\tinput: 18 ('F')\n",
      "\toutput: 47 ('i')\n",
      "Step    1\n",
      "\tinput: 47 ('i')\n",
      "\toutput: 56 ('r')\n",
      "Step    2\n",
      "\tinput: 56 ('r')\n",
      "\toutput: 57 ('s')\n",
      "Step    3\n",
      "\tinput: 57 ('s')\n",
      "\toutput: 58 ('t')\n",
      "Step    4\n",
      "\tinput: 58 ('t')\n",
      "\toutput: 1 (' ')\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Each index of these vectors is processed as a one\n",
    "    time step. For the input at time step 0, the model\n",
    "    receives the index for \"F\" and tries to predict\n",
    "    the index for \"i\" as the next character. At the\n",
    "    next timestep, it does the same thing but the RNN\n",
    "    considers the previous step context in addition\n",
    "    to the current input character.\n",
    "\"\"\"\n",
    "\n",
    "for i, (input_idx, target_idx) in enumerate(zip(input_exp[:5], target_exp[:5])):\n",
    "    print(\"Step {:4d}\".format(i))\n",
    "    print(\"\\tinput: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "    print(\"\\toutput: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "<PrefetchDataset shapes: ((64, 100), (64, 100)), types: (tf.int32, tf.int32)>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True).prefetch(1)\n",
    "\n",
    "dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Model\n",
    "# Embedding: input layer map the numbers of each character to a vector\n",
    "# with embedding_dim\n",
    "# GRU: special type of RNN with size units=rnn_units\n",
    "# Dense: vocab_size outputs\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "embedding_dim = 256\n",
    "\n",
    "rnn_units = 1024"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           16640     \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (64, None, 1024)          3938304   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 65)            66625     \n",
      "=================================================================\n",
      "Total params: 4,021,569\n",
      "Trainable params: 4,021,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=(batch_size, None)),\n",
    "        tf.keras.layers.GRU(units=rnn_units,\n",
    "                            return_sequences=True,\n",
    "                            stateful=True,\n",
    "                            recurrent_initializer='glorot_uniform'),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "\n",
    "model = build_model(vocab_size, embedding_dim, rnn_units, BATCH_SIZE)\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "<TakeDataset shapes: ((64, 100), (64, 100)), types: (tf.int32, tf.int32)>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.take(1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[52  1 39 ... 57  8  0]\n",
      " [50 39 47 ...  0 23 21]\n",
      " [ 0  0 23 ... 39 49  1]\n",
      " ...\n",
      " [10  0 35 ... 53 59 50]\n",
      " [34 21 30 ... 46 43  1]\n",
      " [63  1 46 ...  0 20 43]], shape=(64, 100), dtype=int32)\n",
      "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    For each character the model looks up the\n",
    "    embedding, runs the GRU one timestep with\n",
    "    the embedding as input, and applies the dense\n",
    "    layer to generate logits predicting the log-likelihood of the next character:\n",
    "\"\"\"\n",
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    print(input_example_batch)\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (64, None, 256)           16640     \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (64, None, 1024)          3938304   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (64, None, 65)            66625     \n",
      "=================================================================\n",
      "Total params: 4,021,569\n",
      "Trainable params: 4,021,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "array([11, 53, 56,  8, 62, 53, 16, 25, 40, 17, 26, 18,  4, 52,  3, 51,  3,\n       29,  4, 35, 43, 26, 10,  8, 14, 42, 24, 42, 57, 40, 39, 11,  2, 21,\n       25, 54, 22, 42, 39, 52, 52,  7, 54,  7, 36, 54, 61, 17, 44,  3, 39,\n       45, 63,  5, 35, 39, 37, 12, 31, 61, 41, 39, 56, 52, 39, 42, 33, 25,\n       31, 33,  0,  9, 64, 25, 10, 34, 50, 25, 12,  5,  4, 51, 33, 50, 39,\n       22, 22, 44,  9,  2, 59, 54, 21, 13, 42, 36,  9, 21, 31, 60],\n      dtype=int64)"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
    "sampled_indices"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      " '.\\nTo Bolingbroke are we sworn subjects now,\\nWhose state and honour I for aye allow.\\n\\nDUCHESS OF YORK'\n",
      "Next Char Predictions: \n",
      " \";or.xoDMbENF&n$m$Q&WeN:.BdLdsba;!IMpJdann-p-XpwEf$agy'WaY?SwcarnadUMSU\\n3zM:VlM?'&mUlaJJf3!upIAdX3ISv\"\n"
     ]
    }
   ],
   "source": [
    "print(\"Input: \\n\", repr(''.join(idx2char[input_example_batch[0]])))\n",
    "print(\"Next Char Predictions: \\n\", repr(''.join(idx2char[sampled_indices])))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "4.173761"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def loss_sparse(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "example_batch_loss = loss_sparse(target_example_batch, example_batch_predictions)\n",
    "example_batch_loss.numpy().mean()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# model.compile(optimizer='adam', loss=loss)\n",
    "#\n",
    "# checkpoint_dir = './training_checkpoint'\n",
    "#\n",
    "# checkpoint_predix = os.path.join(checkpoint_dir, 'ckpt_{epoch}')\n",
    "#\n",
    "# checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "#     filepath=checkpoint_predix,\n",
    "#     save_weights_only=True\n",
    "# )\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "@tf.function\n",
    "def train_step(inp, target):\n",
    "    with tf.GradientTape() as g:\n",
    "        predictions = model(inp)\n",
    "        loss = tf.reduce_mean(loss_sparse(target, predictions))\n",
    "\n",
    "    gradients = g.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-1-18a672eccfe6>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[0mcheckpoint_dir\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m'./training_checkpoint'\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 4\u001B[1;33m \u001B[0mcheckpoint_predix\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcheckpoint_dir\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'ckpt_{epoch}'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      5\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      6\u001B[0m checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
      "\u001B[1;31mNameError\u001B[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "checkpoint_dir = './training_checkpoint'\n",
    "\n",
    "checkpoint_predix = os.path.join(checkpoint_dir, 'ckpt_{epoch}')\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_predix,\n",
    "    save_weights_only=True\n",
    ")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    # reset hidden state\n",
    "    model.reset_states()\n",
    "\n",
    "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
    "        loss = train_step(inp, target)\n",
    "\n",
    "        if batch_n % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {}'.format(epoch + 1, batch_n, loss))\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        model.save_weights(checkpoint_predix.format(epoch=epoch))\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1, loss))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "\n",
    "\n",
    "model.save_weights(checkpoint_predix.format(epoch=epoch))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "    \"\"\"\n",
    "    Chose a start string, init RNN state and set the number\n",
    "    of characters to generate\n",
    "    Get the prediction distribution of next character using the start string and RNN state\n",
    "    Use categorical distribution to calculate the index of predicted character\n",
    "    and use this predicted character as our next input\n",
    "    The RNN state returned by the model is fed back into the model so that it now has more context,\n",
    "    After predicting the next character, the modified RNN states are again fed back into the model\n",
    "    :param model:\n",
    "    :param start_string:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    num_generate = 1000\n",
    "\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0) # convert to 2d tensor\n",
    "\n",
    "    text_generated = []\n",
    "\n",
    "    # Love results in more predictable text\n",
    "    # High otherwise\n",
    "    temperature = 1.0\n",
    "\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "\n",
    "        # remove batch dimension\n",
    "        predictions = tf.squeeze(predictions, axis=0)\n",
    "\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
    "\n",
    "        input_eval = tf.expand_dims([predicted_id], axis=0)\n",
    "\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return start_string + ''.join(text_generated)\n",
    "\n",
    "print(generate_text(model, start_string=\"ROMEO: \"))\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}