{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file:  stories\\001.txt\n",
      "File 001.txt already exits\n",
      "Downloading file:  stories\\002.txt\n",
      "File 002.txt already exits\n",
      "Downloading file:  stories\\003.txt\n",
      "File 003.txt already exits\n",
      "Downloading file:  stories\\004.txt\n",
      "File 004.txt already exits\n",
      "Downloading file:  stories\\005.txt\n",
      "File 005.txt already exits\n",
      "Downloading file:  stories\\006.txt\n",
      "File 006.txt already exits\n",
      "Downloading file:  stories\\007.txt\n",
      "File 007.txt already exits\n",
      "Downloading file:  stories\\008.txt\n",
      "File 008.txt already exits\n",
      "Downloading file:  stories\\009.txt\n",
      "File 009.txt already exits\n",
      "Downloading file:  stories\\010.txt\n",
      "File 010.txt already exits\n",
      "Downloading file:  stories\\011.txt\n",
      "File 011.txt already exits\n",
      "Downloading file:  stories\\012.txt\n",
      "File 012.txt already exits\n",
      "Downloading file:  stories\\013.txt\n",
      "File 013.txt already exits\n",
      "Downloading file:  stories\\014.txt\n",
      "File 014.txt already exits\n",
      "Downloading file:  stories\\015.txt\n",
      "File 015.txt already exits\n",
      "Downloading file:  stories\\016.txt\n",
      "File 016.txt already exits\n",
      "Downloading file:  stories\\017.txt\n",
      "File 017.txt already exits\n",
      "Downloading file:  stories\\018.txt\n",
      "File 018.txt already exits\n",
      "Downloading file:  stories\\019.txt\n",
      "File 019.txt already exits\n",
      "Downloading file:  stories\\020.txt\n",
      "File 020.txt already exits\n",
      "Downloading file:  stories\\021.txt\n",
      "File 021.txt already exits\n",
      "Downloading file:  stories\\022.txt\n",
      "File 022.txt already exits\n",
      "Downloading file:  stories\\023.txt\n",
      "File 023.txt already exits\n",
      "Downloading file:  stories\\024.txt\n",
      "File 024.txt already exits\n",
      "Downloading file:  stories\\025.txt\n",
      "File 025.txt already exits\n",
      "Downloading file:  stories\\026.txt\n",
      "File 026.txt already exits\n",
      "Downloading file:  stories\\027.txt\n",
      "File 027.txt already exits\n",
      "Downloading file:  stories\\028.txt\n",
      "File 028.txt already exits\n",
      "Downloading file:  stories\\029.txt\n",
      "File 029.txt already exits\n",
      "Downloading file:  stories\\030.txt\n",
      "File 030.txt already exits\n",
      "Downloading file:  stories\\031.txt\n",
      "File 031.txt already exits\n",
      "Downloading file:  stories\\032.txt\n",
      "File 032.txt already exits\n",
      "Downloading file:  stories\\033.txt\n",
      "File 033.txt already exits\n",
      "Downloading file:  stories\\034.txt\n",
      "File 034.txt already exits\n",
      "Downloading file:  stories\\035.txt\n",
      "File 035.txt already exits\n",
      "Downloading file:  stories\\036.txt\n",
      "File 036.txt already exits\n",
      "Downloading file:  stories\\037.txt\n",
      "File 037.txt already exits\n",
      "Downloading file:  stories\\038.txt\n",
      "File 038.txt already exits\n",
      "Downloading file:  stories\\039.txt\n",
      "File 039.txt already exits\n",
      "Downloading file:  stories\\040.txt\n",
      "File 040.txt already exits\n",
      "Downloading file:  stories\\041.txt\n",
      "File 041.txt already exits\n",
      "Downloading file:  stories\\042.txt\n",
      "File 042.txt already exits\n",
      "Downloading file:  stories\\043.txt\n",
      "File 043.txt already exits\n",
      "Downloading file:  stories\\044.txt\n",
      "File 044.txt already exits\n",
      "Downloading file:  stories\\045.txt\n",
      "File 045.txt already exits\n",
      "Downloading file:  stories\\046.txt\n",
      "File 046.txt already exits\n",
      "Downloading file:  stories\\047.txt\n",
      "File 047.txt already exits\n",
      "Downloading file:  stories\\048.txt\n",
      "File 048.txt already exits\n",
      "Downloading file:  stories\\049.txt\n",
      "File 049.txt already exits\n",
      "Downloading file:  stories\\050.txt\n",
      "File 050.txt already exits\n",
      "Downloading file:  stories\\051.txt\n",
      "File 051.txt already exits\n",
      "Downloading file:  stories\\052.txt\n",
      "File 052.txt already exits\n",
      "Downloading file:  stories\\053.txt\n",
      "File 053.txt already exits\n",
      "Downloading file:  stories\\054.txt\n",
      "File 054.txt already exits\n",
      "Downloading file:  stories\\055.txt\n",
      "File 055.txt already exits\n",
      "Downloading file:  stories\\056.txt\n",
      "File 056.txt already exits\n",
      "Downloading file:  stories\\057.txt\n",
      "File 057.txt already exits\n",
      "Downloading file:  stories\\058.txt\n",
      "File 058.txt already exits\n",
      "Downloading file:  stories\\059.txt\n",
      "File 059.txt already exits\n",
      "Downloading file:  stories\\060.txt\n",
      "File 060.txt already exits\n",
      "Downloading file:  stories\\061.txt\n",
      "File 061.txt already exits\n",
      "Downloading file:  stories\\062.txt\n",
      "File 062.txt already exits\n",
      "Downloading file:  stories\\063.txt\n",
      "File 063.txt already exits\n",
      "Downloading file:  stories\\064.txt\n",
      "File 064.txt already exits\n",
      "Downloading file:  stories\\065.txt\n",
      "File 065.txt already exits\n",
      "Downloading file:  stories\\066.txt\n",
      "File 066.txt already exits\n",
      "Downloading file:  stories\\067.txt\n",
      "File 067.txt already exits\n",
      "Downloading file:  stories\\068.txt\n",
      "File 068.txt already exits\n",
      "Downloading file:  stories\\069.txt\n",
      "File 069.txt already exits\n",
      "Downloading file:  stories\\070.txt\n",
      "File 070.txt already exits\n",
      "Downloading file:  stories\\071.txt\n",
      "File 071.txt already exits\n",
      "Downloading file:  stories\\072.txt\n",
      "File 072.txt already exits\n",
      "Downloading file:  stories\\073.txt\n",
      "File 073.txt already exits\n",
      "Downloading file:  stories\\074.txt\n",
      "File 074.txt already exits\n",
      "Downloading file:  stories\\075.txt\n",
      "File 075.txt already exits\n",
      "Downloading file:  stories\\076.txt\n",
      "File 076.txt already exits\n",
      "Downloading file:  stories\\077.txt\n",
      "File 077.txt already exits\n",
      "Downloading file:  stories\\078.txt\n",
      "File 078.txt already exits\n",
      "Downloading file:  stories\\079.txt\n",
      "File 079.txt already exits\n",
      "Downloading file:  stories\\080.txt\n",
      "File 080.txt already exits\n",
      "Downloading file:  stories\\081.txt\n",
      "File 081.txt already exits\n",
      "Downloading file:  stories\\082.txt\n",
      "File 082.txt already exits\n",
      "Downloading file:  stories\\083.txt\n",
      "File 083.txt already exits\n",
      "Downloading file:  stories\\084.txt\n",
      "File 084.txt already exits\n",
      "Downloading file:  stories\\085.txt\n",
      "File 085.txt already exits\n",
      "Downloading file:  stories\\086.txt\n",
      "File 086.txt already exits\n",
      "Downloading file:  stories\\087.txt\n",
      "File 087.txt already exits\n",
      "Downloading file:  stories\\088.txt\n",
      "File 088.txt already exits\n",
      "Downloading file:  stories\\089.txt\n",
      "File 089.txt already exits\n",
      "Downloading file:  stories\\090.txt\n",
      "File 090.txt already exits\n",
      "Downloading file:  stories\\091.txt\n",
      "File 091.txt already exits\n",
      "Downloading file:  stories\\092.txt\n",
      "File 092.txt already exits\n",
      "Downloading file:  stories\\093.txt\n",
      "File 093.txt already exits\n",
      "Downloading file:  stories\\094.txt\n",
      "File 094.txt already exits\n",
      "Downloading file:  stories\\095.txt\n",
      "File 095.txt already exits\n",
      "Downloading file:  stories\\096.txt\n",
      "File 096.txt already exits\n",
      "Downloading file:  stories\\097.txt\n",
      "File 097.txt already exits\n",
      "Downloading file:  stories\\098.txt\n",
      "File 098.txt already exits\n",
      "Downloading file:  stories\\099.txt\n",
      "File 099.txt already exits\n",
      "Downloading file:  stories\\100.txt\n",
      "File 100.txt already exits\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.cs.cmu.edu/~spok/grimmtmp/'\n",
    "\n",
    "dir_name = 'stories'\n",
    "if not os.path.exists(dir_name):\n",
    "    os.mkdir(dir_name)\n",
    "\n",
    "def download_stories(filename):\n",
    "    print(\"Downloading file: \", dir_name + os.sep + filename)\n",
    "    if not os.path.exists(os.path.join(dir_name, filename)):\n",
    "        filename, _ = urlretrieve(url + filename, dir_name+os.sep+filename)\n",
    "    else:\n",
    "        print(\"File %s already exits\" % filename)\n",
    "    return filename\n",
    "\n",
    "\n",
    "filenames = [format(i, '03d') + '.txt' for i in range(1, 101)]\n",
    "\n",
    "for fn in filenames:\n",
    "    download_stories(fn)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing file stories\\001.txt\n",
      "Data size (chars) (document 0) 3667\n",
      "Sample string ['in', ' o', 'ld', 'en', ' t', 'im', 'es', ' w', 'he', 'n ', 'wi', 'sh', 'in', 'g ', 'st', 'il', 'l ', 'he', 'lp', 'ed', ' o', 'ne', ', ', 'th', 'er', 'e ', 'li', 've', 'd ', 'a ', 'ki', 'ng', '\\nw', 'ho', 'se', ' d', 'au', 'gh', 'te', 'rs', ' w', 'er', 'e ', 'al', 'l ', 'be', 'au', 'ti', 'fu', 'l,']\n",
      "\n",
      "processing file stories\\002.txt\n",
      "Data size (chars) (document 1) 4928\n",
      "Sample string ['ha', 'rd', ' b', 'y ', 'a ', 'gr', 'ea', 't ', 'fo', 're', 'st', ' d', 'we', 'lt', ' a', ' w', 'oo', 'd-', 'cu', 'tt', 'er', ' w', 'it', 'h ', 'hi', 's ', 'wi', 'fe', ', ', 'wh', 'o ', 'ha', 'd ', 'an', '\\no', 'nl', 'y ', 'ch', 'il', 'd,', ' a', ' l', 'it', 'tl', 'e ', 'gi', 'rl', ' t', 'hr', 'ee']\n",
      "\n",
      "processing file stories\\003.txt\n",
      "Data size (chars) (document 2) 9745\n",
      "Sample string ['a ', 'ce', 'rt', 'ai', 'n ', 'fa', 'th', 'er', ' h', 'ad', ' t', 'wo', ' s', 'on', 's,', ' t', 'he', ' e', 'ld', 'er', ' o', 'f ', 'wh', 'om', ' w', 'as', ' s', 'ma', 'rt', ' a', 'nd', '\\ns', 'en', 'si', 'bl', 'e,', ' a', 'nd', ' c', 'ou', 'ld', ' d', 'o ', 'ev', 'er', 'yt', 'hi', 'ng', ', ', 'bu']\n",
      "\n",
      "processing file stories\\004.txt\n",
      "Data size (chars) (document 3) 2852\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', 'n ', 'ol', 'd ', 'go', 'at', ' w', 'ho', ' h', 'ad', ' s', 'ev', 'en', ' l', 'it', 'tl', 'e ', 'ki', 'ds', ', ', 'an', 'd\\n', 'lo', 've', 'd ', 'th', 'em', ' w', 'it', 'h ', 'al', 'l ', 'th', 'e ', 'lo', 've', ' o']\n",
      "\n",
      "processing file stories\\005.txt\n",
      "Data size (chars) (document 4) 8189\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', 'n ', 'ol', 'd ', 'ki', 'ng', ' w', 'ho', ' w', 'as', ' i', 'll', ' a', 'nd', ' t', 'ho', 'ug', 'ht', ' t', 'o\\n', 'hi', 'ms', 'el', 'f ', \"'i\", ' a', 'm ', 'ly', 'in', 'g ', 'on', ' w', 'ha', 't ', 'mu', 'st', ' b']\n",
      "\n",
      "processing file stories\\006.txt\n",
      "Data size (chars) (document 5) 4369\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' p', 'ea', 'sa', 'nt', ' w', 'ho', ' h', 'ad', ' d', 'ri', 've', 'n ', 'hi', 's ', 'co', 'w ', 'to', ' t', 'he', ' f', 'ai', 'r,', ' a', 'nd', ' s', 'ol', 'd\\n', 'he', 'r ', 'fo', 'r ', 'se', 've', 'n ', 'ta', 'le', 'rs', '. ', ' o', 'n ', 'th', 'e ']\n",
      "\n",
      "processing file stories\\007.txt\n",
      "Data size (chars) (document 6) 5216\n",
      "Sample string ['th', 'er', 'e ', 'we', 're', ' o', 'nc', 'e ', 'up', 'on', ' a', ' t', 'im', 'e ', 'a ', 'ki', 'ng', ' a', 'nd', ' a', ' q', 'ue', 'en', ' w', 'ho', ' l', 'iv', 'ed', '\\nh', 'ap', 'pi', 'ly', ' t', 'og', 'et', 'he', 'r ', 'an', 'd ', 'ha', 'd ', 'tw', 'el', 've', ' c', 'hi', 'ld', 're', 'n,', ' b']\n",
      "\n",
      "processing file stories\\008.txt\n",
      "Data size (chars) (document 7) 6097\n",
      "Sample string ['li', 'tt', 'le', ' b', 'ro', 'th', 'er', ' t', 'oo', 'k ', 'hi', 's ', 'li', 'tt', 'le', ' s', 'is', 'te', 'r ', 'by', ' t', 'he', ' h', 'an', 'd ', 'an', 'd ', 'sa', 'id', ', ', 'si', 'nc', 'e\\n', 'ou', 'r ', 'mo', 'th', 'er', ' d', 'ie', 'd ', 'we', ' h', 'av', 'e ', 'ha', 'd ', 'no', ' h', 'ap']\n",
      "\n",
      "processing file stories\\009.txt\n",
      "Data size (chars) (document 8) 3699\n",
      "Sample string ['th', 'er', 'e ', 'we', 're', ' o', 'nc', 'e ', 'a ', 'ma', 'n ', 'an', 'd ', 'a ', 'wo', 'ma', 'n ', 'wh', 'o ', 'ha', 'd ', 'lo', 'ng', ' i', 'n ', 'va', 'in', '\\nw', 'is', 'he', 'd ', 'fo', 'r ', 'a ', 'ch', 'il', 'd.', '  ', 'at', ' l', 'en', 'gt', 'h ', 'th', 'e ', 'wo', 'ma', 'n ', 'ho', 'pe']\n",
      "\n",
      "processing file stories\\010.txt\n",
      "Data size (chars) (document 9) 5268\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' m', 'an', ' w', 'ho', 'se', ' w', 'if', 'e ', 'di', 'ed', ', ', 'an', 'd ', 'a ', 'wo', 'ma', 'n ', 'wh', 'os', 'e ', 'hu', 'sb', 'an', 'd\\n', 'di', 'ed', ', ', 'an', 'd ', 'th', 'e ', 'ma', 'n ', 'ha', 'd ', 'a ', 'da', 'ug', 'ht', 'er', ', ', 'an']\n",
      "\n",
      "processing file stories\\011.txt\n",
      "Data size (chars) (document 10) 2377\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' g', 'ir', 'l ', 'wh', 'o ', 'wa', 's ', 'id', 'le', ' a', 'nd', ' w', 'ou', 'ld', ' n', 'ot', ' s', 'pi', 'n,', ' a', 'nd', '\\nl', 'et', ' h', 'er', ' m', 'ot', 'he', 'r ', 'sa', 'y ', 'wh', 'at', ' s', 'he', ' w', 'ou', 'ld', ', ', 'sh', 'e ', 'co']\n",
      "\n",
      "processing file stories\\012.txt\n",
      "Data size (chars) (document 11) 7695\n",
      "Sample string ['ha', 'rd', ' b', 'y ', 'a ', 'gr', 'ea', 't ', 'fo', 're', 'st', ' d', 'we', 'lt', ' a', ' p', 'oo', 'r ', 'wo', 'od', '-c', 'ut', 'te', 'r ', 'wi', 'th', ' h', 'is', ' w', 'if', 'e\\n', 'an', 'd ', 'hi', 's ', 'tw', 'o ', 'ch', 'il', 'dr', 'en', '. ', ' t', 'he', ' b', 'oy', ' w', 'as', ' c', 'al']\n",
      "\n",
      "processing file stories\\013.txt\n",
      "Data size (chars) (document 12) 3665\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' o', 'n ', 'a ', 'ti', 'me', ' a', ' p', 'oo', 'r ', 'ma', 'n,', ' w', 'ho', ' c', 'ou', 'ld', ' n', 'o ', 'lo', 'ng', 'er', '\\ns', 'up', 'po', 'rt', ' h', 'is', ' o', 'nl', 'y ', 'so', 'n.', '  ', 'th', 'en', ' s', 'ai', 'd ', 'th', 'e ', 'so', 'n,', ' d']\n",
      "\n",
      "processing file stories\\014.txt\n",
      "Data size (chars) (document 13) 4178\n",
      "Sample string ['a ', 'lo', 'ng', ' t', 'im', 'e ', 'ag', 'o ', 'th', 'er', 'e ', 'li', 've', 'd ', 'a ', 'ki', 'ng', ' w', 'ho', ' w', 'as', ' f', 'am', 'ed', ' f', 'or', ' h', 'is', ' w', 'is', 'do', 'm\\n', 'th', 'ro', 'ug', 'h ', 'al', 'l ', 'th', 'e ', 'la', 'nd', '. ', ' n', 'ot', 'hi', 'ng', ' w', 'as', ' h']\n",
      "\n",
      "processing file stories\\015.txt\n",
      "Data size (chars) (document 14) 8674\n",
      "Sample string ['on', 'e ', 'su', 'mm', 'er', \"'s\", ' m', 'or', 'ni', 'ng', ' a', ' l', 'it', 'tl', 'e ', 'ta', 'il', 'or', ' w', 'as', ' s', 'it', 'ti', 'ng', ' o', 'n ', 'hi', 's ', 'ta', 'bl', 'e\\n', 'by', ' t', 'he', ' w', 'in', 'do', 'w,', ' h', 'e ', 'wa', 's ', 'in', ' g', 'oo', 'd ', 'sp', 'ir', 'it', 's,']\n",
      "\n",
      "processing file stories\\016.txt\n",
      "Data size (chars) (document 15) 7018\n",
      "Sample string ['\\tc', 'in', 'de', 're', 'll', 'a\\n', 'th', 'e ', 'wi', 'fe', ' o', 'f ', 'a ', 'ri', 'ch', ' m', 'an', ' f', 'el', 'l ', 'si', 'ck', ', ', 'an', 'd ', 'as', ' s', 'he', ' f', 'el', 't ', 'th', 'at', ' h', 'er', ' e', 'nd', '\\nw', 'as', ' d', 'ra', 'wi', 'ng', ' n', 'ea', 'r,', ' s', 'he', ' c', 'al']\n",
      "\n",
      "processing file stories\\017.txt\n",
      "Data size (chars) (document 16) 3039\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' k', 'in', \"g'\", 's ', 'so', 'n ', 'wh', 'o ', 'wa', 's ', 'se', 'iz', 'ed', ' w', 'it', 'h ', 'a ', 'de', 'si', 're', ' t', 'o ', 'tr', 'av', 'el', '\\na', 'bo', 'ut', ' t', 'he', ' w', 'or', 'ld', ', ', 'an', 'd ', 'to', 'ok', ' n', 'o ', 'on', 'e ']\n",
      "\n",
      "processing file stories\\018.txt\n",
      "Data size (chars) (document 17) 3020\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' w', 'id', 'ow', ' w', 'ho', ' h', 'ad', ' t', 'wo', ' d', 'au', 'gh', 'te', 'rs', ' -', ' o', 'ne', ' o', 'f\\n', 'wh', 'om', ' w', 'as', ' p', 're', 'tt', 'y ', 'an', 'd ', 'in', 'du', 'st', 'ri', 'ou', 's,', ' w', 'hi', 'ls', 't ', 'th', 'e ', 'ot']\n",
      "\n",
      "processing file stories\\019.txt\n",
      "Data size (chars) (document 18) 2465\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' m', 'an', ' w', 'ho', ' h', 'ad', ' s', 'ev', 'en', ' s', 'on', 's,', ' a', 'nd', ' s', 'ti', 'll', ' h', 'e ', 'ha', 'd\\n', 'no', ' d', 'au', 'gh', 'te', 'r,', ' h', 'ow', 'ev', 'er', ' m', 'uc', 'h ', 'he', ' w', 'is', 'he', 'd ', 'fo', 'r ', 'on']\n",
      "\n",
      "processing file stories\\020.txt\n",
      "Data size (chars) (document 19) 3703\n",
      "Sample string ['\\tl', 'it', 'tl', 'e ', 're', 'd-', 'ca', 'p\\n', '\\no', 'nc', 'e ', 'up', 'on', ' a', ' t', 'im', 'e ', 'th', 'er', 'e ', 'wa', 's ', 'a ', 'de', 'ar', ' l', 'it', 'tl', 'e ', 'gi', 'rl', ' w', 'ho', ' w', 'as', ' l', 'ov', 'ed', '\\nb', 'y ', 'ev', 'er', 'y ', 'on', 'e ', 'wh', 'o ', 'lo', 'ok', 'ed']\n",
      "\n",
      "processing file stories\\021.txt\n",
      "Data size (chars) (document 20) 1924\n",
      "Sample string ['in', ' a', ' c', 'er', 'ta', 'in', ' c', 'ou', 'nt', 'ry', ' t', 'he', 're', ' w', 'as', ' o', 'nc', 'e ', 'gr', 'ea', 't ', 'la', 'me', 'nt', 'at', 'io', 'n ', 'ov', 'er', ' a', '\\nw', 'il', 'd ', 'bo', 'ar', ' t', 'ha', 't ', 'la', 'id', ' w', 'as', 'te', ' t', 'he', ' f', 'ar', 'me', \"r'\", 's ']\n",
      "\n",
      "processing file stories\\022.txt\n",
      "Data size (chars) (document 21) 6561\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' p', 'oo', 'r ', 'wo', 'ma', 'n ', 'wh', 'o ', 'ga', 've', ' b', 'ir', 'th', ' t', 'o ', 'a ', 'li', 'tt', 'le', ' s', 'on', ',\\n', 'an', 'd ', 'as', ' h', 'e ', 'ca', 'me', ' i', 'nt', 'o ', 'th', 'e ', 'wo', 'rl', 'd ', 'wi', 'th', ' a', ' c', 'au']\n",
      "\n",
      "processing file stories\\023.txt\n",
      "Data size (chars) (document 22) 5956\n",
      "Sample string ['a ', 'ce', 'rt', 'ai', 'n ', 'mi', 'll', 'er', ' h', 'ad', ' l', 'it', 'tl', 'e ', 'by', ' l', 'it', 'tl', 'e ', 'fa', 'll', 'en', ' i', 'nt', 'o ', 'po', 've', 'rt', 'y,', ' a', 'nd', '\\nh', 'ad', ' n', 'ot', 'hi', 'ng', ' l', 'ef', 't ', 'bu', 't ', 'hi', 's ', 'mi', 'll', ' a', 'nd', ' a', ' l']\n",
      "\n",
      "processing file stories\\024.txt\n",
      "Data size (chars) (document 23) 2529\n",
      "Sample string ['th', 'e ', 'mo', 'th', 'er', ' o', 'f ', 'ha', 'ns', ' s', 'ai', 'd,', ' w', 'hi', 'th', 'er', ' a', 'wa', 'y,', ' h', 'an', 's.', '  ', 'ha', 'ns', ' a', 'ns', 'we', 're', 'd,', ' t', 'o\\n', 'gr', 'et', 'el', '. ', ' b', 'eh', 'av', 'e ', 'we', 'll', ', ', 'ha', 'ns', '. ', ' o', 'h,', ' i', \"'l\"]\n",
      "\n",
      "processing file stories\\025.txt\n",
      "Data size (chars) (document 24) 2416\n",
      "Sample string ['an', ' a', 'ge', 'd ', 'co', 'un', 't ', 'on', 'ce', ' l', 'iv', 'ed', ' i', 'n ', 'sw', 'it', 'ze', 'rl', 'an', 'd,', ' w', 'ho', ' h', 'ad', ' a', 'n ', 'on', 'ly', ' s', 'on', ',\\n', 'bu', 't ', 'he', ' w', 'as', ' s', 'tu', 'pi', 'd,', ' a', 'nd', ' c', 'ou', 'ld', ' l', 'ea', 'rn', ' n', 'ot']\n",
      "\n",
      "processing file stories\\026.txt\n",
      "Data size (chars) (document 25) 3369\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' m', 'an', ' w', 'ho', ' h', 'ad', ' a', ' d', 'au', 'gh', 'te', 'r ', 'wh', 'o ', 'wa', 's ', 'ca', 'll', 'ed', ' c', 'le', 've', 'r\\n', 'el', 'si', 'e.', '  ', 'an', 'd ', 'wh', 'en', ' s', 'he', ' h', 'ad', ' g', 'ro', 'wn', ' u', 'p ', 'he', 'r ']\n",
      "\n",
      "processing file stories\\027.txt\n",
      "Data size (chars) (document 26) 10013\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' t', 'ai', 'lo', 'r ', 'wh', 'o ', 'ha', 'd ', 'th', 're', 'e ', 'so', 'ns', ', ', 'an', 'd\\n', 'on', 'ly', ' o', 'ne', ' g', 'oa', 't.', '  ', 'bu', 't ', 'as', ' t', 'he', ' g', 'oa', 't ', 'su', 'pp', 'or', 'te']\n",
      "\n",
      "processing file stories\\028.txt\n",
      "Data size (chars) (document 27) 5788\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' p', 'oo', 'r ', 'pe', 'as', 'an', 't ', 'wh', 'o ', 'sa', 't ', 'in', ' t', 'he', ' e', 've', 'ni', 'ng', ' b', 'y ', 'th', 'e\\n', 'he', 'ar', 'th', ' a', 'nd', ' p', 'ok', 'ed', ' t', 'he', ' f', 'ir', 'e,', ' a', 'nd', ' h', 'is', ' w', 'if', 'e ']\n",
      "\n",
      "processing file stories\\029.txt\n",
      "Data size (chars) (document 28) 1335\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' p', 'oo', 'r ', 'se', 'rv', 'an', 't-', 'gi', 'rl', ' w', 'ho', ' w', 'as', ' i', 'nd', 'us', 'tr', 'io', 'us', ' a', 'nd', ' c', 'le', 'an', 'ly', '\\na', 'nd', ' s', 'we', 'pt', ' t', 'he', ' h', 'ou', 'se', ' e', 've', 'ry', ' d', 'ay', ', ', 'an']\n",
      "\n",
      "processing file stories\\030.txt\n",
      "Data size (chars) (document 29) 3591\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' m', 'il', 'le', 'r,', ' w', 'ho', ' h', 'ad', ' a', ' b', 'ea', 'ut', 'if', 'ul', '\\nd', 'au', 'gh', 'te', 'r,', ' a', 'nd', ' a', 's ', 'sh', 'e ', 'wa', 's ', 'gr', 'ow', 'n ', 'up', ', ', 'he', ' w', 'is', 'he']\n",
      "\n",
      "processing file stories\\031.txt\n",
      "Data size (chars) (document 30) 1624\n",
      "Sample string ['a ', 'po', 'or', ' m', 'an', ' h', 'ad', ' s', 'o ', 'ma', 'ny', ' c', 'hi', 'ld', 're', 'n ', 'th', 'at', ' h', 'e ', 'ha', 'd ', 'al', 're', 'ad', 'y ', 'as', 'ke', 'd\\n', 'ev', 'er', 'yo', 'ne', ' i', 'n ', 'th', 'e ', 'wo', 'rl', 'd ', 'to', ' b', 'e ', 'go', 'df', 'at', 'he', 'r,', ' a', 'nd']\n",
      "\n",
      "processing file stories\\032.txt\n",
      "Data size (chars) (document 31) 758\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' l', 'it', 'tl', 'e ', 'gi', 'rl', ' w', 'ho', ' w', 'as', ' o', 'bs', 'ti', 'na', 'te', ' a', 'nd', ' i', 'nq', 'ui', 'si', 'ti', 've', ',\\n', 'an', 'd ', 'wh', 'en', ' h', 'er', ' p', 'ar', 'en', 'ts', ' t', 'ol', 'd ', 'he', 'r ', 'to', ' d', 'o ']\n",
      "\n",
      "processing file stories\\033.txt\n",
      "Data size (chars) (document 32) 3121\n",
      "Sample string ['a ', 'po', 'or', ' m', 'an', ' h', 'ad', ' t', 'we', 'lv', 'e ', 'ch', 'il', 'dr', 'en', ' a', 'nd', ' w', 'as', ' f', 'or', 'ce', 'd ', 'to', ' w', 'or', 'k ', 'ni', 'gh', 't ', 'an', 'd\\n', 'da', 'y ', 'to', ' g', 'iv', 'e ', 'th', 'em', ' e', 've', 'n ', 'br', 'ea', 'd.', '  ', 'wh', 'en', ' t']\n",
      "\n",
      "processing file stories\\034.txt\n",
      "Data size (chars) (document 33) 4192\n",
      "Sample string ['a ', 'ce', 'rt', 'ai', 'n ', 'ta', 'il', 'or', ' h', 'ad', ' a', ' s', 'on', ', ', 'wh', 'o ', 'ha', 'pp', 'en', 'ed', ' t', 'o ', 'be', ' s', 'ma', 'll', ', ', 'an', 'd\\n', 'no', ' b', 'ig', 'ge', 'r ', 'th', 'an', ' a', ' t', 'hu', 'mb', ', ', 'an', 'd ', 'on', ' t', 'hi', 's ', 'ac', 'co', 'un']\n",
      "\n",
      "processing file stories\\035.txt\n",
      "Data size (chars) (document 34) 3650\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' w', 'iz', 'ar', 'd ', 'wh', 'o ', 'us', 'ed', ' t', 'o ', 'ta', 'ke', ' t', 'he', ' f', 'or', 'm ', 'of', ' a', ' p', 'oo', 'r\\n', 'ma', 'n,', ' a', 'nd', ' w', 'en', 't ', 'to', ' h', 'ou', 'se', 's ', 'an', 'd ', 'be', 'gg', 'ed', ', ', 'an', 'd ']\n",
      "\n",
      "processing file stories\\036.txt\n",
      "Data size (chars) (document 35) 8219\n",
      "Sample string ['it', ' i', 's ', 'no', 'w ', 'lo', 'ng', ' a', 'go', ', ', 'qu', 'it', 'e ', 'tw', 'o ', 'th', 'ou', 'sa', 'nd', ' y', 'ea', 'rs', ', ', 'si', 'nc', 'e ', 'th', 'er', 'e ', 'wa', 's\\n', 'a ', 'ri', 'ch', ' m', 'an', ' w', 'ho', ' h', 'ad', ' a', ' b', 'ea', 'ut', 'if', 'ul', ' a', 'nd', ' p', 'io']\n",
      "\n",
      "processing file stories\\037.txt\n",
      "Data size (chars) (document 36) 2151\n",
      "Sample string ['a ', 'fa', 'rm', 'er', ' o', 'nc', 'e ', 'ha', 'd ', 'a ', 'fa', 'it', 'hf', 'ul', ' d', 'og', ' c', 'al', 'le', 'd ', 'su', 'lt', 'an', ', ', 'wh', 'o ', 'ha', 'd ', 'gr', 'ow', 'n\\n', 'ol', 'd,', ' a', 'nd', ' l', 'os', 't ', 'al', 'l ', 'hi', 's ', 'te', 'et', 'h,', ' s', 'o ', 'th', 'at', ' h']\n",
      "\n",
      "processing file stories\\038.txt\n",
      "Data size (chars) (document 37) 5129\n",
      "Sample string ['on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ', ', 'a ', 'ce', 'rt', 'ai', 'n ', 'ki', 'ng', ' w', 'as', ' h', 'un', 'ti', 'ng', ' i', 'n ', 'a ', 'gr', 'ea', 't ', 'fo', 're', 'st', ',\\n', 'an', 'd ', 'he', ' c', 'ha', 'se', 'd ', 'a ', 'wi', 'ld', ' b', 'ea', 'st', ' s', 'o ', 'ea', 'ge', 'rl']\n",
      "\n",
      "processing file stories\\039.txt\n",
      "Data size (chars) (document 38) 3472\n",
      "Sample string ['\\tb', 'ri', 'ar', '-r', 'os', 'e\\n', '\\na', ' l', 'on', 'g ', 'ti', 'me', ' a', 'go', ' t', 'he', 're', ' w', 'er', 'e ', 'a ', 'ki', 'ng', ' a', 'nd', ' q', 'ue', 'en', ' w', 'ho', ' s', 'ai', 'd ', 'ev', 'er', 'y\\n', 'da', 'y,', ' a', 'h,', ' i', 'f ', 'on', 'ly', ' w', 'e ', 'ha', 'd ', 'a ', 'ch']\n",
      "\n",
      "processing file stories\\040.txt\n",
      "Data size (chars) (document 39) 2490\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' f', 'or', 'es', 'te', 'r ', 'wh', 'o ', 'we', 'nt', ' i', 'nt', 'o ', 'th', 'e ', 'fo', 're', 'st', ' t', 'o ', 'hu', 'nt', ',\\n', 'an', 'd ', 'as', ' h', 'e ', 'en', 'te', 're', 'd ', 'it', ' h', 'e ', 'he', 'ar', 'd ', 'a ', 'so', 'un', 'd ', 'of']\n",
      "\n",
      "processing file stories\\041.txt\n",
      "Data size (chars) (document 40) 4273\n",
      "Sample string ['a ', 'ki', 'ng', ' h', 'ad', ' a', ' d', 'au', 'gh', 'te', 'r ', 'wh', 'o ', 'wa', 's ', 'be', 'au', 'ti', 'fu', 'l ', 'be', 'yo', 'nd', ' a', 'll', ' m', 'ea', 'su', 're', ',\\n', 'bu', 't ', 'so', ' p', 'ro', 'ud', ' a', 'nd', ' h', 'au', 'gh', 'ty', ' w', 'it', 'ha', 'l ', 'th', 'at', ' n', 'o ']\n",
      "\n",
      "processing file stories\\042.txt\n",
      "Data size (chars) (document 41) 8327\n",
      "Sample string ['\\ts', 'no', 'w ', 'wh', 'it', 'e ', 'an', 'd ', 'th', 'e ', 'se', 've', 'n ', 'dw', 'ar', 'fs', '\\n\\n', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' i', 'n ', 'th', 'e ', 'mi', 'dd', 'le', ' o', 'f ', 'wi', 'nt', 'er', ', ', 'wh', 'en', ' t', 'he', ' f', 'la', 'ke', 's ', 'of', '\\ns', 'no', 'w ']\n",
      "\n",
      "processing file stories\\043.txt\n",
      "Data size (chars) (document 42) 6128\n",
      "Sample string ['th', 'er', 'e ', 'we', 're', ' o', 'nc', 'e ', 'th', 're', 'e ', 'br', 'ot', 'he', 'rs', ' w', 'ho', ' h', 'ad', ' f', 'al', 'le', 'n ', 'de', 'ep', 'er', ' a', 'nd', ' d', 'ee', 'pe', 'r ', 'in', 'to', '\\np', 'ov', 'er', 'ty', ', ', 'an', 'd ', 'at', ' l', 'as', 't ', 'th', 'ei', 'r ', 'ne', 'ed']\n",
      "\n",
      "processing file stories\\044.txt\n",
      "Data size (chars) (document 43) 2819\n",
      "Sample string ['\\tr', 'um', 'pe', 'ls', 'ti', 'lt', 'sk', 'in', '\\n\\n', 'on', 'ce', ' t', 'he', 're', ' w', 'as', ' a', ' m', 'il', 'le', 'r ', 'wh', 'o ', 'wa', 's ', 'po', 'or', ', ', 'bu', 't ', 'wh', 'o ', 'ha', 'd ', 'a ', 'be', 'au', 'ti', 'fu', 'l\\n', 'da', 'ug', 'ht', 'er', '. ', ' n', 'ow', ' i', 't ', 'ha']\n",
      "\n",
      "processing file stories\\045.txt\n",
      "Data size (chars) (document 44) 3822\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' w', 'om', 'an', ' w', 'ho', ' w', 'as', ' a', ' r', 'ea', 'l ', 'wi', 'tc', 'h ', 'an', 'd ', 'ha', 'd ', 'tw', 'o\\n', 'da', 'ug', 'ht', 'er', 's,', ' o', 'ne', ' u', 'gl', 'y ', 'an', 'd ', 'wi', 'ck', 'ed', ', ']\n",
      "\n",
      "processing file stories\\046.txt\n",
      "Data size (chars) (document 45) 7772\n",
      "Sample string ['in', ' o', 'ld', 'en', ' t', 'im', 'es', ' t', 'he', 're', ' w', 'as', ' a', ' k', 'in', 'g,', ' w', 'ho', ' h', 'ad', ' b', 'eh', 'in', 'd ', 'hi', 's ', 'pa', 'la', 'ce', ' a', '\\nb', 'ea', 'ut', 'if', 'ul', ' p', 'le', 'as', 'ur', 'e-', 'ga', 'rd', 'en', ' i', 'n ', 'wh', 'ic', 'h ', 'th', 'er']\n",
      "\n",
      "processing file stories\\047.txt\n",
      "Data size (chars) (document 46) 22158\n",
      "Sample string ['th', 'er', 'e ', 'we', 're', ' o', 'nc', 'e ', 'up', 'on', ' a', ' t', 'im', 'e ', 'tw', 'o ', 'br', 'ot', 'he', 'rs', ', ', 'on', 'e ', 'ri', 'ch', ' a', 'nd', ' t', 'he', ' o', 'th', 'er', '\\np', 'oo', 'r.', '  ', 'th', 'e ', 'ri', 'ch', ' o', 'ne', ' w', 'as', ' a', ' g', 'ol', 'ds', 'mi', 'th']\n",
      "\n",
      "processing file stories\\048.txt\n",
      "Data size (chars) (document 47) 2169\n",
      "Sample string ['tw', 'o ', 'ki', 'ng', \"s'\", ' s', 'on', 's ', 'on', 'ce', ' w', 'en', 't ', 'ou', 't ', 'in', ' s', 'ea', 'rc', 'h ', 'of', ' a', 'dv', 'en', 'tu', 're', 's,', ' a', 'nd', ' f', 'el', 'l ', 'in', 'to', '\\na', ' w', 'il', 'd,', ' d', 'is', 'or', 'de', 'rl', 'y ', 'wa', 'y ', 'of', ' l', 'iv', 'in']\n",
      "\n",
      "processing file stories\\049.txt\n",
      "Data size (chars) (document 48) 2822\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' k', 'in', 'g ', 'wh', 'o ', 'ha', 'd ', 'th', 're', 'e ', 'so', 'ns', ', ', 'of', ' w', 'ho', 'm ', 'tw', 'o\\n', 'we', 're', ' c', 'le', 've', 'r ', 'an', 'd ', 'wi', 'se', ', ', 'bu', 't ', 'th', 'e ', 'th', 'ir']\n",
      "\n",
      "processing file stories\\050.txt\n",
      "Data size (chars) (document 49) 4034\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'a ', 'ma', 'n ', 'wh', 'o ', 'ha', 'd ', 'th', 're', 'e ', 'so', 'ns', ', ', 'th', 'e ', 'yo', 'un', 'ge', 'st', ' o', 'f ', 'wh', 'om', ' w', 'as', ' c', 'al', 'le', 'd\\n', 'du', 'mm', 'li', 'ng', ', ', 'an', 'd ', 'wa', 's ', 'de', 'sp', 'is', 'ed', ', ', 'mo', 'ck']\n",
      "\n",
      "processing file stories\\051.txt\n",
      "Data size (chars) (document 50) 5608\n",
      "Sample string ['\\ta', 'll', 'er', 'le', 'ir', 'au', 'h\\n', '\\nt', 'he', 're', ' w', 'as', ' o', 'nc', 'e ', 'up', 'on', ' a', ' t', 'im', 'e ', 'a ', 'ki', 'ng', ' w', 'ho', ' h', 'ad', ' a', ' w', 'if', 'e ', 'wi', 'th', ' g', 'ol', 'de', 'n ', 'ha', 'ir', ',\\n', 'an', 'd ', 'sh', 'e ', 'wa', 's ', 'so', ' b', 'ea']\n",
      "\n",
      "processing file stories\\052.txt\n",
      "Data size (chars) (document 51) 1287\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' w', 'om', 'an', ' a', 'nd', ' h', 'er', ' d', 'au', 'gh', 'te', 'r ', 'wh', 'o ', 'li', 've', 'd ', 'in', ' a', '\\np', 're', 'tt', 'y ', 'ga', 'rd', 'en', ' w', 'it', 'h ', 'ca', 'bb', 'ag', 'es', '. ', ' a', 'nd', ' a', ' l', 'it', 'tl', 'e ', 'ha']\n",
      "\n",
      "processing file stories\\053.txt\n",
      "Data size (chars) (document 52) 2841\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' k', 'in', \"g'\", 's ', 'so', 'n ', 'wh', 'o ', 'ha', 'd ', 'a ', 'br', 'id', 'e ', 'wh', 'om', ' h', 'e ', 'lo', 've', 'd ', 've', 'ry', ' m', 'uc', 'h.', '\\na', 'nd', ' w', 'he', 'n ', 'he', ' w', 'as', ' s', 'it', 'ti', 'ng', ' b', 'es', 'id', 'e ']\n",
      "\n",
      "processing file stories\\054.txt\n",
      "Data size (chars) (document 53) 1922\n",
      "Sample string ['ha', 'ns', ' w', 'is', 'he', 'd ', 'to', ' p', 'ut', ' h', 'is', ' s', 'on', ' t', 'o ', 'le', 'ar', 'n ', 'a ', 'tr', 'ad', 'e,', ' s', 'o ', 'he', ' w', 'en', 't ', 'in', 'to', ' t', 'he', '\\nc', 'hu', 'rc', 'h ', 'an', 'd ', 'pr', 'ay', 'ed', ' t', 'o ', 'ou', 'r ', 'lo', 'rd', ' g', 'od', ' t']\n",
      "\n",
      "processing file stories\\055.txt\n",
      "Data size (chars) (document 54) 2573\n",
      "Sample string ['a ', 'fa', 'th', 'er', ' o', 'nc', 'e ', 'ca', 'll', 'ed', ' h', 'is', ' t', 'hr', 'ee', ' s', 'on', 's ', 'be', 'fo', 're', ' h', 'im', ', ', 'an', 'd ', 'he', ' g', 'av', 'e ', 'to', ' t', 'he', '\\nf', 'ir', 'st', ' a', ' c', 'oc', 'k,', ' t', 'o ', 'th', 'e ', 'se', 'co', 'nd', ' a', ' s', 'cy']\n",
      "\n",
      "processing file stories\\056.txt\n",
      "Data size (chars) (document 55) 5285\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' m', 'an', ' w', 'ho', ' u', 'nd', 'er', 'st', 'oo', 'd ', 'al', 'l ', 'ki', 'nd', 's ', 'of', ' a', 'rt', 's.', '  ', 'he', ' s', 'er', 've', 'd ', 'in', '\\nw', 'ar', ', ', 'an', 'd ', 'be', 'ha', 've', 'd ', 'we', 'll', ' a', 'nd', ' b', 'ra', 've']\n",
      "\n",
      "processing file stories\\057.txt\n",
      "Data size (chars) (document 56) 971\n",
      "Sample string ['th', 'e ', 'sh', 'e-', 'wo', 'lf', ' b', 'ro', 'ug', 'ht', ' i', 'nt', 'o ', 'th', 'e ', 'wo', 'rl', 'd ', 'a ', 'yo', 'un', 'g ', 'on', 'e,', ' a', 'nd', ' i', 'nv', 'it', 'ed', ' t', 'he', ' f', 'ox', '\\nt', 'o ', 'be', ' g', 'od', 'fa', 'th', 'er', '. ', ' a', 'ft', 'er', ' a', 'll', ', ', 'he']\n",
      "\n",
      "processing file stories\\058.txt\n",
      "Data size (chars) (document 57) 4538\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' q', 'ue', 'en', ' t', 'o ', 'wh', 'om', ' g', 'od', ' h', 'ad', ' g', 'iv', 'en', ' n', 'o ', 'ch', 'il', 'dr', 'en', '.\\n', 'ev', 'er', 'y ', 'mo', 'rn', 'in', 'g ', 'sh', 'e ', 'we', 'nt', ' i', 'nt', 'o ', 'th']\n",
      "\n",
      "processing file stories\\059.txt\n",
      "Data size (chars) (document 58) 636\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' v', 'er', 'y ', 'ol', 'd ', 'ma', 'n,', ' w', 'ho', 'se', ' e', 'ye', 's ', 'ha', 'd ', 'be', 'co', 'me', ' d', 'im', ', ', 'hi', 's ', 'ea', 'rs', '\\nd', 'ul', 'l ', 'of', ' h', 'ea', 'ri', 'ng', ', ', 'hi', 's ', 'kn', 'ee', 's ', 'tr', 'em', 'bl']\n",
      "\n",
      "processing file stories\\060.txt\n",
      "Data size (chars) (document 59) 786\n",
      "Sample string ['a ', 'li', 'tt', 'le', ' b', 'ro', 'th', 'er', ' a', 'nd', ' s', 'is', 'te', 'r ', 'we', 're', ' o', 'nc', 'e ', 'pl', 'ay', 'in', 'g ', 'by', ' a', ' w', 'el', 'l,', ' a', 'nd', ' w', 'hi', 'le', '\\nt', 'he', 'y ', 'we', 're', ' t', 'hu', 's ', 'pl', 'ay', 'in', 'g,', ' t', 'he', 'y ', 'bo', 'th']\n",
      "\n",
      "processing file stories\\061.txt\n",
      "Data size (chars) (document 60) 10687\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'e ', 'up', 'on', ' a', ' t', 'im', 'e ', 'a ', 'gr', 'ea', 't ', 'wa', 'r,', ' a', 'nd', ' w', 'he', 'n ', 'it', ' c', 'am', 'e ', 'to', ' a', 'n ', 'en', 'd,', '\\nm', 'an', 'y ', 'so', 'ld', 'ie', 'rs', ' w', 'er', 'e ', 'di', 'sc', 'ha', 'rg', 'ed', '. ', ' t']\n",
      "\n",
      "processing file stories\\062.txt\n",
      "Data size (chars) (document 61) 5105\n",
      "Sample string ['ha', 'ns', ' h', 'ad', ' s', 'er', 've', 'd ', 'hi', 's ', 'ma', 'st', 'er', ' f', 'or', ' s', 'ev', 'en', ' y', 'ea', 'rs', ', ', 'so', ' h', 'e ', 'sa', 'id', ' t', 'o ', 'hi', 'm,', '\\nm', 'as', 'te', 'r,', ' m', 'y ', 'ti', 'me', ' i', 's ', 'up', ', ', 'no', 'w ', 'i ', 'sh', 'ou', 'ld', ' b']\n",
      "\n",
      "processing file stories\\063.txt\n",
      "Data size (chars) (document 62) 1127\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' y', 'ou', 'ng', ' p', 'ea', 'sa', 'nt', ' n', 'am', 'ed', ' h', 'an', 's,', ' w', 'ho', 'se', ' u', 'nc', 'le', '\\nw', 'an', 'te', 'd ', 'to', ' f', 'in', 'd ', 'hi', 'm ', 'a ', 'ri', 'ch', ' w', 'if', 'e.', '  ']\n",
      "\n",
      "processing file stories\\064.txt\n",
      "Data size (chars) (document 63) 4981\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' p', 'oo', 'r ', 'ma', 'n ', 'an', 'd ', 'a ', 'po', 'or', ' w', 'om', 'an', ' w', 'ho', ' h', 'ad', ' n', 'ot', 'hi', 'ng', ' b', 'ut', ' a', '\\nl', 'it', 'tl', 'e ', 'co', 'tt', 'ag', 'e,', ' a', 'nd', ' w', 'ho', ' e', 'ar', 'ne', 'd ', 'th', 'ei']\n",
      "\n",
      "processing file stories\\065.txt\n",
      "Data size (chars) (document 64) 6006\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' m', 'an', ' w', 'ho', ' w', 'as', ' a', 'bo', 'ut', ' t', 'o ', 'se', 't ', 'ou', 't ', 'on', ' a', ' l', 'on', 'g\\n', 'jo', 'ur', 'ne', 'y,', ' a', 'nd', ' o', 'n ', 'pa', 'rt', 'in', 'g ', 'he', ' a', 'sk', 'ed']\n",
      "\n",
      "processing file stories\\066.txt\n",
      "Data size (chars) (document 65) 5900\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', 'n ', 'ol', 'd ', 'qu', 'ee', 'n ', 'wh', 'os', 'e ', 'hu', 'sb', 'an', 'd ', 'ha', 'd ', 'be', 'en', ' d', 'ea', 'd\\n', 'fo', 'r ', 'ma', 'ny', ' y', 'ea', 'rs', ', ', 'an', 'd ', 'sh', 'e ', 'ha', 'd ', 'a ', 'be']\n",
      "\n",
      "processing file stories\\067.txt\n",
      "Data size (chars) (document 66) 7837\n",
      "Sample string ['on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' c', 'ou', 'nt', 'ry', 'ma', 'n ', 'ha', 'd ', 'a ', 'so', 'n ', 'wh', 'o ', 'wa', 's ', 'as', ' b', 'ig', ' a', 's ', 'a ', 'th', 'um', 'b,', '\\na', 'nd', ' d', 'id', ' n', 'ot', ' b', 'ec', 'om', 'e ', 'an', 'y ', 'bi', 'gg', 'er', ', ', 'an']\n",
      "\n",
      "processing file stories\\068.txt\n",
      "Data size (chars) (document 67) 4717\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' r', 'ic', 'h ', 'ki', 'ng', ' w', 'ho', ' h', 'ad', ' t', 'hr', 'ee', ' d', 'au', 'gh', 'te', 'rs', ', ', 'wh', 'o\\n', 'da', 'il', 'y ', 'we', 'nt', ' t', 'o ', 'wa', 'lk', ' i', 'n ', 'th', 'e ', 'pa', 'la', 'ce']\n",
      "\n",
      "processing file stories\\069.txt\n",
      "Data size (chars) (document 68) 6233\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'a ', 'ce', 'rt', 'ai', 'n ', 'me', 'rc', 'ha', 'nt', ' w', 'ho', ' h', 'ad', ' t', 'wo', ' c', 'hi', 'ld', 're', 'n,', ' a', ' b', 'oy', ' a', 'nd', ' a', ' g', 'ir', 'l,', '\\nt', 'he', 'y ', 'we', 're', ' b', 'ot', 'h ', 'yo', 'un', 'g,', ' a', 'nd', ' c', 'ou', 'ld']\n",
      "\n",
      "processing file stories\\070.txt\n",
      "Data size (chars) (document 69) 5664\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' q', 'ue', 'en', ' w', 'ho', ' h', 'ad', ' a', ' l', 'it', 'tl', 'e ', 'da', 'ug', 'ht', 'er', ' w', 'ho', '\\nw', 'as', ' s', 'ti', 'll', ' s', 'o ', 'yo', 'un', 'g ', 'th', 'at', ' s', 'he', ' h', 'ad', ' t', 'o ']\n",
      "\n",
      "processing file stories\\071.txt\n",
      "Data size (chars) (document 70) 3569\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' p', 'oo', 'r ', 'pe', 'as', 'an', 't ', 'wh', 'o ', 'ha', 'd ', 'no', ' l', 'an', 'd,', ' b', 'ut', ' o', 'nl', 'y ', 'a ', 'sm', 'al', 'l\\n', 'ho', 'us', 'e,', ' a', 'nd', ' o', 'ne', ' d', 'au', 'gh', 'te', 'r.', '  ', 'th', 'en', ' s', 'ai', 'd ']\n",
      "\n",
      "processing file stories\\072.txt\n",
      "Data size (chars) (document 71) 3793\n",
      "Sample string ['ab', 'ou', 't ', 'a ', 'th', 'ou', 'sa', 'nd', ' o', 'r ', 'mo', 're', ' y', 'ea', 'rs', ' a', 'go', ', ', 'th', 'er', 'e ', 'we', 're', ' i', 'n ', 'th', 'is', '\\nc', 'ou', 'nt', 'ry', ' n', 'ot', 'hi', 'ng', ' b', 'ut', ' s', 'ma', 'll', ' k', 'in', 'gs', ', ', 'an', 'd ', 'on', 'e ', 'of', ' t']\n",
      "\n",
      "processing file stories\\073.txt\n",
      "Data size (chars) (document 72) 5980\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' k', 'in', 'g ', 'wh', 'o ', 'ha', 'd ', 'an', ' i', 'll', 'ne', 'ss', ', ', 'an', 'd ', 'no', ' o', 'ne', ' b', 'el', 'ie', 've', 'd ', 'th', 'at', ' h', 'e\\n', 'wo', 'ul', 'd ', 'co', 'me', ' o', 'ut', ' o', 'f ', 'it', ' w', 'it', 'h ', 'hi', 's ']\n",
      "\n",
      "processing file stories\\074.txt\n",
      "Data size (chars) (document 73) 4518\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' p', 'oo', 'r ', 'wo', 'od', 'cu', 'tt', 'er', ' w', 'ho', ' t', 'oi', 'le', 'd ', 'fr', 'om', ' e', 'ar', 'ly', '\\nm', 'or', 'ni', 'ng', ' t', 'il', 'l ', 'la', 'te', ' a', 't ', 'ni', 'gh', 't.', '  ', 'wh', 'en', ' a', 't ', 'la', 'st', ' h', 'e ']\n",
      "\n",
      "processing file stories\\075.txt\n",
      "Data size (chars) (document 74) 3247\n",
      "Sample string ['a ', 'di', 'sc', 'ha', 'rg', 'ed', ' s', 'ol', 'di', 'er', ' h', 'ad', ' n', 'ot', 'hi', 'ng', ' t', 'o ', 'li', 've', ' o', 'n,', ' a', 'nd', ' d', 'id', ' n', 'ot', ' k', 'no', 'w ', 'ho', 'w ', 'to', '\\nm', 'ak', 'e ', 'hi', 's ', 'wa', 'y.', '  ', 'so', ' h', 'e ', 'we', 'nt', ' o', 'ut', ' i']\n",
      "\n",
      "processing file stories\\076.txt\n",
      "Data size (chars) (document 75) 5130\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' y', 'ou', 'ng', ' f', 'el', 'lo', 'w ', 'wh', 'o ', 'en', 'li', 'st', 'ed', ' a', 's ', 'a ', 'so', 'ld', 'ie', 'r,', ' c', 'on', 'du', 'ct', 'ed', '\\nh', 'im', 'se', 'lf', ' b', 'ra', 've', 'ly', ', ', 'an', 'd ', 'wa', 's ', 'al', 'wa', 'ys', ' t']\n",
      "\n",
      "processing file stories\\077.txt\n",
      "Data size (chars) (document 76) 2401\n",
      "Sample string ['on', 'ce', ' i', 'n ', 'su', 'mm', 'er', '-t', 'im', 'e ', 'th', 'e ', 'be', 'ar', ' a', 'nd', ' t', 'he', ' w', 'ol', 'f ', 'we', 're', ' w', 'al', 'ki', 'ng', ' i', 'n ', 'th', 'e ', 'fo', 're', 'st', ',\\n', 'an', 'd ', 'th', 'e ', 'be', 'ar', ' h', 'ea', 'rd', ' a', ' b', 'ir', 'd ', 'si', 'ng']\n",
      "\n",
      "processing file stories\\078.txt\n",
      "Data size (chars) (document 77) 624\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'a ', 'po', 'or', ' b', 'ut', ' g', 'oo', 'd ', 'li', 'tt', 'le', ' g', 'ir', 'l ', 'wh', 'o ', 'li', 've', 'd ', 'al', 'on', 'e ', 'wi', 'th', ' h', 'er', '\\nm', 'ot', 'he', 'r,', ' a', 'nd', ' t', 'he', 'y ', 'no', ' l', 'on', 'ge', 'r ', 'ha', 'd ', 'an', 'yt', 'hi']\n",
      "\n",
      "processing file stories\\079.txt\n",
      "Data size (chars) (document 78) 3991\n",
      "Sample string ['on', 'e ', 'da', 'y ', 'a ', 'pe', 'as', 'an', 't ', 'to', 'ok', ' h', 'is', ' g', 'oo', 'd ', 'ha', 'ze', 'l-', 'st', 'ic', 'k ', 'ou', 't ', 'of', ' t', 'he', ' c', 'or', 'ne', 'r\\n', 'an', 'd ', 'sa', 'id', ' t', 'o ', 'hi', 's ', 'wi', 'fe', ', ', 'tr', 'in', 'a,', ' i', ' a', 'm ', 'go', 'in']\n",
      "\n",
      "processing file stories\\080.txt\n",
      "Data size (chars) (document 79) 1426\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' l', 'it', 'tl', 'e ', 'ch', 'il', 'd ', 'wh', 'os', 'e ', 'mo', 'th', 'er', ' g', 'av', 'e ', 'he', 'r ', 'ev', 'er', 'y\\n', 'af', 'te', 'rn', 'oo', 'n ', 'a ', 'sm', 'al', 'l ', 'bo', 'wl', ' o', 'f ', 'mi', 'lk', ' a', 'nd', ' b', 're', 'ad', ', ']\n",
      "\n",
      "processing file stories\\081.txt\n",
      "Data size (chars) (document 80) 3574\n",
      "Sample string ['in', ' a', ' c', 'er', 'ta', 'in', ' m', 'il', 'l ', 'li', 've', 'd ', 'an', ' o', 'ld', ' m', 'il', 'le', 'r ', 'wh', 'o ', 'ha', 'd ', 'ne', 'it', 'he', 'r ', 'wi', 'fe', ' n', 'or', ' c', 'hi', 'ld', ',\\n', 'an', 'd ', 'th', 're', 'e ', 'ap', 'pr', 'en', 'ti', 'ce', 's ', 'se', 'rv', 'ed', ' u']\n",
      "\n",
      "processing file stories\\082.txt\n",
      "Data size (chars) (document 81) 10822\n",
      "Sample string ['hi', 'll', ' a', 'nd', ' v', 'al', 'e ', 'do', ' n', 'ot', ' m', 'ee', 't,', ' b', 'ut', ' t', 'he', ' c', 'hi', 'ld', 're', 'n ', 'of', ' m', 'en', ' d', 'o,', ' g', 'oo', 'd ', 'an', 'd ', 'ba', 'd.', '\\ni', 'n ', 'th', 'is', ' w', 'ay', ' a', ' s', 'ho', 'em', 'ak', 'er', ' a', 'nd', ' a', ' t']\n",
      "\n",
      "processing file stories\\083.txt\n",
      "Data size (chars) (document 82) 5480\n",
      "Sample string ['\\th', 'an', 's ', 'th', 'e ', 'he', 'dg', 'eh', 'og', '\\n\\n', 'th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' c', 'ou', 'nt', 'ry', ' m', 'an', ' w', 'ho', ' h', 'ad', ' m', 'on', 'ey', ' a', 'nd', ' l', 'an', 'd ', 'in', ' p', 'le', 'nt', 'y,', ' b', 'ut', '\\nh', 'ow', 'ev', 'er', ' r', 'ic', 'h ']\n",
      "\n",
      "processing file stories\\084.txt\n",
      "Data size (chars) (document 83) 658\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' m', 'ot', 'he', 'r ', 'wh', 'o ', 'ha', 'd ', 'a ', 'li', 'tt', 'le', ' b', 'oy', ' o', 'f ', 'se', 've', 'n ', 'ye', 'ar', 's ', 'ol', 'd,', ' w', 'ho', '\\nw', 'as', ' s', 'o ', 'ha', 'nd', 'so', 'me', ' a', 'nd', ' l', 'ov', 'ab', 'le', ' t', 'ha']\n",
      "\n",
      "processing file stories\\085.txt\n",
      "Data size (chars) (document 84) 5989\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' y', 'ou', 'ng', ' f', 'el', 'lo', 'w ', 'wh', 'o ', 'ha', 'd ', 'le', 'ar', 'nt', ' t', 'he', ' t', 'ra', 'de', ' o', 'f ', 'lo', 'ck', 'sm', 'it', 'h,', '\\na', 'nd', ' t', 'ol', 'd ', 'hi', 's ', 'fa', 'th', 'er', ' h', 'e ', 'wo', 'ul', 'd ', 'no']\n",
      "\n",
      "processing file stories\\086.txt\n",
      "Data size (chars) (document 85) 8758\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' k', 'in', 'g ', 'wh', 'o ', 'ha', 'd ', 'a ', 'li', 'tt', 'le', ' b', 'oy', ' i', 'n ', 'wh', 'os', 'e ', 'st', 'ar', 's\\n', 'it', ' h', 'ad', ' b', 'ee', 'n ', 'fo', 're', 'to', 'ld', ' t', 'ha', 't ', 'he', ' s']\n",
      "\n",
      "processing file stories\\087.txt\n",
      "Data size (chars) (document 86) 3109\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' p', 'ri', 'nc', 'es', 's ', 'wh', 'o ', 'wa', 's ', 'ex', 'tr', 'em', 'el', 'y ', 'pr', 'ou', 'd.', ' i', 'f ', 'a\\n', 'wo', 'oe', 'r ', 'ca', 'me', ' s', 'he', ' g', 'av', 'e ', 'hi', 'm ', 'so', 'me', ' r', 'id']\n",
      "\n",
      "processing file stories\\088.txt\n",
      "Data size (chars) (document 87) 1365\n",
      "Sample string ['a ', 'ta', 'il', 'or', \"'s\", ' a', 'pp', 're', 'nt', 'ic', 'e ', 'wa', 's ', 'tr', 'av', 'el', 'in', 'g ', 'ab', 'ou', 't ', 'th', 'e ', 'wo', 'rl', 'd ', 'in', ' s', 'ea', 'rc', 'h ', 'of', '\\nw', 'or', 'k,', ' a', 'nd', ' a', 't ', 'on', 'e ', 'ti', 'me', ' h', 'e ', 'co', 'ul', 'd ', 'fi', 'nd']\n",
      "\n",
      "processing file stories\\089.txt\n",
      "Data size (chars) (document 88) 4538\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' o', 'n ', 'a ', 'ti', 'me', ' a', ' s', 'ol', 'di', 'er', ' w', 'ho', ' f', 'or', ' m', 'an', 'y ', 'ye', 'ar', 's ', 'ha', 'd ', 'se', 'rv', 'ed', ' t', 'he', '\\nk', 'in', 'g ', 'fa', 'it', 'hf', 'ul', 'ly', ', ', 'bu', 't ', 'wh', 'en', ' t', 'he', ' w']\n",
      "\n",
      "processing file stories\\090.txt\n",
      "Data size (chars) (document 89) 345\n",
      "Sample string ['on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' t', 'he', 're', ' w', 'as', ' a', ' c', 'hi', 'ld', ' w', 'ho', ' w', 'as', ' w', 'il', 'lf', 'ul', ', ', 'an', 'd ', 'wo', 'ul', 'd ', 'no', 't ', 'do', '\\nw', 'ha', 't ', 'he', 'r ', 'mo', 'th', 'er', ' w', 'is', 'he', 'd.', '  ', 'fo', 'r ', 'th']\n",
      "\n",
      "processing file stories\\091.txt\n",
      "Data size (chars) (document 90) 5460\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' k', 'in', \"g'\", 's ', 'so', 'n,', ' w', 'ho', ' w', 'as', ' n', 'o ', 'lo', 'ng', 'er', ' c', 'on', 'te', 'nt', ' t', 'o ', 'st', 'ay', ' a', 't\\n', 'ho', 'me', ' i', 'n ', 'hi', 's ', 'fa', 'th', 'er', \"'s\", ' h', 'ou', 'se', ', ', 'an', 'd ', 'as']\n",
      "\n",
      "processing file stories\\092.txt\n",
      "Data size (chars) (document 91) 6854\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' y', 'ou', 'ng', ' h', 'un', 'ts', 'ma', 'n ', 'wh', 'o ', 'we', 'nt', ' i', 'nt', 'o ', 'th', 'e ', 'fo', 're', 'st', ' t', 'o ', 'li', 'e ', 'in', '\\nw', 'ai', 't.', '  ', 'he', ' h', 'ad', ' a', ' f', 're', 'sh', ' a', 'nd', ' j', 'oy', 'ou', 's ']\n",
      "\n",
      "processing file stories\\093.txt\n",
      "Data size (chars) (document 92) 2314\n",
      "Sample string ['a ', 'po', 'or', ' s', 'er', 'va', 'nt', '-g', 'ir', 'l ', 'wa', 's ', 'on', 'ce', ' t', 'ra', 've', 'li', 'ng', ' w', 'it', 'h ', 'th', 'e ', 'fa', 'mi', 'ly', ' w', 'it', 'h ', 'wh', 'ic', 'h ', 'sh', 'e\\n', 'wa', 's ', 'in', ' s', 'er', 'vi', 'ce', ', ', 'th', 'ro', 'ug', 'h ', 'a ', 'gr', 'ea']\n",
      "\n",
      "processing file stories\\094.txt\n",
      "Data size (chars) (document 93) 1706\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' m', 'an', ' w', 'ho', ' h', 'ad', ' t', 'hr', 'ee', ' s', 'on', 's,', ' a', 'nd', ' n', 'ot', 'hi', 'ng', ' e', 'ls', 'e ', 'in', ' t', 'he', '\\nw', 'or', 'ld', ' b', 'ut', ' t', 'he', ' h', 'ou', 'se', ' i', 'n ', 'wh', 'ic', 'h ', 'he', ' l', 'iv']\n",
      "\n",
      "processing file stories\\095.txt\n",
      "Data size (chars) (document 94) 3229\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'a ', 'gr', 'ea', 't ', 'wa', 'r,', ' a', 'nd', ' t', 'he', ' k', 'in', 'g ', 'ha', 'd ', 'ma', 'ny', ' s', 'ol', 'di', 'er', 's,', ' b', 'ut', ' g', 'av', 'e ', 'th', 'em', '\\ns', 'ma', 'll', ' p', 'ay', ', ', 'so', ' s', 'ma', 'll', ' t', 'ha', 't ', 'th', 'ey', ' c']\n",
      "\n",
      "processing file stories\\096.txt\n",
      "Data size (chars) (document 95) 4954\n",
      "Sample string ['on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' l', 'iv', 'ed', ' a', ' m', 'an', ' a', 'nd', ' a', ' w', 'om', 'an', ' w', 'ho', ' s', 'o ', 'lo', 'ng', ' a', 's ', 'th', 'ey', ' w', 'er', 'e\\n', 'ri', 'ch', ' h', 'ad', ' n', 'o ', 'ch', 'il', 'dr', 'en', ', ', 'bu', 't ', 'wh', 'en', ' t', 'he']\n",
      "\n",
      "processing file stories\\097.txt\n",
      "Data size (chars) (document 96) 5732\n",
      "Sample string ['in', ' t', 'he', ' d', 'ay', 's ', 'wh', 'en', ' w', 'is', 'hi', 'ng', ' w', 'as', ' s', 'ti', 'll', ' o', 'f ', 'so', 'me', ' u', 'se', ', ', 'a ', 'ki', 'ng', \"'s\", ' s', 'on', ' w', 'as', '\\nb', 'ew', 'it', 'ch', 'ed', ' b', 'y ', 'an', ' o', 'ld', ' w', 'it', 'ch', ', ', 'an', 'd ', 'sh', 'ut']\n",
      "\n",
      "processing file stories\\098.txt\n",
      "Data size (chars) (document 97) 4334\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' p', 'oo', 'r ', 'ma', 'n ', 'wh', 'o ', 'ha', 'd ', 'fo', 'ur', ' s', 'on', 's,', ' a', 'nd', ' w', 'he', 'n ', 'th', 'ey', ' w', 'er', 'e ', 'gr', 'ow', 'n\\n', 'up', ', ', 'he', ' s', 'ai', 'd ', 'to', ' t', 'he', 'm,', ' \"', 'my', ' d', 'ea', 'r ']\n",
      "\n",
      "processing file stories\\099.txt\n",
      "Data size (chars) (document 98) 7090\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' w', 'om', 'an', ' w', 'ho', ' h', 'ad', ' t', 'hr', 'ee', ' d', 'au', 'gh', 'te', 'rs', ', ', 'th', 'e ', 'el', 'de', 'st', ' o', 'f ', 'wh', 'om', '\\nw', 'as', ' c', 'al', 'le', 'd ', 'on', 'e-', 'ey', 'e,', ' b', 'ec', 'au', 'se', ' s', 'he', ' h']\n",
      "\n",
      "processing file stories\\100.txt\n",
      "Data size (chars) (document 99) 1007\n",
      "Sample string ['\"g', 'oo', 'd-', 'da', 'y,', ' f', 'at', 'he', 'r ', 'ho', 'll', 'en', 'th', 'e.', '\" ', '\"m', 'an', 'y ', 'th', 'an', 'ks', ', ', 'pi', 'f-', 'pa', 'f-', 'po', 'lt', 'ri', 'e.', '\" ', '\"m', 'ay', ' i', '\\nb', 'e ', 'al', 'lo', 'we', 'd ', 'to', ' h', 'av', 'e ', 'yo', 'ur', ' d', 'au', 'gh', 'te']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "    with open(filename) as f:\n",
    "        data =  tf.compat.as_str(f.read())\n",
    "        data = data.lower()\n",
    "        data = list(data)\n",
    "    return data\n",
    "\n",
    "global documents\n",
    "documents = []\n",
    "num_files = 100\n",
    "for i in range(num_files):\n",
    "    print(\"processing file %s\" % os.path.join(dir_name, filenames[i]))\n",
    "    chars = read_data(os.path.join(dir_name, filenames[i]))\n",
    "\n",
    "    # break into bigrams\n",
    "    two_grams = [''.join(chars[ch_i:ch_i+2]) for ch_i in range(0, len(chars)-2, 2)]\n",
    "    # Create document\n",
    "    documents.append(two_grams)\n",
    "    print(\"Data size (chars) (document %d) %d\" % (i, len(two_grams)))\n",
    "    print(\"Sample string %s\\n\" % (two_grams[:50]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "449177 character found.\n",
      "Most common words (+UNK) [('e ', 15229), ('he', 15164), (' t', 13443), ('th', 13076), ('d ', 10687)]\n",
      "Least common words (+UNK) [('bj', 1), ('ii', 1), ('i?', 1), ('z ', 1), ('c.', 1), ('\"k', 1), ('pw', 1), ('f?', 1), (' z', 1), ('xq', 1), ('nm', 1), ('m?', 1), ('\\t\"', 1), ('\\tw', 1), ('tz', 1)]\n",
      "Sample data [15, 28, 86, 23, 3, 95, 74, 11, 2, 16]\n",
      "Sample data [22, 156, 25, 37, 82, 185, 43, 9, 90, 19]\n",
      "Vocabulary:  544\n"
     ]
    }
   ],
   "source": [
    "# Build dictionaries\n",
    "# dictionary: maps a string word to an ID (e.g. {I:0, like:1, to:2, go:3, school:4})\n",
    "# reverse_dictionary: maps an ID to a string word (e.g. {0:I, 1:like, 2:to, 3:go, 4:school}\n",
    "# count: List of list of (word, frequency) elements (e.g. [(I,1),(like,1),(to,2),(go,1),(school,1)]\n",
    "# data : Contain the string of text we read, where string words are replaced with word IDs (e.g. [0, 1, 2, 3, 2, 4])\n",
    "\n",
    "def build_dataset(documents):\n",
    "    chars = []\n",
    "    # list of lists\n",
    "    data_list = []\n",
    "\n",
    "    for d in documents:\n",
    "        chars.extend(d)\n",
    "    print('%d character found.' % len(chars))\n",
    "\n",
    "    count = []\n",
    "    # bigrams sorted by their frequency\n",
    "    count.extend(collections.Counter(chars).most_common())\n",
    "\n",
    "    # Create dict map word to id by given the current length of the dictionary\n",
    "    # UNK is for two rare word\n",
    "    dictionary = dict({'UNK': 0})\n",
    "    for char, c in count:\n",
    "        # Only add if its frequency is more than 10\n",
    "        if c > 10:\n",
    "            dictionary[char] = len(dictionary)\n",
    "    unk_count = 0\n",
    "    # replace word with id of word\n",
    "    for d in documents:\n",
    "        data = list()\n",
    "        for char in d:\n",
    "            # if word in dictionary use the id of word\n",
    "            # otherwise use id of UNK\n",
    "            if char in dictionary:\n",
    "                index = dictionary[char]\n",
    "            else:\n",
    "                index = dictionary['UNK']\n",
    "                unk_count += 1\n",
    "            data.append(index)\n",
    "        data_list.append(data)\n",
    "\n",
    "    # dict map id to word\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data_list, count, dictionary, reverse_dictionary\n",
    "\n",
    "data_list, count, dictionary, reverse_dictionary = build_dataset(documents)\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Least common words (+UNK)', count[-15:])\n",
    "print('Sample data', data_list[0][:10])\n",
    "print('Sample data', data_list[1][:10])\n",
    "print('Vocabulary: ',len(dictionary))\n",
    "vocabulary_size = len(dictionary)\n",
    "del documents  # To reduce memory."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Unrolled index 0\n",
      "\tInputs:\n",
      "\te  (1), \tki (131), \t d (48), \t w (11), \tbe (70), \n",
      "\tOutput:\n",
      "\tli (98), \tng (33), \tau (195), \ter (14), \tau (195), \n",
      "\n",
      "Unrolled index 1\n",
      "\tInputs:\n",
      "\tli (98), \tng (33), \tau (195), \ter (14), \tau (195), \n",
      "\tOutput:\n",
      "\tve (41), \t\n",
      "w (169), \tgh (106), \te  (1), \tti (112), \n",
      "\n",
      "Unrolled index 2\n",
      "\tInputs:\n",
      "\tve (41), \t\n",
      "w (169), \tgh (106), \te  (1), \tti (112), \n",
      "\tOutput:\n",
      "\td  (5), \tho (62), \tte (61), \tal (84), \tfu (228), \n",
      "\n",
      "Unrolled index 3\n",
      "\tInputs:\n",
      "\td  (5), \tho (62), \tte (61), \tal (84), \tfu (228), \n",
      "\tOutput:\n",
      "\ta  (82), \tse (58), \trs (137), \tl  (57), \tl, (257), \n",
      "\n",
      "Unrolled index 4\n",
      "\tInputs:\n",
      "\ta  (82), \tse (58), \trs (137), \tl  (57), \tbe (70), \n",
      "\tOutput:\n",
      "\tki (131), \t d (48), \t w (11), \tbe (70), \tau (195), "
     ]
    }
   ],
   "source": [
    "class DataGeneratorOHE(object):\n",
    "    def __init__(self, text, batch_size, num_unroll):\n",
    "        # text bigrams by its id\n",
    "        self._text = text\n",
    "        # number of bigrams in text\n",
    "        self._text_size = len(self._text)\n",
    "        self._batch_size = batch_size\n",
    "        # Num unroll is the number of steps unroll the RNN\n",
    "        # in a single training step\n",
    "        self._num_unroll = num_unroll\n",
    "        # Break text into several segments and the batch data\n",
    "        # is sampled by sampling a single item from a single segment\n",
    "        self._segments = self._text_size // self._batch_size\n",
    "        self._cursor = [offset * self._segments for offset in range(self._batch_size)]\n",
    "\n",
    "    def next_batch(self):\n",
    "        \"\"\"\n",
    "\n",
    "        :return: next batch of data\n",
    "        \"\"\"\n",
    "        # train inputs (one hot encoded) and train outputs (one hot encoded)\n",
    "        batch_data = np.zeros((self._batch_size, vocabulary_size,), dtype=np.float32)\n",
    "        batch_label = np.zeros((self._batch_size, vocabulary_size), dtype=np.float32)\n",
    "\n",
    "        for b in range(self._batch_size):\n",
    "            # reset back to begin when exceed batch_size\n",
    "            if self._cursor[b] + 1 >= self._text_size:\n",
    "                self._cursor[b] = b * self._segments\n",
    "\n",
    "            # Add text at cursor as input\n",
    "            batch_data[b, self._text[self._cursor[b]]] = 1.0\n",
    "\n",
    "            # Add preceding bigrams as the label\n",
    "            batch_label[b, self._text[self._cursor[b] + 1]] = 1.0\n",
    "\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "\n",
    "        return batch_data, batch_label\n",
    "\n",
    "    def unroll_batches(self):\n",
    "        \"\"\"\n",
    "\n",
    "        :return: a list of num_unroll batches required by training of the RNN\n",
    "        \"\"\"\n",
    "        unroll_data, unroll_labels = [], []\n",
    "        for ui in range(self._num_unroll):\n",
    "            data, labels = self.next_batch()\n",
    "            unroll_data.append(data)\n",
    "            unroll_labels.append(labels)\n",
    "        return unroll_data, unroll_labels\n",
    "\n",
    "    def reset_indices(self):\n",
    "        \"\"\"\n",
    "        Reset indices\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self._cursor = [offset * self._segments for offset in range(self._batch_size)]\n",
    "\n",
    "data_gen = DataGeneratorOHE(data_list[0][25:50], 5, 5)\n",
    "u_data_unroll, u_label_unroll = data_gen.unroll_batches()\n",
    "\n",
    "for ui, (data, label) in enumerate(zip(u_data_unroll, u_label_unroll)):\n",
    "    print('\\n\\nUnrolled index %d'%ui)\n",
    "    dat_ind = np.argmax(data,axis=1)\n",
    "    lbl_ind = np.argmax(label,axis=1)\n",
    "    print('\\tInputs:')\n",
    "    for sing_dat in dat_ind:\n",
    "        print('\\t%s (%d)'%(reverse_dictionary[sing_dat],sing_dat),end=\", \")\n",
    "    print('\\n\\tOutput:')\n",
    "    for sing_lbl in lbl_ind:\n",
    "        print('\\t%s (%d)'%(reverse_dictionary[sing_lbl],sing_lbl),end=\", \")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# LSTM: (each gate has three sets of weights (1 for current, 1 for previous, 1 for bias)\n",
    "#   Cell state\n",
    "#   Hidden state\n",
    "#   Input gate\n",
    "#   Forget gate\n",
    "#   Output gate\n",
    "\n",
    "# hyperparameter\n",
    "\n",
    "# neurons in hidden state\n",
    "num_nodes = 128\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# the number of time steps used in truncated BPTT\n",
    "num_unrolling = 50\n",
    "\n",
    "# regular\n",
    "dropout = .0\n",
    "\n",
    "filename_extension = ''\n",
    "if dropout>0.0:\n",
    "    filename_extension = '_dropout'\n",
    "\n",
    "filename_to_save = 'lstm'+filename_extension+'.csv'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "train_inputs, train_labels = [], []\n",
    "# Unroll training inputs\n",
    "for ui in range(num_unrolling):\n",
    "    train_inputs.append(tf.compat.v1.placeholder(tf.float32, shape=(batch_size, vocabulary_size),\n",
    "                                                 name='train_inputs_%d' % ui))\n",
    "    train_labels.append(tf.compat.v1.placeholder(tf.float32, shape=(batch_size, vocabulary_size),\n",
    "                                                 name='train_label_%d' % ui))\n",
    "\n",
    "# Validation data\n",
    "valid_inputs = tf.compat.v1.placeholder(tf.float32, shape=(1, vocabulary_size), name='valid_inputs')\n",
    "valid_labels = tf.compat.v1.placeholder(tf.float32, shape=(1, vocabulary_size), name='valid_labels')\n",
    "\n",
    "# Test data\n",
    "test_input = tf.compat.v1.placeholder(tf.float32, shape=(1, vocabulary_size), name='test_input')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\nguyen trung tam\\pycharmprojects\\deeplearning001\\venv\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "data": {
      "text/plain": "(<tf.Variable 'Variable_12:0' shape=(128, 544) dtype=float32>,\n <tf.Variable 'Variable_13:0' shape=(544,) dtype=float32>)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input gate - How much memory to write to cell state\n",
    "# connects current input to the input gate\n",
    "ix = tf.Variable(tf.compat.v1.truncated_normal(shape=(vocabulary_size, num_nodes), stddev=.02))\n",
    "# connects the previous hidden state to the input gate\n",
    "im = tf.Variable(tf.compat.v1.truncated_normal((num_nodes, num_nodes), stddev=.02))\n",
    "# bias of input gate\n",
    "ib = tf.Variable(tf.compat.v1.random_uniform((1, num_nodes),-0.02, 0.02))\n",
    "\n",
    "# Forget gate - how much memory to discard from cell state\n",
    "# connect current input to he forget gate\n",
    "fx = tf.Variable(tf.compat.v1.truncated_normal((vocabulary_size, num_nodes), stddev=.02))\n",
    "# connect the previous hidden state to the forget gate\n",
    "fm = tf.Variable(tf.random.truncated_normal((num_nodes, num_nodes), stddev=0.02))\n",
    "# bias of forget gate\n",
    "fb = tf.Variable(tf.random.uniform(shape=(1, num_nodes), minval=-0.02, maxval=0.02))\n",
    "\n",
    "# Candidate - compute the current cell state\n",
    "# connect current input to candidate\n",
    "cx = tf.Variable(tf.random.truncated_normal((vocabulary_size, num_nodes), stddev=0.02))\n",
    "# connect previous hidden state to the candidate\n",
    "cm = tf.Variable(tf.random.truncated_normal((num_nodes, num_nodes), stddev=0.02))\n",
    "# bias of candidate\n",
    "cb = tf.Variable(tf.random.uniform((1, num_nodes), minval=-0.02, maxval=0.02))\n",
    "\n",
    "# Output gate - how much memory to output from cell state\n",
    "ox = tf.Variable(tf.random.truncated_normal((vocabulary_size, num_nodes), stddev=.02))\n",
    "om = tf.Variable(tf.random.truncated_normal((num_nodes, num_nodes), stddev=.02))\n",
    "ob = tf.Variable(tf.random.uniform((1, num_nodes), minval=-0.02, maxval=0.02))\n",
    "\n",
    "# Softmax classifier weights and biases\n",
    "w = tf.Variable(tf.random.truncated_normal((num_nodes, vocabulary_size), stddev=0.02))\n",
    "b = tf.Variable(tf.random.uniform((vocabulary_size,), minval=-0.02, maxval=0.02))\n",
    "\n",
    "# Variables saving state across unrollings\n",
    "# hidden state\n",
    "saved_output = tf.Variable(tf.zeros((batch_size, num_nodes)), trainable=False, name=\"train_hidden\")\n",
    "# cell state\n",
    "saved_state = tf.Variable(tf.zeros((batch_size, num_nodes)), trainable=False, name=\"train_cell\")\n",
    "\n",
    "# Variables for validation\n",
    "saved_valid_output = tf.Variable(tf.zeros((1, num_nodes)), trainable=False, name='valid_hidden')\n",
    "saved_valid_state = tf.Variable(tf.zeros((1, num_nodes)), trainable=False, name='valid_cell')\n",
    "\n",
    "# Variables for testing\n",
    "saved_test_output = tf.Variable(tf.zeros((1, num_nodes)), trainable=False, name=\"test_hidden\")\n",
    "saved_test_state = tf.Variable(tf.zeros((1, num_nodes)), trainable=False, name='test_cell')\n",
    "\n",
    "w, b"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# cell computation\n",
    "def lstm_cell(i, o, state):\n",
    "    \"\"\"\n",
    "\n",
    "    :param i: input text for training\n",
    "    :param o: output is output from previous cell or hidden state\n",
    "    :param state: the previous cell state\n",
    "\n",
    "    forget_gate = σ(Wf · concat(h_t-1, x_t) + bf)\n",
    "    input_gate = σ(Wi · concat(h_t-1, x_t) + bi)\n",
    "    candidate = tanh(Wc · concat(h_t-1, x_t) + bc)\n",
    "    cell_state = forget_gate * previous_cell_state + input_gate * candidate\n",
    "    output_gate = σ(Wo · concat(h_t-1, x_t) + bo)\n",
    "    hidden_state = output_gate * tanh(cell_state)\n",
    "    :return: output (hidden state) and cell state\n",
    "    \"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    candidate_update = tf.tanh(tf.matmul(i, cx) + tf.matmul(o, cm) + cb)\n",
    "    state = forget_gate * state + input_gate * candidate_update\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "epsilon = 1e-10\n",
    "# keeps the calculated state outputs in all the unrollings to calculate loss\n",
    "outputs = []\n",
    "\n",
    "# will be update at each step of unrolling\n",
    "output = saved_output\n",
    "state = saved_state\n",
    "\n",
    "# Compute the hidden state (output) and cell state (state)\n",
    "for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    output = tf.nn.dropout(output, rate=dropout)\n",
    "    outputs.append(output)\n",
    "\n",
    "logits = tf.matmul(tf.concat(values=outputs, axis=0), w) + b\n",
    "\n",
    "train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# training perplexity\n",
    "train_perplexity_without_exp = tf.reduce_sum(\n",
    "    tf.concat(train_labels, axis=0) * -tf.math.log(tf.concat(train_prediction, axis=0) + epsilon)) / (num_unrolling *\n",
    "                                                                                                batch_size)\n",
    "\n",
    "# Validation\n",
    "valid_output, valid_state = lstm_cell(\n",
    "    valid_inputs, saved_valid_output, saved_valid_state\n",
    ")\n",
    "# Logits\n",
    "valid_logits = tf.matmul(valid_output, w) + b\n",
    "\n",
    "# Make sure state are updated\n",
    "with tf.control_dependencies([saved_valid_output.assign(valid_output),\n",
    "                              saved_valid_state.assign(valid_state)]):\n",
    "    valid_prediction = tf.nn.softmax(valid_logits)\n",
    "\n",
    "# Validation perplexity\n",
    "valid_perplexity_without_exp = tf.reduce_sum(valid_labels * -tf.math.log(valid_prediction + epsilon))\n",
    "\n",
    "# Testing\n",
    "test_output, test_state = lstm_cell(\n",
    "    test_input, saved_test_output, saved_test_state\n",
    ")\n",
    "# Make sure state are updated\n",
    "with tf.control_dependencies([saved_test_output.assign(test_output),\n",
    "                              saved_test_state.assign(test_state)]):\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(test_output, w) + b)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Loss\n",
    "with tf.control_dependencies([saved_output.assign(output),\n",
    "                              saved_state.assign(state)]):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(values=train_labels, axis=0),\n",
    "        logits=logits\n",
    "    ))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "lr_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "inc_lr_step = tf.compat.v1.assign(lr_step, lr_step + 1)\n",
    "tf_learning_rate = tf.compat.v1.train.exponential_decay(0.001, lr_step, decay_steps=1, decay_rate=.5)\n",
    "\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(tf_learning_rate)\n",
    "gradients, variables = zip(*optimizer.compute_gradients(loss))\n",
    "gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "\n",
    "optimizer = optimizer.apply_gradients(zip(gradients, variables))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "(<tf.Operation 'group_deps' type=NoOp>,\n <tf.Operation 'group_deps_1' type=NoOp>,\n <tf.Operation 'group_deps_2' type=NoOp>)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reset train\n",
    "reset_train_state = tf.group(tf.compat.v1.assign(saved_state, tf.zeros((batch_size, num_nodes))),\n",
    "                             tf.compat.v1.assign(saved_output, tf.zeros((batch_size, num_nodes))))\n",
    "\n",
    "# Reset valid state\n",
    "reset_valid_state = tf.group(tf.compat.v1.assign(saved_valid_state, tf.zeros((1, num_nodes))),\n",
    "                             tf.compat.v1.assign(saved_valid_output, tf.zeros((1, num_nodes))))\n",
    "\n",
    "# Reset test state\n",
    "reset_test_state = tf.group(saved_test_output.assign((tf.random.normal((1, num_nodes), stddev=.05))),\n",
    "                            saved_test_state.assign((tf.random.normal((1, num_nodes), stddev=.05))))\n",
    "\n",
    "reset_train_state, reset_valid_state, reset_test_state"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# break repetition in text\n",
    "# instead of getting the word with highest prediction, we sample randomly\n",
    "# where the probability of being selected given by their prediction prob\n",
    "\n",
    "def sample(distribution):\n",
    "    \"\"\"\n",
    "    Greedy Sampling\n",
    "    Pick the three best prob given by LSTM and sample one\n",
    "    of them with very high prob of pick the best one\n",
    "    :param distribution:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    best_indices = np.argsort(distribution)[-3:]\n",
    "    best_probs = distribution[best_indices] / np.sum(distribution[best_indices])\n",
    "    best_idx = np.random.choice(best_indices, p=best_probs)\n",
    "\n",
    "    return best_idx"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# Train LSTM\n",
    "# For each document train LSTM with steps_per_document steps\n",
    "# And then generate some text from random picked bigram\n",
    "decay_threshold = 5\n",
    "# If valid perplexity does not decrease for decay_threshold\n",
    "# then decrease the learning rate\n",
    "# perplexity increase count\n",
    "decay_count = 0\n",
    "\n",
    "min_perplexity = 1e10\n",
    "\n",
    "def decay_learning_rate(session, perplexity):\n",
    "    global min_perplexity, decay_count, min_perplexity\n",
    "    if perplexity < min_perplexity:\n",
    "        decay_count = 0\n",
    "        min_perplexity = perplexity\n",
    "    else:\n",
    "        decay_count += 1\n",
    "\n",
    "    if decay_count >= decay_threshold:\n",
    "        print(\"Reducing learning rate...\")\n",
    "        decay_count = 0\n",
    "        session.run(inc_lr_step)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize global variables ...\n"
     ]
    }
   ],
   "source": [
    "# Training Validation and Generation\n",
    "\n",
    "num_steps = 20\n",
    "steps_per_document = 100\n",
    "valid_summary = 1\n",
    "train_doc_count = 100\n",
    "docs_per_step = 10\n",
    "\n",
    "# Train perplexity over time\n",
    "train_perplexity = []\n",
    "valid_perplexity = []\n",
    "\n",
    "session = tf.compat.v1.InteractiveSession()\n",
    "\n",
    "# Initialize variables\n",
    "print(\"Initialize global variables ...\")\n",
    "tf.compat.v1.global_variables_initializer().run()\n",
    "\n",
    "# mean loss\n",
    "average_loss = 0.0\n",
    "\n",
    "# Use first 10 documents for validation\n",
    "\n",
    "doc_ids = []\n",
    "for di in range(num_files):\n",
    "    if len(data_list[di]) > 10 * steps_per_document:\n",
    "        doc_ids.append(di)\n",
    "    if len(doc_ids) == 10:\n",
    "        break\n",
    "\n",
    "data_gens = []\n",
    "valid_gens = []\n",
    "\n",
    "for fi in range(num_files):\n",
    "    # Get all bigrams if document id is not in the validation document\n",
    "    if fi not in doc_ids:\n",
    "        data_gens.append(DataGeneratorOHE(data_list[fi], batch_size=batch_size, num_unroll=num_unrolling))\n",
    "    # if document is in the validation doc ids, only get the up to steps_per_document bigrams\n",
    "    # and use the last steps_per_document as validation\n",
    "    else:\n",
    "        data_gens.append(DataGeneratorOHE(data_list[fi][:-steps_per_document], batch_size, num_unrolling))\n",
    "        # Validation data generator\n",
    "        valid_gens.append(DataGeneratorOHE(data_list[fi][-steps_per_document:], 1, 1))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------Training step 0 ------------------------\n",
      "(47), (74), (94), (50), (58), (9), (7), (66), (62), (76), \n",
      "Step: 1 Average loss 4.307, Perplexity 74.218\n",
      "Valid perplexity: 71.261\n",
      "\n",
      "========================New text========================\n",
      "awetar, and\n",
      "and not the will then the and the young to the begar, the the young to then eat to the night, and not the young wrens the will, and said, and the young that, and said, but that to the and saw hen and and and anver the young to to the\n",
      "and the\n",
      "and sat the\n",
      "young the young to look the young to the fare to to the will the fardoing then to the young wren, and said, and that the fore to the begand said, and said, but the\n",
      "and the and the\n",
      "young to the beached, and said, and not that will the young wrat to come, and the will, and the young, to the to the begand the young, the begand not the young to then the will wast the will wast the\n",
      "will, and the and said, then that beare ing and the withe and dren, and the young the with to the\n",
      "and said, but, and begand not that the for as and said,\n",
      "and not the and dren, but, then that the begand that sunt to to the young the will, and saw the young wrent the and dren, and that the fordon.  and the will that beare in then the young the young to the----------------------------\n",
      "\n",
      "------------------------Training step 1 ------------------------\n",
      "(69), (79), (71), (44), (90), (29), (58), (55), (0), (98), \n",
      "Step: 2 Average loss 3.039, Perplexity 20.878\n",
      "Valid perplexity: 48.560\n",
      "\n",
      "========================New text========================\n",
      "\n",
      "foor cold all to wo-eyes, and whon heare to the sher their two-eyes, and three-eye and the knoth it, and that shome, and the came their wite and took ther took they were in and took and with the tree with the tround be the tooker forthe her and beautiful beeyy and three-eye and that then said to he with\n",
      "othereatere and the knother.  the knother that to the knight agan that two-eyes, \"three-eye ound that the knight, her, what be such her two-eyes, and the came their, and the tree the with beauseeyed the well been streen to the whold some me to her and said two-eyes, and so the her, again and they three-eyes with her, and two-eyes wants and saw and her the knot and should became, and said, her the knoth her two-e an, foreat, and the tree with the treach of the known beautime, and beautireeyes, and the know was shome, forter, and been shapple, for two eas from more you been there, and they to her and said to the treat to her her they were in took they the tree to the tree wash theirther to----------------------------\n",
      "\n",
      "------------------------Training step 2 ------------------------\n",
      "(46), (76), (71), (40), (53), (42), (34), (85), (80), (49), \n",
      "Step: 3 Average loss 2.588, Perplexity 13.297\n",
      "Valid perplexity: 44.783\n",
      "\n",
      "========================New text========================\n",
      "ngy asked at out, i wind the fatest with he self who his cake.  when his had said on had now saw that to down told but wast a man of a whous wit we kingdow, and the fore to be on that the king's whis bestere the king wholer him make a with the king with his madeyou come of them, and on where if the sent wenh his little grey\n",
      "was to begge, and the kingdom the man becould he was she it, to the you had to his good, and and the\n",
      "king's death, dummling inherited his kingdom and little give his the ship, who her come said, and on hed watere.\n",
      "ed, and thent he was to the kind say the that and made whe winter hearrited, and he said he sat you she could to a land and that with the king's saw the mould with hing, and not down and began was see do once, and made whom he have his that whe wout stooking that and ling the this kind and began whe was cake to day this the king was as well there for whom he had giving in the have his kingdom and lived to take, and but to there said servant to eat into the c----------------------------\n",
      "\n",
      "------------------------Training step 3 ------------------------\n",
      "(3), (19), (64), (58), (2), (44), (94), (57), (71), (33), \n",
      "Step: 4 Average loss 2.172, Perplexity 8.777\n",
      "Valid perplexity: 42.862\n",
      "\n",
      "========================New text========================\n",
      "orself them to the fown him up in the hower, all\n",
      "the mad long coorset, if you\n",
      "all theyese it, answered the fox.  his did the little taken, and a did nothingerarrible the said to his\n",
      "pocket of and willin to becan it, answer stroness the fox her.  the settle took him his fathe still be dred to the fre first his child furwer,\n",
      "and littly becarried it, howered the served on her little, and the fox let the fox let him go ander, and him up the kecn stong.            stm stree three had too, the fart melike that the mitter to\n",
      "the conten and there, it in\n",
      "the for them all set more that nothing with our ing he castly in the powls in the yard the took the yard the proserved he was a said, her.  i will take you all belong tere the little,\n",
      "and himself\n",
      "carried him up again.  she helow the children he would not came anards, and shall the see blows were to his trucbly became.  at lar  said of reping of the childers, she would not sone it all, and before the frestere a littleas staing.  and the blows coul----------------------------\n",
      "\n",
      "------------------------Training step 4 ------------------------\n",
      "(37), (6), (36), (66), (14), (73), (55), (49), (9), (20), \n",
      "Step: 5 Average loss 2.123, Perplexity 8.353\n",
      "Valid perplexity: 60.249\n",
      "\n",
      "========================New text========================\n",
      " him, my king, you have\n",
      "it the could hom a bone the king, thound whole skein by across and said, his great as a good a drind with me.  thereuple he said in the bripthan, i must to one\n",
      "to little\n",
      "asng.  the king threlet that it stronger by the dood benid not bell thould baid the lion by and thout, washer, was come ble herselp in the mort would make a by and\n",
      "carriage on heard to your daughter with ther to my lastle\n",
      "came do light. his brother the even her daughter in the said the\n",
      "barridge, in earn the boar of his wife.  but she went out own he ack and to it to rest up, and the his hearnswere out for heart wasing so the began to side as itdede mand\n",
      "red toned the boned th, will it was toothed.  the boar, and drank to at rived it at into the king, he said to the ship, i will brother could not deny the deed, and\n",
      "was sewn up in a sack and drowned.  but the bones of the murdered\n",
      "man care laid to rest in a beautiful tomb in the churchyard.  the she her brothis came to ke its on, and as the ked tood----------------------------\n",
      "\n",
      "------------------------Training step 5 ------------------------\n",
      "(5), (93), (37), (54), (31), (81), (58), (95), (70), (40), \n",
      "Step: 6 Average loss 1.742, Perplexity 5.707\n",
      "Valid perplexity: 50.686\n",
      "\n",
      "========================New text========================\n",
      "s who happened, and saw\n",
      "the yourn who hand\n",
      "to had to be houghter father and on she told took her and\n",
      "her broke a peanight.  when and would had to doing, at you must take your his back.  who called herseen at sheps of her father.  i am the\n",
      "thered the wicked must took the fire.  and then the saw the oxen it all her to in take withouther and king's some of the rew itarried, er that this the king said, he wassevinged and she could havonger to her full\n",
      "speepick fire.  then the saw her on her, and where have had to you all her for fisher move.  all\n",
      "the shalls ind do lives yourself, but you areat fat teat his what he had hall to roke, and what the present the must to dumUNKnd brought to he eing three of here took man.  when th tole to be\n",
      "asked his so the mangethere, asked where here well it, all hus wand, and she concees her that you alsed and her happens in the must before. her he said, his eving, when her to die.  so she was to me in the rous.  whome fat nothing fore to the fordinand the faiti----------------------------\n",
      "\n",
      "------------------------Training step 6 ------------------------\n",
      "(43), (32), (48), (81), (98), (39), (28), (1), (51), (2), \n",
      "Step: 7 Average loss 1.712, Perplexity 5.543\n",
      "Valid perplexity: 38.977\n",
      "\n",
      "========================New text========================\n",
      "that the king warms of she.',  'not the your cangth, and the youther will not home.' which came him ' to the young contle.  they gave a chand as an heare the old lirrow, and a maidend together, and what the young father them fire was one warms of then as wenting father had her home, there strung, and the youth went to they was to driven the your comes on her a carried home, and say on the fire, and who asked to shut his brine together, and\n",
      "saw now, and took him in the marning the they said 'ife,  'you have i cannothinger, and the queen felting\n",
      "into at his brought and whate they have\n",
      "hare never.  showed threw that for it are, tail of toom to have and had i wi the fire, but he could hus and had sheled, but shudder said her\n",
      "'one and said he.  'i have had a find began to beauder.  then she was a long as take and seen arming\n",
      "ree became again.  the oxe other chave his and lat, and that you will spirs and as the thang nothinger, said he,  'and a when she said h 'it out the tree, and the crow th----------------------------\n",
      "\n",
      "------------------------Training step 7 ------------------------\n",
      "(66), (21), (33), (75), (99), (67), (43), (6), (5), (63), \n",
      "Step: 8 Average loss 1.757, Perplexity 5.798\n",
      "Valid perplexity: 47.600\n",
      "\n",
      "========================New text========================\n",
      "hands come forest, and said, what had will him and raid, asked i have you are little down, whon as he wishou wed him.  then the find again.  in the foreat, and as when the forth and the king's daughter.  and last had stay heir, answered to go out for the which he have standed a goling will\n",
      "he has still of the\n",
      "jew has and to be are in to his, and the father said he, and the father that go of my dearestill the forest, where you will go an a great foldid with the long was stan, and that if you was still been and\n",
      "begs  to father, and to the light hand into at the shagest, they to histed his stone to his whole he\n",
      "has should bed the from when the the had long is his ove, and bridderfore.  said, hear is were bried him.  they were hadly, and then i will gold.\n",
      "\n",
      "shree daynithers, answered her and to his lew hased, said, but\n",
      "heound a greold hight\n",
      "of the from himself\n",
      "away by the great joich and whole can age beard the brother, whole which whole which which was comhind again.  at laid the other with ----------------------------\n",
      "\n",
      "------------------------Training step 8 ------------------------\n",
      "(80), (78), (84), (49), (18), (13), (86), (93), (55), (69), \n",
      "Step: 9 Average loss 1.680, Perplexity 5.367\n",
      "Valid perplexity: 39.948\n",
      "\n",
      "========================New text========================\n",
      "t his going on the whole down to be eat unith himself begain, but where had still restick the maiden and rouse saw, he had sourted him to the beenour in the mornin on the wore that she wall largerman and it was about at last the hand when\n",
      "the man.  ah, the golden the servan and days, shand he have a daughter, was to fire and three day, and they were he sen, as a lay said all that them.  when the king was infore\n",
      "the giantered him to be and the gold chesched the measuch it on.  they heard to the golden the man be beach other, they carried to her, that the horsebraved\n",
      "his lew in the did,\n",
      "and the man be could sho was not like here it this.  in her,\n",
      "and he had a vistro.  and then went to the golden and saw had seed, and we see me freor, and there so toosed the here drived with time that shome must her.\"\n",
      "\n",
      "when they.  \"i will stall nothing every together happeny thing thaty he had gone and said, i thes we have manted from\n",
      "thinging and he came to the gold him, and had she sound all the diddle an----------------------------\n",
      "\n",
      "------------------------Training step 9 ------------------------\n",
      "(67), (79), (71), (63), (9), (94), (30), (83), (97), (22), \n",
      "Step: 10 Average loss 1.501, Perplexity 4.485\n",
      "Valid perplexity: 48.219\n",
      "\n",
      "========================New text========================\n",
      "god to the coor of saway into\n",
      "hile, and the and the king's herds come and thatch fromes a queen said, if your deare in\n",
      "into god's eater into should, and wife her mother was a patcen his brothers came again, and he had came that they were staid him to each that it they were all were over\n",
      "the and together, which was a shipe was sign bens the sitter againg, and cange came and that, the place\n",
      "and then answed, but if you cannot himself, and said to these the whate rester, but wife you wild, splated him, and has dear him to the maiden was that the devind so out with my heart in the world was again, and said,\n",
      "the answer that is no be cangers with my that they liver so she aljoicing come a des littly the man, and took thre, and had fall on coversted me.\"  then the tur whold\n",
      "she came and ther, then all tell that he have his sleep.  then shelor again, and hild came in theird his leaster in the wife.  that the king was inlying, and the knother wept her mother, she and quiell whost\n",
      "again, and\n",
      "little----------------------------\n",
      "\n",
      "------------------------Training step 10 ------------------------\n",
      "(19), (85), (90), (12), (42), (31), (81), (97), (43), (68), \n",
      "Step: 11 Average loss 1.781, Perplexity 5.936\n",
      "Valid perplexity: 41.814\n",
      "\n",
      "========================New text========================\n",
      " my head and said, never come to them, what is had a some to his was a let himself behind her that, and he saw all dived him and and beater with he did not besible and will your me then his cherUNKisted without hind, but they have to his wife and the ground, and said, and distered hisseathers, and said, the king came in that i could have the giant and hissed without sister.  when the ship were, and and fortume had and comelver head, that he did\n",
      "not low agains off at his were, and had on the piek ith them, and went to it, and began forms not in the\n",
      "manikin, and said, he wifte in he tood not only in a of the places to all his name, but ano ming, and would have you that name, and was his put into the pole off, and head here lived up, she\n",
      "had gome hisself, and shamried out her had been him if i have his well, and put on yes, son, and she wented to his never come?  then he\n",
      "wept of her the for it, it and courcalsented away said, you shall be answed to but once became shall never they trice and ----------------------------\n",
      "\n",
      "------------------------Training step 11 ------------------------\n",
      "(65), (11), (83), (96), (10), (33), (53), (12), (68), (43), \n",
      "Step: 12 Average loss 1.505, Perplexity 4.505\n",
      "Valid perplexity: 48.660\n",
      "\n",
      "Reducing learning rate...\n",
      "========================New text========================\n",
      "r and said, why and\n",
      "the fox left tog, her aged wanget will\n",
      "ever had happed again.\n",
      "\n",
      "what\n",
      "shrose in the great condtry home, and assacess,\n",
      "UNK  now one knew he was not know what not know is now laid in a yourself, she if you mare you give me, and shout to the three threw could have the the never conrayes, but he was on the girl.  then sho he alard of the said kill, now hansing myself, said his the open you that name, but when you first a miller's saw the bring said me, if and dran befound the names of the queen back again, and said, then the queen into age\n",
      "more for that he had one of his als were in his whole leg went in the manntant on told his\n",
      "there, and raile man my name, the\n",
      "perelet his\n",
      "beinn alive again four beaut my maniking, there goldenter a master'shing and liftle golding, and said, she i has two place i with him,, but i am,\n",
      "but so mad is\n",
      "what his deed the then the queen, when she wanted to the heads on the thought to i more himself in the broom with every of him, and you the second----------------------------\n",
      "\n",
      "------------------------Training step 12 ------------------------\n",
      "(13), (41), (69), (99), (39), (8), (22), (10), (74), (89), \n",
      "Step: 13 Average loss 1.863, Perplexity 6.444\n",
      "Valid perplexity: 56.173\n",
      "\n",
      "========================New text========================\n",
      "a white her mother was was it was gold, and the kings, and there when that her had in that\n",
      "in the chand with at his left her platter, that he the great rest,\n",
      "been thereatend to the lead at hom in his\n",
      "poor, and goenwas ather and cond, and when the little that herself was obliged to go to\n",
      "the grave, and strike the arm with a rod, and when she had done that,\n",
      "it was drawn in, and then the child hao rest beaut the\n",
      "graves, but the latelight at his had never laid,\n",
      "that\n",
      "ill to do, and the second, and at last the child had rest best the child had go in earth her are with a, that had done mother, and that the kfore it, and then at last the child had rest qin,\n",
      "heaveled the devil it was mocked and the eatill had not with a realed over her her, and caust did not know when the most out with was plead, and in\n",
      "rade had gone with the leaver mothing,\n",
      "and work that into the firsts was do yond that it had then that in the child had not hosteal her arraight, and that he was becould not began to but a so\n",
      "deep----------------------------\n",
      "\n",
      "------------------------Training step 13 ------------------------\n",
      "(52), (54), (75), (5), (80), (99), (0), (17), (1), (6), \n",
      "Step: 14 Average loss 1.948, Perplexity 7.017\n",
      "Valid perplexity: 35.738\n",
      "\n",
      "========================New text========================\n",
      "t, she was cock in her.  he should not see down on the wook thred, she came to to better.\" \"a husband and to the hearthy of his trad-door the together.  your reat fare in the world, and when they she had been do in her, and was things.  she could not the breut and a she castle to heavens, the king's daughter there was her.  then they carriate so one he all is the\n",
      "cook went man doin the door and said to her, and said she, \"yes, they had\n",
      "bearn and councing, and to be toger it was leaver with her as the back they went out with you armself to much it was and at went out of the table, and he heard in his face.  then she thought to the devil with at that with her.  then the king who was not ceive will be this the took the sevening ans, what was standing the door from the table, and said, ano little\n",
      "man walk before you disty art was boutter, took not lived.  but the maiden said, she heart, and when they were awas to more a shorld on strang-whis beautiful\n",
      "trucperfore a plearly he who has some wh----------------------------\n",
      "\n",
      "------------------------Training step 14 ------------------------\n",
      "(47), (10), (23), (35), (34), (76), (58), (1), (29), (70), \n",
      "Step: 15 Average loss 1.916, Perplexity 6.794\n",
      "Valid perplexity: 33.870\n",
      "\n",
      "========================New text========================\n",
      "g together a fore ast her father she was to me, and was now his how headmithe goode of uuln beman,\n",
      "and when\n",
      "the king and he drew the\n",
      "brief, and the grearest back, and when i will go meme of that the friefore hing, and there she was good dead them, and she said 'my dry of the robber, you\n",
      "sat a handful of drew that the who took hing.  there and other the king once, and that decond take a foxest for there herself, she was and beautiful by years.  she saw that the king's homes was goosefore a little dreimered to the king and said she her eyes, they was all his once.  the person time king's didde armans.\n",
      "\n",
      "i came to my norrought a forest, and was ashed all the king's daughter i will have you here, when a firece when\n",
      "the robbers peaslet she peeped the felmes of god, he was for the warle.  so to the queen as all her back highe oven them was she castle from the was wedding to his home, and\n",
      "the peasinto the stand and said she, to hard well, and then he said to the girl one is yours were table and ----------------------------\n",
      "\n",
      "------------------------Training step 15 ------------------------\n",
      "(13), (16), (80), (99), (65), (83), (19), (4), (34), (48), \n",
      "Step: 16 Average loss 1.713, Perplexity 5.545\n",
      "Valid perplexity: 33.784\n",
      "\n",
      "========================New text========================\n",
      "u, when he went doing, and fone of the councilliUNKows whome the bread, and the twould are lighd to but went to herself, and that is not knke a great beautiful that he charced and the king has deept it without and began to slippers.  at she alked her delifete to a great offeat, and jumped through are away, the world had been them what\n",
      "are told, and had spokkly withought the kingdon, and then her the wint to\n",
      "a king on the\n",
      "ton at.\n",
      "\n",
      "then she jumped, but that she manched to the two maidle, and sonstood\n",
      "cittle ready half all there that which she carped a rought will betther's heart.  then the two three had peeping, and was a red all the forrowfuld could no longer keeping to bride so fally you have bace, and the bridegroom said, what when to hease, withful of pred of the window and said,\n",
      "if you better to the king, and donso and the wizard and\n",
      "coull, no gell and all began to a would a smelUNKful of pready had twen appeared to the tree.  then the kingdom the wizared and\n",
      "still his, and thought to b----------------------------\n",
      "\n",
      "------------------------Training step 16 ------------------------\n",
      "(1), (75), (56), (99), (77), (79), (76), (9), (54), (89), \n",
      "Step: 17 Average loss 1.379, Perplexity 3.970\n",
      "Valid perplexity: 41.718\n",
      "\n",
      "========================New text========================\n",
      "e was full of the child had he eathe,\n",
      "and no, seat her did,\n",
      "      w cooking, and did not leave off unce asure, as the king was shall not be earth that the preace.  nothing came that he for a her pleryest and ler his had hed, he were once her arm\n",
      "still, she gat of theach other washer hall shall sate all the fours lighted in a may, and god the\n",
      "mother herself was obliged to go to\n",
      "the grave, and strike the arm with a rod, and when she had done that,\n",
      "it was drawn in, and then at last the child had rest beneath the\n",
      "ground.\n",
      "\n",
      "the king had deave there, but which was they had done that,\n",
      "it was drawn in, and then at last the child had rest beneath the\n",
      "ground.  she was to suck a hunt, he was forthere, and it was that was spread.  nothers great gravenout to the late the king, and had red-cap.  at the fatter her gold on his lettle.  then the king balked in hou.  did not red and she had dead the wind great death,\n",
      "and took the frothere, when the king only streed herse, and had rest bed.\n",
      "the king, the wa----------------------------\n",
      "\n",
      "------------------------Training step 17 ------------------------\n",
      "(20), (61), (48), (4), (21), (42), (1), (45), (43), (78), \n",
      "Step: 18 Average loss 2.028, Perplexity 7.602\n",
      "Valid perplexity: 33.393\n",
      "\n",
      "========================New text========================\n",
      "ed, and the manikin what was it also the man servant him that he was all his aunfered to be that with her beautiful help of it.  so may ast ould not belp thought, and who is no other as the fox and this the two whis leas the fair, and there is straight, is become of my name.  this like afterwards asked, the prefect thither, and that you have you shalgive my and before her go sitter, answer, what are said, i taker by side, thought he would all your simpinto be ordered that he had burnt the window again a great and councing of a may their was agranger, and the fox got head.             you ween shing to the people cried the forest, and that he also a little hand, he hease that you will be eated her again of\n",
      "the goldegret out\n",
      "hom.  the people there in another, and as if you\n",
      "no name to time in heard all the millittle mout in he said to me, things on the\n",
      "house.\n",
      "\n",
      "then the trusts imat is in the whole severe in the sured, who had to be so\n",
      "to cominger.  there by\n",
      "the\n",
      "finebout the peasant, said the----------------------------\n",
      "\n",
      "------------------------Training step 18 ------------------------\n",
      "(3), (79), (90), (50), (20), (37), (40), (61), (12), (1), \n",
      "Step: 19 Average loss 1.813, Perplexity 6.131\n",
      "Valid perplexity: 31.958\n",
      "\n",
      "========================New text========================\n",
      "s beautiful man home and shaoped, and there alive toler the stonger that the from her little gave\n",
      "the boot.  he rentied\n",
      "them into\n",
      "the bargain, and he had not given allothe three ready, and then she came the councillorst for if he buited to the stood still of night, and felt on the virgin mary lighed me her so her woun he began to you will go to little round her to see the roanes on the\n",
      "good which he had have then they saw that it was to deave. there\n",
      "when she three done and the somenting the girl would be\n",
      "you shalloed in the\n",
      "great love one, and her dearr came to her to swie there, and the virgin mary took to little sold by her she\n",
      "had repledand fresh he will not heart began to be anted and they brew of a the forbit, and when he couldened all head with his the whole would be\n",
      "not been came with her with her of the millir's child ball brown her arms and loudly 'the\n",
      "queen when the little heaves, he was beaning the for me.  when they careed the queen and are in her by where it is not the child----------------------------\n",
      "\n",
      "------------------------Training step 19 ------------------------\n",
      "(85), (75), (34), (52), (4), (40), (64), (69), (71), (11), \n",
      "Step: 20 Average loss 1.915, Perplexity 6.788\n",
      "Valid perplexity: 25.826\n",
      "\n",
      "========================New text========================\n",
      ".\n",
      "she went into it out of him.  the\n",
      "witced again, and now that all thought at however.  so thay he furme.\" and then the brother, and bed it.  ther, he wanted the mantle,\n",
      "sat in the water, and they looked in time.  now now were his prenced and threw down,\n",
      "and when the woman said the my take a greats and had away there the was about to her in they were back and better arming tood, and croslost had were better and who had brought in the water, but the king was full oved.  it put out at the was\n",
      "into they went out of with her.  herself, i will not get out again.  so he castle anything to the great stretch out in and cantle heared.  the man.  then they came to them, but all the gir will not run inteway.  the witched upilt fetched in their with with, and has been lived again a shave be garmen aing their would not see into the hanseng that ere\n",
      "rested ther.  but on him that he had been to do.  that they were all gather and rust, and they had been not, who replied up the water,\n",
      "and their pies a ma----------------------------\n",
      "------------Saved variable------------\n"
     ]
    }
   ],
   "source": [
    "feed_dict = {}\n",
    "save_path = \"./my_model/my_saved_variable\"\n",
    "\n",
    "for step in range(num_steps):\n",
    "    print(\"\\n\" +  \"-\"*24 + \"Training step %d \" % step + \"-\"*24)\n",
    "    for di in np.random.permutation(train_doc_count)[:docs_per_step]:\n",
    "        doc_perplexity = 0\n",
    "        for doc_step_id in range(steps_per_document):\n",
    "            # Get set of unrolled batches\n",
    "            u_data, u_labels = data_gens[di].unroll_batches()\n",
    "\n",
    "            # Populate the feed dict by using each of the data batches\n",
    "            # present in the unrolled batches\n",
    "            for ui, (data, label) in enumerate(zip(u_data, u_labels)):\n",
    "                feed_dict[train_inputs[ui]] = data\n",
    "                feed_dict[train_labels[ui]] = label\n",
    "\n",
    "            # Run operation\n",
    "            _, l, step_perplexity = session.run([optimizer, loss, train_perplexity_without_exp],\n",
    "                                               feed_dict=feed_dict)\n",
    "            # Update perplexity\n",
    "            doc_perplexity += step_perplexity\n",
    "\n",
    "            # Update loss\n",
    "            average_loss += step_perplexity\n",
    "        print(\"(%d), \" % di, end='')\n",
    "        # resetting hidden state after processing a single document\n",
    "        # It's still questionable if this adds value in terms of learning\n",
    "        # One one hand it's intuitive to reset the state when learning a new document\n",
    "        # On the other hand this approach creates a bias for the state to be zero\n",
    "        # We encourage the reader to investigate further the effect of resetting the state\n",
    "        #session.run(reset_train_state) # resetting hidden state for each document\n",
    "\n",
    "    if (step + 1) % valid_summary == 0:\n",
    "        # average loss\n",
    "        average_loss = average_loss / (valid_summary * docs_per_step * steps_per_document)\n",
    "\n",
    "        print(\"\\nStep: %d Average loss %.3f, Perplexity %.3f\" % (step + 1, average_loss, np.exp(average_loss)))\n",
    "        train_perplexity.append(np.exp(average_loss))\n",
    "\n",
    "        # reset loss\n",
    "        average_loss = 0\n",
    "        valid_loss = 0\n",
    "\n",
    "        # Valid perplexity\n",
    "        for v_doc_id in range(10):\n",
    "            # divide by 2 due to bigrams\n",
    "            for v_step in range(steps_per_document // 2):\n",
    "                u_valid_data, u_valid_label = valid_gens[v_doc_id].unroll_batches()\n",
    "\n",
    "                # Run validation phase\n",
    "                v_perplexity_out = session.run([valid_perplexity_without_exp],\n",
    "                                           feed_dict={valid_inputs: u_valid_data[0], valid_labels:\n",
    "                                               u_valid_label[0]})\n",
    "                valid_loss += v_perplexity_out[0]\n",
    "\n",
    "            session.run(reset_valid_state)\n",
    "\n",
    "            # Reset validation date generator curosr\n",
    "            valid_gens[v_doc_id].reset_indices()\n",
    "\n",
    "        v_perplexity = np.exp(valid_loss / (steps_per_document * 10.0 // 2))\n",
    "        print(\"Valid perplexity: %.3f\\n\" % v_perplexity)\n",
    "        valid_perplexity.append(v_perplexity)\n",
    "        decay_learning_rate(session, v_perplexity)\n",
    "\n",
    "        # Generate new text\n",
    "        # Generate 500 bigrams with one segment\n",
    "        segments_to_generate = 1\n",
    "        chars_in_segment = 500\n",
    "\n",
    "        for _ in range(segments_to_generate):\n",
    "            print(\"=\"*24 + \"New text\" + \"=\"*24 )\n",
    "\n",
    "            # start with random word\n",
    "            test_word = np.zeros((1, vocabulary_size), dtype=np.float32)\n",
    "            rand_doc = data_list[np.random.randint(0, num_files)]\n",
    "            test_word[0, rand_doc[np.random.randint(0, len(rand_doc))]] = 1.0\n",
    "            print(reverse_dictionary[np.argmax(test_word[0])], end='')\n",
    "\n",
    "            # Generating words by feeding the previous prediction\n",
    "            # as current input in a recursive manner\n",
    "            for _ in range(chars_in_segment):\n",
    "                sample_pred = session.run(test_prediction, feed_dict={test_input: test_word})\n",
    "                next_ind = sample(sample_pred.ravel())\n",
    "                test_word = np.zeros((1, vocabulary_size), dtype=np.float32)\n",
    "                test_word[0, next_ind] = 1.0\n",
    "                print(reverse_dictionary[next_ind], end='')\n",
    "\n",
    "            # Reset train state\n",
    "            session.run(reset_test_state)\n",
    "            print(\"-\" * 28)\n",
    "\n",
    "\n",
    "# Saved all variables in session into file to restore later\n",
    "print(\"-\" * 12 + \"Saved variable\" + \"-\" * 12)\n",
    "saver = tf.compat.v1.train.Saver()\n",
    "saver.save(session, save_path, global_step=num_steps)\n",
    "\n",
    "session.close()\n",
    "#\n",
    "# with open(filename_to_save, 'wt') as f:\n",
    "#     writer = csv.writer(f, delimiter=',')\n",
    "#     writer.writerow(train_perplexity)\n",
    "#     writer.writerow(valid_perplexity)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# Beam search\n",
    "beam_length = 5\n",
    "beam_neighbors = 5\n",
    "sample_beam_inputs = [tf.compat.v1.placeholder(tf.float32, shape=(1, vocabulary_size)) for _ in range(beam_neighbors)]\n",
    "\n",
    "best_beam_index = tf.compat.v1.placeholder(shape=None, dtype=tf.int32)\n",
    "best_neighbor_beam_indices = tf.compat.v1.placeholder(shape=(beam_neighbors,), dtype=tf.int32)\n",
    "\n",
    "# output of each beam\n",
    "saved_sample_beam_output = [tf.Variable(tf.zeros((1, num_nodes))) for _ in range(beam_neighbors)]\n",
    "# state of each beam\n",
    "saved_sample_beam_state = [tf.Variable(tf.zeros((1, num_nodes))) for _ in range(beam_neighbors)]\n",
    "\n",
    "# Reset the sample beam states\n",
    "reset_sample_beam_state = tf.group(\n",
    "    *[saved_sample_beam_output[vi].assign(tf.zeros((1, num_nodes))) for vi in range(beam_neighbors)],\n",
    "    *[saved_sample_beam_state[vi].assign(tf.zeros((1, num_nodes))) for vi in range(beam_neighbors)]\n",
    ")\n",
    "\n",
    "# Stack to perform gather\n",
    "stacked_beam_outputs = tf.stack(saved_sample_beam_output)\n",
    "stacked_beam_states = tf.stack(saved_sample_beam_state)\n",
    "\n",
    "# The beam states for each beam (there are beam_neighbor-many beams) needs to be updated at every depth of tree\n",
    "# Consider an example where you have 3 classes where we get the best two neighbors (marked with star)\n",
    "#     a`      b*       c\n",
    "#   / | \\   / | \\    / | \\\n",
    "#  a  b c  a* b` c  a  b  c\n",
    "# Since both the candidates from level 2 comes from the parent b\n",
    "# We need to update both states/outputs from saved_sample_beam_state/output to have index 1 (corresponding to parent b)\n",
    "update_sample_beam_state = tf.group(\n",
    "    *[saved_sample_beam_output[vi].assign(tf.gather_nd(stacked_beam_outputs, [best_neighbor_beam_indices[vi]]))\n",
    "      for vi in range(beam_neighbors)],\n",
    "    *[saved_sample_beam_state[vi].assign(tf.gather_nd(stacked_beam_states, [best_neighbor_beam_indices[vi]]))\n",
    "      for vi in range(beam_neighbors)]\n",
    ")\n",
    "\n",
    "sample_beam_outputs, sample_beam_states = [], []\n",
    "for vi in range(beam_neighbors):\n",
    "    tmp_output, tmp_state = lstm_cell(\n",
    "        sample_beam_inputs[vi],\n",
    "        saved_sample_beam_output[vi],\n",
    "        saved_sample_beam_state[vi]\n",
    "    )\n",
    "    sample_beam_outputs.append(tmp_output)\n",
    "    sample_beam_states.append(tmp_state)\n",
    "\n",
    "\n",
    "# For a given set of beams, outputs a list of prediction vectors of size beam_neighbors\n",
    "# Each beam having the predictions for full vocabulary\n",
    "sample_beam_predictions = []\n",
    "for vi in range(beam_neighbors):\n",
    "    with tf.control_dependencies([saved_sample_beam_output[vi].assign(sample_beam_outputs[vi]),\n",
    "                                  saved_sample_beam_state[vi].assign(sample_beam_states[vi])]):\n",
    "        sample_beam_predictions.append(tf.nn.softmax(tf.matmul(sample_beam_outputs[vi], w) + b))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# Train LSTM on the available data and generate text using\n",
    "# the trained LSTM for several steps. From each document we extract text\n",
    "# for steps_per_document steps to train the LSTM on.\n",
    "\n",
    "# Learning rate schedule\n",
    "\n",
    "decay_threshold = 5\n",
    "decay_count = 0\n",
    "\n",
    "min_perplexity = 1e10\n",
    "\n",
    "def decay_learning_rate(session, v_perplexity):\n",
    "    global decay_count, decay_threshold, min_perplexity\n",
    "    if v_perplexity < min_perplexity:\n",
    "        decay_count = 0\n",
    "        min_perplexity = v_perplexity\n",
    "    else:\n",
    "        decay_count += 1\n",
    "\n",
    "    if decay_count >= decay_threshold:\n",
    "        print(\"\\t Reducing learning rate\")\n",
    "        decay_count = 0\n",
    "        session.run(inc_lr_step)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# Beam Prediction search\n",
    "test_word = None\n",
    "\n",
    "def get_beam_prediction(session):\n",
    "    \"\"\"\n",
    "    # Generating words within a segment with Beam Search\n",
    "    # To make some calculations clearer, we use the example as follows\n",
    "    # We have three classes with beam_neighbors=2 (best candidate denoted by *,\n",
    "    # second best candidate denoted by `)\n",
    "    # For simplicity we assume best candidate always have probability of 0.5 in output prediction\n",
    "    # second best has 0.2 output prediction\n",
    "    #           a`                   b*                   c                <--- root level\n",
    "    #    /     |     \\         /     |     \\        /     |     \\\n",
    "    #   a      b      c       a*     b`     c      a      b      c         <--- depth 1\n",
    "    # / | \\  / | \\  / | \\   / | \\  / | \\  / | \\  / | \\  / | \\  / | \\\n",
    "    # a b c  a b c  a b c   a*b c  a`b c  a b c  a b c  a b c  a b c       <--- depth 2\n",
    "    # So the best beams at depth 2 would be\n",
    "    # b-a-a and b-b-a\n",
    "    :param session:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    global test_word\n",
    "    global sample_beam_predictions\n",
    "    global update_sample_beam_state\n",
    "\n",
    "    # Calculate the candidates at the root level\n",
    "    feed_dict = {}\n",
    "    for b_n_i in range(beam_neighbors):\n",
    "        feed_dict.update({sample_beam_inputs[b_n_i]: test_word})\n",
    "\n",
    "    # Calculate sample predictions for all neighbors with the same starting word\n",
    "    # It is important to update the state for all instances of beam search\n",
    "    sample_preds_root = session.run(sample_beam_predictions, feed_dict=feed_dict)\n",
    "    sample_preds_root = sample_preds_root[0]\n",
    "\n",
    "    this_level_candidates = (np.argsort(sample_preds_root, axis=1).ravel()[::-1])[:beam_neighbors]\n",
    "\n",
    "    # probabilities of top k candidates is .5 and .2\n",
    "    this_level_probs = sample_preds_root[0, this_level_candidates]\n",
    "\n",
    "    # Update test sequence produced by each beam from the root level calculation\n",
    "    test_sequences = ['' for _ in range(beam_neighbors)]\n",
    "    for b_n_i in range(beam_neighbors):\n",
    "        test_sequences[b_n_i] += reverse_dictionary[this_level_candidates[b_n_i]]\n",
    "\n",
    "    # Make the calculations for the rest of the depth of the beam search tree\n",
    "    for b_i in range(beam_length-1):\n",
    "        # Candidate words for each beam\n",
    "        test_words = []\n",
    "        # Predicted word for each beam\n",
    "        pred_words = []\n",
    "\n",
    "        # computing feed_dict for the beam search (except root)\n",
    "        # feed dict should contain the best words found by the previous level of search\n",
    "\n",
    "        feed_dict = {}\n",
    "        for p_idx, pred_i in enumerate(this_level_candidates):\n",
    "            # Update the feed_dict for getting next predictions\n",
    "            test_words.append(np.zeros((1, vocabulary_size), dtype=np.float32))\n",
    "            test_words[p_idx][0, this_level_candidates[p_idx]] = 1.0\n",
    "\n",
    "            feed_dict.update({sample_beam_inputs[p_idx]:test_words[p_idx]})\n",
    "\n",
    "        # Calculation the predictions for all neighbors in beams search\n",
    "        # This is a list of vectors where each vector is the prediction vector for a certain beam\n",
    "        # For level 1 in our example, the prediction values for\n",
    "        #      b             a  (previous beam search results)\n",
    "        # [a,  b,  c],  [a,  b,  c] (current level predictions) would be\n",
    "        # [0.1,0.1,0.1],[0.5,0.2,0]\n",
    "        sample_preds_all_neighbors = session.run(sample_beam_predictions, feed_dict=feed_dict)\n",
    "\n",
    "        # Create a single vector with\n",
    "        # Making our example [0.1,0.1,0.1,0.5,0.2,0]\n",
    "        sample_preds_all_neighbors_concat = np.concatenate(sample_preds_all_neighbors, axis=1)\n",
    "\n",
    "        # Update this_level_candidates to be used for the next iter\n",
    "        # Update the probabilities for each beams with maximum value from above vector\n",
    "        this_level_candidates = np.argsort(sample_preds_all_neighbors_concat.ravel())[::-1][:beam_neighbors]\n",
    "\n",
    "        # In the example this would be [1,1]\n",
    "        parent_beam_indices = this_level_candidates // vocabulary_size\n",
    "\n",
    "        # normalize this_level_candidates to fall between [0,vocabulary_size]\n",
    "        # In this example this would be [0,1]\n",
    "        this_level_candidates = (this_level_candidates % vocabulary_size).tolist()\n",
    "\n",
    "        # Here we update the final state of each beam to be\n",
    "        # the state that was at the index 1. Because for both the candidates at this level the parent is\n",
    "        # at index 1 (that is b from root level)\n",
    "        session.run(update_sample_beam_state, feed_dict={best_neighbor_beam_indices: parent_beam_indices})\n",
    "\n",
    "        # Here we update the joint probabilities of each beam and add the newly found candidates to the sequence\n",
    "        tmp_this_level_probs = np.asarray(this_level_probs)\n",
    "        tmp_test_sequences = list(test_sequences) # currently [b,a]\n",
    "\n",
    "        for b_n_i in range(beam_neighbors):\n",
    "            # We make the b_n_i element of this_level_probs to be the probability of parents\n",
    "            # In the example the parent indices are [1,1]\n",
    "            # So this_level_probs become [0.5,0.5]\n",
    "            this_level_probs[b_n_i] = tmp_this_level_probs[parent_beam_indices[b_n_i]]\n",
    "\n",
    "            # Next we multiple these by the probabilities of the best candidates from current level\n",
    "            # [0.5*0.5, 0.5*0.2] = [0.25,0.1]\n",
    "            this_level_probs[b_n_i] *= sample_preds_all_neighbors[parent_beam_indices[b_n_i]][0, this_level_candidates[b_n_i]]\n",
    "\n",
    "            # Make the b_n_i element of test_sequences to be the correct parent of the current best candidates\n",
    "            # In the example this becomes [b, b]\n",
    "            test_sequences[b_n_i] = tmp_test_sequences[parent_beam_indices[b_n_i]]\n",
    "\n",
    "            # Now we append the current best candidates\n",
    "            # In this example this becomes [ba,bb]\n",
    "            test_sequences[b_n_i] += reverse_dictionary[this_level_candidates[b_n_i]]\n",
    "\n",
    "            # Create one-hot-encoded representation for each candidate\n",
    "            pred_words.append(np.zeros((1, vocabulary_size), dtype=np.float32))\n",
    "            pred_words[b_n_i][0, this_level_candidates[b_n_i]] = 1.0\n",
    "\n",
    "    # Calculate best beam id based on the highest beam probability\n",
    "    # Using the highest beam probability always lead to very monotonic text\n",
    "    # Let us sample one randomly where one being sampled is decided by the likelihood of that beam\n",
    "    rand_cand_ids = np.argsort(this_level_probs)[-3:]\n",
    "    rand_cand_probs = this_level_candidates[rand_cand_ids] / np.sum(this_level_probs[rand_cand_ids])\n",
    "    rand_id = np.random.choice(rand_cand_ids, p=rand_cand_probs)\n",
    "\n",
    "    best_beam_id = parent_beam_indices[rand_id]\n",
    "\n",
    "    # Update state and output variables for test prediction\n",
    "    session.run(update_sample_beam_state, feed_dict={best_neighbor_beam_indices: [best_beam_id for _ in range(beam_neighbors)]})\n",
    "\n",
    "    # Make the last word from the beam\n",
    "    test_word = pred_words[best_beam_id]\n",
    "\n",
    "    return test_sequences[best_beam_id]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "\n",
      "------------------------Training step 0 ------------------------\n",
      "(93)(37)(44)(41)(6)(78)(32)(14)(64)(1)Average loss at step 1: 4.381960\n",
      "\tPerplexity at step 1: 79.994663\n",
      "Valid Perplexity: 59.20\n",
      "\n",
      "========================New text========================\n",
      "e [3 1 0]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-24-99c713191882>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m    130\u001B[0m         \u001B[1;31m# as current input in a recursive manner\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    131\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0m_\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mchars_in_segment\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 132\u001B[1;33m             \u001B[0mtest_sequence\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mget_beam_prediction\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msession\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    133\u001B[0m             \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtest_sequence\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mend\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m''\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    134\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-23-ba1b0827c9ed>\u001B[0m in \u001B[0;36mget_beam_prediction\u001B[1;34m(session)\u001B[0m\n\u001B[0;32m    123\u001B[0m     \u001B[0mrand_cand_ids\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0margsort\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mthis_level_probs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m3\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    124\u001B[0m     \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mrand_cand_ids\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 125\u001B[1;33m     \u001B[0mrand_cand_probs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mthis_level_candidates\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mrand_cand_ids\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m/\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msum\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mthis_level_probs\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mrand_cand_ids\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    126\u001B[0m     \u001B[0mrand_id\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrandom\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mchoice\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mrand_cand_ids\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mp\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mrand_cand_probs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    127\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mTypeError\u001B[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "# Training and validation\n",
    "filename_to_save = 'lstm_beam_search_dropout'\n",
    "save_path = \"./my_model/my_saved_beam_search_lstm\"\n",
    "\n",
    "num_steps = 26\n",
    "steps_per_document = 100\n",
    "valid_summary = 1\n",
    "train_doc_count = 100\n",
    "docs_per_step = 10\n",
    "\n",
    "\n",
    "beam_nodes = []\n",
    "\n",
    "beam_train_perplexity_ot = []\n",
    "beam_valid_perplexity_ot = []\n",
    "tf.compat.v1.InteractiveSession.close(session)\n",
    "session = tf.compat.v1.InteractiveSession()\n",
    "\n",
    "tf.compat.v1.global_variables_initializer().run()\n",
    "\n",
    "print('Initialized')\n",
    "average_loss = 0\n",
    "\n",
    "# We use the first 10 documents that has\n",
    "# more than 10*steps_per_document bigrams for creating the validation dataset\n",
    "\n",
    "# Identify the first 10 documents following the above condition\n",
    "long_doc_ids = []\n",
    "for di in range(num_files):\n",
    "    if len(data_list[di]) > 10 * steps_per_document:\n",
    "        long_doc_ids.append(di)\n",
    "    if len(long_doc_ids) == 10:\n",
    "        break\n",
    "\n",
    "# Generating validation data\n",
    "data_gens = []\n",
    "valid_gens = []\n",
    "for fi in range(num_files):\n",
    "    # Get all the bigrams if the document id is not in the validation document ids\n",
    "    if fi not in long_doc_ids:\n",
    "        data_gens.append(DataGeneratorOHE(data_list[fi], batch_size=batch_size, num_unroll=num_unrolling))\n",
    "        # If the document is in the validation doc ids, only get up to the last steps_per_document\n",
    "        # bigrams and use the last steps_per_document bigrams as validation data\n",
    "    else:\n",
    "        data_gens.append(DataGeneratorOHE(data_list[fi][:-steps_per_document], batch_size, num_unrolling))\n",
    "        # Define the validation data generator\n",
    "        valid_gens.append(DataGeneratorOHE(data_list[fi][-steps_per_document:], 1, 1))\n",
    "\n",
    "\n",
    "feed_dict = {}\n",
    "for step in range(num_steps):\n",
    "    print(\"\\n\" +  \"-\"*24 + \"Training step %d \" % step + \"-\"*24)\n",
    "    for di in np.random.permutation(train_doc_count)[:docs_per_step]:\n",
    "        doc_perplexity = 0\n",
    "        for doc_step_id in range(steps_per_document):\n",
    "\n",
    "            # Get a set of unrolled batches\n",
    "            u_data, u_label = data_gens[di].unroll_batches()\n",
    "\n",
    "            # Populate feed dict by using each of the data batches present in the unrolled data\n",
    "            for ui, (data, label) in enumerate(zip(u_data, u_label)):\n",
    "                feed_dict[train_inputs[ui]] = data\n",
    "                feed_dict[train_labels[ui]] = label\n",
    "\n",
    "            # Running the operations\n",
    "            _, l, step_perplexity = session.run([optimizer, loss, train_perplexity_without_exp],\n",
    "                                                feed_dict=feed_dict)\n",
    "\n",
    "            # Update doc_perplexity\n",
    "            doc_perplexity += step_perplexity\n",
    "\n",
    "            # Update average loss\n",
    "            average_loss += step_perplexity\n",
    "        print(\"(%d)\" % di, end='')\n",
    "\n",
    "    # resetting hidden state after processing a single document\n",
    "    # It's still questionable if this adds value in terms of learning\n",
    "    # On one hand it's intuitive to reset the state when learning a new document\n",
    "    # On the other hand this approach creates a bias for the state to be zero\n",
    "    # We encourage the reader to investigate further the effect of resetting the state\n",
    "    #session.run(reset_train_state) # resetting hidden state for each document\n",
    "\n",
    "    if (step + 1) % valid_summary == 0:\n",
    "\n",
    "        # Compute average loss\n",
    "        average_loss = average_loss / (docs_per_step * steps_per_document * valid_summary)\n",
    "\n",
    "        # Print Loss\n",
    "        print('Average loss at step %d: %f' % (step+1, average_loss))\n",
    "        print('\\tPerplexity at step %d: %f' %(step+1, np.exp(average_loss)))\n",
    "        beam_train_perplexity_ot.append(np.exp(average_loss))\n",
    "\n",
    "        average_loss = 0\n",
    "        valid_loss = 0\n",
    "\n",
    "        # Calculate valid perplexity\n",
    "        for v_doc_id in range(10):\n",
    "            for v_step in range(steps_per_document // 2):\n",
    "                u_valid_data, u_valid_labels = valid_gens[v_doc_id].unroll_batches()\n",
    "\n",
    "                # Run validatoin phase related\n",
    "                v_perp = session.run([valid_perplexity_without_exp], feed_dict={valid_inputs: u_valid_data[0],\n",
    "                                                                                valid_labels: u_valid_labels[0]})\n",
    "                valid_loss +=  v_perp.pop()\n",
    "\n",
    "            session.run(reset_valid_state)\n",
    "\n",
    "    v_perplexity = np.exp(valid_loss / (steps_per_document * 10.0 // 2))\n",
    "    print(\"Valid Perplexity: %.2f\\n\" % v_perplexity)\n",
    "\n",
    "    # Decay scheduler\n",
    "    decay_learning_rate(session, v_perplexity)\n",
    "\n",
    "    # Generate new text\n",
    "    # Generate 500 bigrams with one segment\n",
    "    segments_to_generate = 1\n",
    "    chars_in_segment = 500\n",
    "\n",
    "    for _ in range(segments_to_generate):\n",
    "        print(\"=\"*24 + \"New text\" + \"=\"*24 )\n",
    "\n",
    "        # start with random word\n",
    "        test_word = np.zeros((1, vocabulary_size), dtype=np.float32)\n",
    "        rand_doc = data_list[np.random.randint(0, num_files)]\n",
    "        test_word[0, rand_doc[np.random.randint(0, len(rand_doc))]] = 1.0\n",
    "        print(reverse_dictionary[np.argmax(test_word[0])], end='')\n",
    "\n",
    "        # Generating words by feeding the previous prediction\n",
    "        # as current input in a recursive manner\n",
    "        for _ in range(chars_in_segment):\n",
    "            test_sequence = get_beam_prediction(session)\n",
    "            print(test_sequence, end='')\n",
    "\n",
    "        # Reset train state\n",
    "        session.run(reset_test_state)\n",
    "        print(\"-\" * 28)\n",
    "\n",
    "\n",
    "# Saved all variables in session into file to restore later\n",
    "print(\"-\" * 12 + \"Saved variable\" + \"-\" * 12)\n",
    "saver = tf.compat.v1.train.Saver()\n",
    "saver.save(session, save_path, global_step=num_steps)\n",
    "\n",
    "session.close()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}