{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "text = open(path_to_file, 'rb').read().decode('utf-8')\n",
    "print('Length of text: {} characters'.format(len(text)))\n",
    "print(text[:100])\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Unique character in files (a, b, c...)\n",
    "vocab = sorted(set(text))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Vectorize\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "# Convert all character to int base on char2idx dict\n",
    "text_as_int = np.array([char2idx[c] for c in text])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F\n",
      "i\n",
      "r\n",
      "s\n",
      "t\n"
     ]
    }
   ],
   "source": [
    "# Training examples  and targets\n",
    "# Divide text into example sequences, each input sequence will\n",
    "# contain seq_length characters from the text\n",
    "# Each sequence, the targets contain the same seq_length of text, but shifted one character to right\n",
    "\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text) // (seq_length + 1)\n",
    "\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(5):\n",
    "    print(idx2char[i.numpy()])\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
      "\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
      "\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
      "'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
     ]
    }
   ],
   "source": [
    "sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(5):\n",
    "    print(repr(''.join(idx2char[item.numpy()])))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Map text to input and target (both input and target have\n",
    "# the same seq_length but target is shifted to right one character)\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1] # take all except the last character\n",
    "    target_text = chunk[1:] # take all except the first character\n",
    "\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data 'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
      "Target data 'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
     ]
    }
   ],
   "source": [
    "for input_exp, target_exp in dataset.take(1):\n",
    "    print('Input data', repr(''.join(idx2char[input_exp.numpy()])))\n",
    "    print(\"Target data\", repr(''.join(idx2char[target_exp.numpy()])))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    0\n",
      "\tinput: 18 ('F')\n",
      "\toutput: 47 ('i')\n",
      "Step    1\n",
      "\tinput: 47 ('i')\n",
      "\toutput: 56 ('r')\n",
      "Step    2\n",
      "\tinput: 56 ('r')\n",
      "\toutput: 57 ('s')\n",
      "Step    3\n",
      "\tinput: 57 ('s')\n",
      "\toutput: 58 ('t')\n",
      "Step    4\n",
      "\tinput: 58 ('t')\n",
      "\toutput: 1 (' ')\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Each index of these vectors is processed as a one\n",
    "    time step. For the input at time step 0, the model\n",
    "    receives the index for \"F\" and tries to predict\n",
    "    the index for \"i\" as the next character. At the\n",
    "    next timestep, it does the same thing but the RNN\n",
    "    considers the previous step context in addition\n",
    "    to the current input character.\n",
    "\"\"\"\n",
    "\n",
    "for i, (input_idx, target_idx) in enumerate(zip(input_exp[:5], target_exp[:5])):\n",
    "    print(\"Step {:4d}\".format(i))\n",
    "    print(\"\\tinput: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "    print(\"\\toutput: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "<PrefetchDataset shapes: ((64, 100), (64, 100)), types: (tf.int32, tf.int32)>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True).prefetch(1)\n",
    "\n",
    "dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Model\n",
    "# Embedding: input layer map the numbers of each character to a vector\n",
    "# with embedding_dim\n",
    "# GRU: special type of RNN with size units=rnn_units\n",
    "# Dense: vocab_size outputs\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "embedding_dim = 256\n",
    "\n",
    "rnn_units = 1024"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=(batch_size, None)),\n",
    "        tf.keras.layers.GRU(units=rnn_units,\n",
    "                            return_sequences=True,\n",
    "                            stateful=True,\n",
    "                            recurrent_initializer='glorot_uniform'),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "\n",
    "model = build_model(vocab_size, embedding_dim, rnn_units, BATCH_SIZE)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    For each character the model looks up the\n",
    "    embedding, runs the GRU one timestep with\n",
    "    the embedding as input, and applies the dense\n",
    "    layer to generate logits predicting the log-likelihood of the next character:\n",
    "\"\"\"\n",
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           16640     \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (64, None, 1024)          3938304   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 65)            66625     \n",
      "=================================================================\n",
      "Total params: 4,021,569\n",
      "Trainable params: 4,021,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "array([45, 41, 58, 29, 41, 49, 33, 38, 21, 60, 14, 10, 44, 64, 50, 21, 14,\n       25, 21,  6, 24, 13, 33, 18, 20, 38, 63, 54, 18, 59, 53, 40, 45, 11,\n       32, 62, 38, 52, 51,  2, 16, 19, 19, 27, 11, 15, 55, 14, 44, 40, 25,\n       37, 52, 11,  2, 21,  2, 14, 63, 10, 53, 56,  3, 46, 40,  8, 26,  3,\n       58, 47, 30,  6, 53,  2, 14, 44, 12, 32, 15, 12, 56, 47,  6, 29, 60,\n        3,  8,  4, 32, 44, 52, 22, 34, 11, 63, 42, 34, 38, 36, 61],\n      dtype=int64)"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
    "sampled_indices"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      " \"se and liberty,\\nWhich have for long run by the hideous law,\\nAs mice by lions--hath pick'd out an act\"\n",
      "Next Char Predictions: \n",
      " 'gctQckUZIvB:fzlIBMI,LAUFHZypFuobg;TxZnm!DGGO;CqBfbMYn;!I!By:or$hb.N$tiR,o!Bf?TC?ri,Qv$.&TfnJV;ydVZXw'\n"
     ]
    }
   ],
   "source": [
    "print(\"Input: \\n\", repr(''.join(idx2char[input_example_batch[0]])))\n",
    "print(\"Next Char Predictions: \\n\", repr(''.join(idx2char[sampled_indices])))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "4.1739507"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
    "example_batch_loss.numpy().mean()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)\n",
    "\n",
    "checkpoint_dir = './training_checkpoint'\n",
    "\n",
    "checkpoint_predix = os.path.join(checkpoint_dir, 'ckpt_{epoch}')\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_predix,\n",
    "    save_weights_only=True\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# EPOCHS = 10\n",
    "\n",
    "# model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tensorflow.python.keras.layers.embeddings.Embedding object at 0x000001B13852F198> and <tensorflow.python.keras.engine.input_layer.InputLayer object at 0x000001B1385B1B70>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tensorflow.python.keras.layers.recurrent_v2.GRU object at 0x000001B138590F98> and <tensorflow.python.keras.layers.embeddings.Embedding object at 0x000001B13852F198>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tensorflow.python.keras.layers.core.Dense object at 0x000001B1385B1780> and <tensorflow.python.keras.layers.recurrent_v2.GRU object at 0x000001B138590F98>).\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 256)            16640     \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (1, None, 1024)           3938304   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 65)             66625     \n",
      "=================================================================\n",
      "Total params: 4,021,569\n",
      "Trainable params: 4,021,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(vocab_size, embedding_dim=embedding_dim,\n",
    "                    rnn_units=rnn_units, batch_size=1)\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.build(tf.TensorShape((1, None)))\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO: to\n",
      "see it die, since you now. O chargish!\n",
      "Will I remanning to't. Well, ward thy me!\n",
      "What is the fund with dowry present shall not,\n",
      "And then we foh your throne; and so dous absence,\n",
      "He will dispose these town of carlick, cholericy speak.\n",
      "Thou hadst been sting forth infection.\n",
      "\n",
      "CAPULET:\n",
      "And ports of Mercutio, she may;\n",
      "For purthou slew'st it in, and lies a clean.\n",
      "Your loves, you at done; and to refuris,\n",
      "His royal success too!\n",
      "\n",
      "MENENIUS:\n",
      "You'll tell me. Tybruit'd in post.\n",
      "\n",
      "OUFORD:\n",
      "What doth you now, who?\n",
      "\n",
      "CAPULET:\n",
      "We help on his gentle Purpures, stoice of me!\n",
      "Modeous, I have spoken: if doubtless woman\n",
      "The whiced is heaven, his number new up,\n",
      "And fear'st a priest had been unsaul't and time by the spleen unto\n",
      "a wing transfur in!\n",
      "\n",
      "LUCIO:\n",
      "He cannot lers her, that stand complaint from gross: puph mine himself, and with a counterear\n",
      "to bleed his ravedue.\n",
      "\n",
      "HORTENSIO:\n",
      "When I do content the name?\n",
      "\n",
      "JULIET:\n",
      "Every pity; sirs, let's seay but beggar:\n",
      "But yet the ground If,--\n",
      "First Marcius!\n",
      "Come us, it f\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, start_string):\n",
    "    \"\"\"\n",
    "    Chose a start string, init RNN state and set the number\n",
    "    of characters to generate\n",
    "    Get the prediction distribution of next character using the start string and RNN state\n",
    "    Use categorical distribution to calculate the index of predicted character\n",
    "    and use this predicted character as our next input\n",
    "    The RNN state returned by the model is fed back into the model so that it now has more context,\n",
    "    After predicting the next character, the modified RNN states are again fed back into the model\n",
    "    :param model:\n",
    "    :param start_string:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    num_generate = 1000\n",
    "\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0) # convert to 2d tensor\n",
    "\n",
    "    text_generated = []\n",
    "\n",
    "    # Love results in more predictable text\n",
    "    # High otherwise\n",
    "    temperature = 1.0\n",
    "\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "\n",
    "        # remove batch dimension\n",
    "        predictions = tf.squeeze(predictions, axis=0)\n",
    "\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
    "\n",
    "        input_eval = tf.expand_dims([predicted_id], axis=0)\n",
    "\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return start_string + ''.join(text_generated)\n",
    "\n",
    "print(generate_text(model, start_string=\"ROMEO: \"))\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}