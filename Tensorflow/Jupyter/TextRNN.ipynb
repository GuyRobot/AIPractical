{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file:  stories\\001.txt\n",
      "File 001.txt already exits\n",
      "Downloading file:  stories\\002.txt\n",
      "File 002.txt already exits\n",
      "Downloading file:  stories\\003.txt\n",
      "File 003.txt already exits\n",
      "Downloading file:  stories\\004.txt\n",
      "File 004.txt already exits\n",
      "Downloading file:  stories\\005.txt\n",
      "File 005.txt already exits\n",
      "Downloading file:  stories\\006.txt\n",
      "File 006.txt already exits\n",
      "Downloading file:  stories\\007.txt\n",
      "File 007.txt already exits\n",
      "Downloading file:  stories\\008.txt\n",
      "File 008.txt already exits\n",
      "Downloading file:  stories\\009.txt\n",
      "File 009.txt already exits\n",
      "Downloading file:  stories\\010.txt\n",
      "File 010.txt already exits\n",
      "Downloading file:  stories\\011.txt\n",
      "File 011.txt already exits\n",
      "Downloading file:  stories\\012.txt\n",
      "File 012.txt already exits\n",
      "Downloading file:  stories\\013.txt\n",
      "File 013.txt already exits\n",
      "Downloading file:  stories\\014.txt\n",
      "File 014.txt already exits\n",
      "Downloading file:  stories\\015.txt\n",
      "File 015.txt already exits\n",
      "Downloading file:  stories\\016.txt\n",
      "File 016.txt already exits\n",
      "Downloading file:  stories\\017.txt\n",
      "File 017.txt already exits\n",
      "Downloading file:  stories\\018.txt\n",
      "File 018.txt already exits\n",
      "Downloading file:  stories\\019.txt\n",
      "File 019.txt already exits\n",
      "Downloading file:  stories\\020.txt\n",
      "File 020.txt already exits\n",
      "Downloading file:  stories\\021.txt\n",
      "File 021.txt already exits\n",
      "Downloading file:  stories\\022.txt\n",
      "File 022.txt already exits\n",
      "Downloading file:  stories\\023.txt\n",
      "File 023.txt already exits\n",
      "Downloading file:  stories\\024.txt\n",
      "File 024.txt already exits\n",
      "Downloading file:  stories\\025.txt\n",
      "File 025.txt already exits\n",
      "Downloading file:  stories\\026.txt\n",
      "File 026.txt already exits\n",
      "Downloading file:  stories\\027.txt\n",
      "File 027.txt already exits\n",
      "Downloading file:  stories\\028.txt\n",
      "File 028.txt already exits\n",
      "Downloading file:  stories\\029.txt\n",
      "File 029.txt already exits\n",
      "Downloading file:  stories\\030.txt\n",
      "File 030.txt already exits\n",
      "Downloading file:  stories\\031.txt\n",
      "File 031.txt already exits\n",
      "Downloading file:  stories\\032.txt\n",
      "File 032.txt already exits\n",
      "Downloading file:  stories\\033.txt\n",
      "File 033.txt already exits\n",
      "Downloading file:  stories\\034.txt\n",
      "File 034.txt already exits\n",
      "Downloading file:  stories\\035.txt\n",
      "File 035.txt already exits\n",
      "Downloading file:  stories\\036.txt\n",
      "File 036.txt already exits\n",
      "Downloading file:  stories\\037.txt\n",
      "File 037.txt already exits\n",
      "Downloading file:  stories\\038.txt\n",
      "File 038.txt already exits\n",
      "Downloading file:  stories\\039.txt\n",
      "File 039.txt already exits\n",
      "Downloading file:  stories\\040.txt\n",
      "File 040.txt already exits\n",
      "Downloading file:  stories\\041.txt\n",
      "File 041.txt already exits\n",
      "Downloading file:  stories\\042.txt\n",
      "File 042.txt already exits\n",
      "Downloading file:  stories\\043.txt\n",
      "File 043.txt already exits\n",
      "Downloading file:  stories\\044.txt\n",
      "File 044.txt already exits\n",
      "Downloading file:  stories\\045.txt\n",
      "File 045.txt already exits\n",
      "Downloading file:  stories\\046.txt\n",
      "File 046.txt already exits\n",
      "Downloading file:  stories\\047.txt\n",
      "File 047.txt already exits\n",
      "Downloading file:  stories\\048.txt\n",
      "File 048.txt already exits\n",
      "Downloading file:  stories\\049.txt\n",
      "File 049.txt already exits\n",
      "Downloading file:  stories\\050.txt\n",
      "File 050.txt already exits\n",
      "Downloading file:  stories\\051.txt\n",
      "File 051.txt already exits\n",
      "Downloading file:  stories\\052.txt\n",
      "File 052.txt already exits\n",
      "Downloading file:  stories\\053.txt\n",
      "File 053.txt already exits\n",
      "Downloading file:  stories\\054.txt\n",
      "File 054.txt already exits\n",
      "Downloading file:  stories\\055.txt\n",
      "File 055.txt already exits\n",
      "Downloading file:  stories\\056.txt\n",
      "File 056.txt already exits\n",
      "Downloading file:  stories\\057.txt\n",
      "File 057.txt already exits\n",
      "Downloading file:  stories\\058.txt\n",
      "File 058.txt already exits\n",
      "Downloading file:  stories\\059.txt\n",
      "File 059.txt already exits\n",
      "Downloading file:  stories\\060.txt\n",
      "File 060.txt already exits\n",
      "Downloading file:  stories\\061.txt\n",
      "File 061.txt already exits\n",
      "Downloading file:  stories\\062.txt\n",
      "File 062.txt already exits\n",
      "Downloading file:  stories\\063.txt\n",
      "File 063.txt already exits\n",
      "Downloading file:  stories\\064.txt\n",
      "File 064.txt already exits\n",
      "Downloading file:  stories\\065.txt\n",
      "File 065.txt already exits\n",
      "Downloading file:  stories\\066.txt\n",
      "File 066.txt already exits\n",
      "Downloading file:  stories\\067.txt\n",
      "File 067.txt already exits\n",
      "Downloading file:  stories\\068.txt\n",
      "File 068.txt already exits\n",
      "Downloading file:  stories\\069.txt\n",
      "File 069.txt already exits\n",
      "Downloading file:  stories\\070.txt\n",
      "File 070.txt already exits\n",
      "Downloading file:  stories\\071.txt\n",
      "File 071.txt already exits\n",
      "Downloading file:  stories\\072.txt\n",
      "File 072.txt already exits\n",
      "Downloading file:  stories\\073.txt\n",
      "File 073.txt already exits\n",
      "Downloading file:  stories\\074.txt\n",
      "File 074.txt already exits\n",
      "Downloading file:  stories\\075.txt\n",
      "File 075.txt already exits\n",
      "Downloading file:  stories\\076.txt\n",
      "File 076.txt already exits\n",
      "Downloading file:  stories\\077.txt\n",
      "File 077.txt already exits\n",
      "Downloading file:  stories\\078.txt\n",
      "File 078.txt already exits\n",
      "Downloading file:  stories\\079.txt\n",
      "File 079.txt already exits\n",
      "Downloading file:  stories\\080.txt\n",
      "File 080.txt already exits\n",
      "Downloading file:  stories\\081.txt\n",
      "File 081.txt already exits\n",
      "Downloading file:  stories\\082.txt\n",
      "File 082.txt already exits\n",
      "Downloading file:  stories\\083.txt\n",
      "File 083.txt already exits\n",
      "Downloading file:  stories\\084.txt\n",
      "File 084.txt already exits\n",
      "Downloading file:  stories\\085.txt\n",
      "File 085.txt already exits\n",
      "Downloading file:  stories\\086.txt\n",
      "File 086.txt already exits\n",
      "Downloading file:  stories\\087.txt\n",
      "File 087.txt already exits\n",
      "Downloading file:  stories\\088.txt\n",
      "File 088.txt already exits\n",
      "Downloading file:  stories\\089.txt\n",
      "File 089.txt already exits\n",
      "Downloading file:  stories\\090.txt\n",
      "File 090.txt already exits\n",
      "Downloading file:  stories\\091.txt\n",
      "File 091.txt already exits\n",
      "Downloading file:  stories\\092.txt\n",
      "File 092.txt already exits\n",
      "Downloading file:  stories\\093.txt\n",
      "File 093.txt already exits\n",
      "Downloading file:  stories\\094.txt\n",
      "File 094.txt already exits\n",
      "Downloading file:  stories\\095.txt\n",
      "File 095.txt already exits\n",
      "Downloading file:  stories\\096.txt\n",
      "File 096.txt already exits\n",
      "Downloading file:  stories\\097.txt\n",
      "File 097.txt already exits\n",
      "Downloading file:  stories\\098.txt\n",
      "File 098.txt already exits\n",
      "Downloading file:  stories\\099.txt\n",
      "File 099.txt already exits\n",
      "Downloading file:  stories\\100.txt\n",
      "File 100.txt already exits\n"
     ]
    }
   ],
   "source": [
    "\n",
    "url = 'https://www.cs.cmu.edu/~spok/grimmtmp/'\n",
    "\n",
    "dir_name = 'stories'\n",
    "if not os.path.exists(dir_name):\n",
    "    os.mkdir(dir_name)\n",
    "\n",
    "def download_stories(filename):\n",
    "    print(\"Downloading file: \", dir_name + os.sep + filename)\n",
    "    if not os.path.exists(os.path.join(dir_name, filename)):\n",
    "        filename, _ = urlretrieve(url + filename, dir_name+os.sep+filename)\n",
    "    else:\n",
    "        print(\"File %s already exits\" % filename)\n",
    "    return filename\n",
    "\n",
    "\n",
    "filenames = [format(i, '03d') + '.txt' for i in range(1, 101)]\n",
    "\n",
    "for fn in filenames:\n",
    "    download_stories(fn)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing file stories\\001.txt\n",
      "Data size (chars) (document 0) 3667\n",
      "Sample string ['in', ' o', 'ld', 'en', ' t', 'im', 'es', ' w', 'he', 'n ', 'wi', 'sh', 'in', 'g ', 'st', 'il', 'l ', 'he', 'lp', 'ed', ' o', 'ne', ', ', 'th', 'er', 'e ', 'li', 've', 'd ', 'a ', 'ki', 'ng', '\\nw', 'ho', 'se', ' d', 'au', 'gh', 'te', 'rs', ' w', 'er', 'e ', 'al', 'l ', 'be', 'au', 'ti', 'fu', 'l,']\n",
      "\n",
      "processing file stories\\002.txt\n",
      "Data size (chars) (document 1) 4928\n",
      "Sample string ['ha', 'rd', ' b', 'y ', 'a ', 'gr', 'ea', 't ', 'fo', 're', 'st', ' d', 'we', 'lt', ' a', ' w', 'oo', 'd-', 'cu', 'tt', 'er', ' w', 'it', 'h ', 'hi', 's ', 'wi', 'fe', ', ', 'wh', 'o ', 'ha', 'd ', 'an', '\\no', 'nl', 'y ', 'ch', 'il', 'd,', ' a', ' l', 'it', 'tl', 'e ', 'gi', 'rl', ' t', 'hr', 'ee']\n",
      "\n",
      "processing file stories\\003.txt\n",
      "Data size (chars) (document 2) 9745\n",
      "Sample string ['a ', 'ce', 'rt', 'ai', 'n ', 'fa', 'th', 'er', ' h', 'ad', ' t', 'wo', ' s', 'on', 's,', ' t', 'he', ' e', 'ld', 'er', ' o', 'f ', 'wh', 'om', ' w', 'as', ' s', 'ma', 'rt', ' a', 'nd', '\\ns', 'en', 'si', 'bl', 'e,', ' a', 'nd', ' c', 'ou', 'ld', ' d', 'o ', 'ev', 'er', 'yt', 'hi', 'ng', ', ', 'bu']\n",
      "\n",
      "processing file stories\\004.txt\n",
      "Data size (chars) (document 3) 2852\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', 'n ', 'ol', 'd ', 'go', 'at', ' w', 'ho', ' h', 'ad', ' s', 'ev', 'en', ' l', 'it', 'tl', 'e ', 'ki', 'ds', ', ', 'an', 'd\\n', 'lo', 've', 'd ', 'th', 'em', ' w', 'it', 'h ', 'al', 'l ', 'th', 'e ', 'lo', 've', ' o']\n",
      "\n",
      "processing file stories\\005.txt\n",
      "Data size (chars) (document 4) 8189\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', 'n ', 'ol', 'd ', 'ki', 'ng', ' w', 'ho', ' w', 'as', ' i', 'll', ' a', 'nd', ' t', 'ho', 'ug', 'ht', ' t', 'o\\n', 'hi', 'ms', 'el', 'f ', \"'i\", ' a', 'm ', 'ly', 'in', 'g ', 'on', ' w', 'ha', 't ', 'mu', 'st', ' b']\n",
      "\n",
      "processing file stories\\006.txt\n",
      "Data size (chars) (document 5) 4369\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' p', 'ea', 'sa', 'nt', ' w', 'ho', ' h', 'ad', ' d', 'ri', 've', 'n ', 'hi', 's ', 'co', 'w ', 'to', ' t', 'he', ' f', 'ai', 'r,', ' a', 'nd', ' s', 'ol', 'd\\n', 'he', 'r ', 'fo', 'r ', 'se', 've', 'n ', 'ta', 'le', 'rs', '. ', ' o', 'n ', 'th', 'e ']\n",
      "\n",
      "processing file stories\\007.txt\n",
      "Data size (chars) (document 6) 5216\n",
      "Sample string ['th', 'er', 'e ', 'we', 're', ' o', 'nc', 'e ', 'up', 'on', ' a', ' t', 'im', 'e ', 'a ', 'ki', 'ng', ' a', 'nd', ' a', ' q', 'ue', 'en', ' w', 'ho', ' l', 'iv', 'ed', '\\nh', 'ap', 'pi', 'ly', ' t', 'og', 'et', 'he', 'r ', 'an', 'd ', 'ha', 'd ', 'tw', 'el', 've', ' c', 'hi', 'ld', 're', 'n,', ' b']\n",
      "\n",
      "processing file stories\\008.txt\n",
      "Data size (chars) (document 7) 6097\n",
      "Sample string ['li', 'tt', 'le', ' b', 'ro', 'th', 'er', ' t', 'oo', 'k ', 'hi', 's ', 'li', 'tt', 'le', ' s', 'is', 'te', 'r ', 'by', ' t', 'he', ' h', 'an', 'd ', 'an', 'd ', 'sa', 'id', ', ', 'si', 'nc', 'e\\n', 'ou', 'r ', 'mo', 'th', 'er', ' d', 'ie', 'd ', 'we', ' h', 'av', 'e ', 'ha', 'd ', 'no', ' h', 'ap']\n",
      "\n",
      "processing file stories\\009.txt\n",
      "Data size (chars) (document 8) 3699\n",
      "Sample string ['th', 'er', 'e ', 'we', 're', ' o', 'nc', 'e ', 'a ', 'ma', 'n ', 'an', 'd ', 'a ', 'wo', 'ma', 'n ', 'wh', 'o ', 'ha', 'd ', 'lo', 'ng', ' i', 'n ', 'va', 'in', '\\nw', 'is', 'he', 'd ', 'fo', 'r ', 'a ', 'ch', 'il', 'd.', '  ', 'at', ' l', 'en', 'gt', 'h ', 'th', 'e ', 'wo', 'ma', 'n ', 'ho', 'pe']\n",
      "\n",
      "processing file stories\\010.txt\n",
      "Data size (chars) (document 9) 5268\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' m', 'an', ' w', 'ho', 'se', ' w', 'if', 'e ', 'di', 'ed', ', ', 'an', 'd ', 'a ', 'wo', 'ma', 'n ', 'wh', 'os', 'e ', 'hu', 'sb', 'an', 'd\\n', 'di', 'ed', ', ', 'an', 'd ', 'th', 'e ', 'ma', 'n ', 'ha', 'd ', 'a ', 'da', 'ug', 'ht', 'er', ', ', 'an']\n",
      "\n",
      "processing file stories\\011.txt\n",
      "Data size (chars) (document 10) 2377\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' g', 'ir', 'l ', 'wh', 'o ', 'wa', 's ', 'id', 'le', ' a', 'nd', ' w', 'ou', 'ld', ' n', 'ot', ' s', 'pi', 'n,', ' a', 'nd', '\\nl', 'et', ' h', 'er', ' m', 'ot', 'he', 'r ', 'sa', 'y ', 'wh', 'at', ' s', 'he', ' w', 'ou', 'ld', ', ', 'sh', 'e ', 'co']\n",
      "\n",
      "processing file stories\\012.txt\n",
      "Data size (chars) (document 11) 7695\n",
      "Sample string ['ha', 'rd', ' b', 'y ', 'a ', 'gr', 'ea', 't ', 'fo', 're', 'st', ' d', 'we', 'lt', ' a', ' p', 'oo', 'r ', 'wo', 'od', '-c', 'ut', 'te', 'r ', 'wi', 'th', ' h', 'is', ' w', 'if', 'e\\n', 'an', 'd ', 'hi', 's ', 'tw', 'o ', 'ch', 'il', 'dr', 'en', '. ', ' t', 'he', ' b', 'oy', ' w', 'as', ' c', 'al']\n",
      "\n",
      "processing file stories\\013.txt\n",
      "Data size (chars) (document 12) 3665\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' o', 'n ', 'a ', 'ti', 'me', ' a', ' p', 'oo', 'r ', 'ma', 'n,', ' w', 'ho', ' c', 'ou', 'ld', ' n', 'o ', 'lo', 'ng', 'er', '\\ns', 'up', 'po', 'rt', ' h', 'is', ' o', 'nl', 'y ', 'so', 'n.', '  ', 'th', 'en', ' s', 'ai', 'd ', 'th', 'e ', 'so', 'n,', ' d']\n",
      "\n",
      "processing file stories\\014.txt\n",
      "Data size (chars) (document 13) 4178\n",
      "Sample string ['a ', 'lo', 'ng', ' t', 'im', 'e ', 'ag', 'o ', 'th', 'er', 'e ', 'li', 've', 'd ', 'a ', 'ki', 'ng', ' w', 'ho', ' w', 'as', ' f', 'am', 'ed', ' f', 'or', ' h', 'is', ' w', 'is', 'do', 'm\\n', 'th', 'ro', 'ug', 'h ', 'al', 'l ', 'th', 'e ', 'la', 'nd', '. ', ' n', 'ot', 'hi', 'ng', ' w', 'as', ' h']\n",
      "\n",
      "processing file stories\\015.txt\n",
      "Data size (chars) (document 14) 8674\n",
      "Sample string ['on', 'e ', 'su', 'mm', 'er', \"'s\", ' m', 'or', 'ni', 'ng', ' a', ' l', 'it', 'tl', 'e ', 'ta', 'il', 'or', ' w', 'as', ' s', 'it', 'ti', 'ng', ' o', 'n ', 'hi', 's ', 'ta', 'bl', 'e\\n', 'by', ' t', 'he', ' w', 'in', 'do', 'w,', ' h', 'e ', 'wa', 's ', 'in', ' g', 'oo', 'd ', 'sp', 'ir', 'it', 's,']\n",
      "\n",
      "processing file stories\\016.txt\n",
      "Data size (chars) (document 15) 7018\n",
      "Sample string ['\\tc', 'in', 'de', 're', 'll', 'a\\n', 'th', 'e ', 'wi', 'fe', ' o', 'f ', 'a ', 'ri', 'ch', ' m', 'an', ' f', 'el', 'l ', 'si', 'ck', ', ', 'an', 'd ', 'as', ' s', 'he', ' f', 'el', 't ', 'th', 'at', ' h', 'er', ' e', 'nd', '\\nw', 'as', ' d', 'ra', 'wi', 'ng', ' n', 'ea', 'r,', ' s', 'he', ' c', 'al']\n",
      "\n",
      "processing file stories\\017.txt\n",
      "Data size (chars) (document 16) 3039\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' k', 'in', \"g'\", 's ', 'so', 'n ', 'wh', 'o ', 'wa', 's ', 'se', 'iz', 'ed', ' w', 'it', 'h ', 'a ', 'de', 'si', 're', ' t', 'o ', 'tr', 'av', 'el', '\\na', 'bo', 'ut', ' t', 'he', ' w', 'or', 'ld', ', ', 'an', 'd ', 'to', 'ok', ' n', 'o ', 'on', 'e ']\n",
      "\n",
      "processing file stories\\018.txt\n",
      "Data size (chars) (document 17) 3020\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' w', 'id', 'ow', ' w', 'ho', ' h', 'ad', ' t', 'wo', ' d', 'au', 'gh', 'te', 'rs', ' -', ' o', 'ne', ' o', 'f\\n', 'wh', 'om', ' w', 'as', ' p', 're', 'tt', 'y ', 'an', 'd ', 'in', 'du', 'st', 'ri', 'ou', 's,', ' w', 'hi', 'ls', 't ', 'th', 'e ', 'ot']\n",
      "\n",
      "processing file stories\\019.txt\n",
      "Data size (chars) (document 18) 2465\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' m', 'an', ' w', 'ho', ' h', 'ad', ' s', 'ev', 'en', ' s', 'on', 's,', ' a', 'nd', ' s', 'ti', 'll', ' h', 'e ', 'ha', 'd\\n', 'no', ' d', 'au', 'gh', 'te', 'r,', ' h', 'ow', 'ev', 'er', ' m', 'uc', 'h ', 'he', ' w', 'is', 'he', 'd ', 'fo', 'r ', 'on']\n",
      "\n",
      "processing file stories\\020.txt\n",
      "Data size (chars) (document 19) 3703\n",
      "Sample string ['\\tl', 'it', 'tl', 'e ', 're', 'd-', 'ca', 'p\\n', '\\no', 'nc', 'e ', 'up', 'on', ' a', ' t', 'im', 'e ', 'th', 'er', 'e ', 'wa', 's ', 'a ', 'de', 'ar', ' l', 'it', 'tl', 'e ', 'gi', 'rl', ' w', 'ho', ' w', 'as', ' l', 'ov', 'ed', '\\nb', 'y ', 'ev', 'er', 'y ', 'on', 'e ', 'wh', 'o ', 'lo', 'ok', 'ed']\n",
      "\n",
      "processing file stories\\021.txt\n",
      "Data size (chars) (document 20) 1924\n",
      "Sample string ['in', ' a', ' c', 'er', 'ta', 'in', ' c', 'ou', 'nt', 'ry', ' t', 'he', 're', ' w', 'as', ' o', 'nc', 'e ', 'gr', 'ea', 't ', 'la', 'me', 'nt', 'at', 'io', 'n ', 'ov', 'er', ' a', '\\nw', 'il', 'd ', 'bo', 'ar', ' t', 'ha', 't ', 'la', 'id', ' w', 'as', 'te', ' t', 'he', ' f', 'ar', 'me', \"r'\", 's ']\n",
      "\n",
      "processing file stories\\022.txt\n",
      "Data size (chars) (document 21) 6561\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' p', 'oo', 'r ', 'wo', 'ma', 'n ', 'wh', 'o ', 'ga', 've', ' b', 'ir', 'th', ' t', 'o ', 'a ', 'li', 'tt', 'le', ' s', 'on', ',\\n', 'an', 'd ', 'as', ' h', 'e ', 'ca', 'me', ' i', 'nt', 'o ', 'th', 'e ', 'wo', 'rl', 'd ', 'wi', 'th', ' a', ' c', 'au']\n",
      "\n",
      "processing file stories\\023.txt\n",
      "Data size (chars) (document 22) 5956\n",
      "Sample string ['a ', 'ce', 'rt', 'ai', 'n ', 'mi', 'll', 'er', ' h', 'ad', ' l', 'it', 'tl', 'e ', 'by', ' l', 'it', 'tl', 'e ', 'fa', 'll', 'en', ' i', 'nt', 'o ', 'po', 've', 'rt', 'y,', ' a', 'nd', '\\nh', 'ad', ' n', 'ot', 'hi', 'ng', ' l', 'ef', 't ', 'bu', 't ', 'hi', 's ', 'mi', 'll', ' a', 'nd', ' a', ' l']\n",
      "\n",
      "processing file stories\\024.txt\n",
      "Data size (chars) (document 23) 2529\n",
      "Sample string ['th', 'e ', 'mo', 'th', 'er', ' o', 'f ', 'ha', 'ns', ' s', 'ai', 'd,', ' w', 'hi', 'th', 'er', ' a', 'wa', 'y,', ' h', 'an', 's.', '  ', 'ha', 'ns', ' a', 'ns', 'we', 're', 'd,', ' t', 'o\\n', 'gr', 'et', 'el', '. ', ' b', 'eh', 'av', 'e ', 'we', 'll', ', ', 'ha', 'ns', '. ', ' o', 'h,', ' i', \"'l\"]\n",
      "\n",
      "processing file stories\\025.txt\n",
      "Data size (chars) (document 24) 2416\n",
      "Sample string ['an', ' a', 'ge', 'd ', 'co', 'un', 't ', 'on', 'ce', ' l', 'iv', 'ed', ' i', 'n ', 'sw', 'it', 'ze', 'rl', 'an', 'd,', ' w', 'ho', ' h', 'ad', ' a', 'n ', 'on', 'ly', ' s', 'on', ',\\n', 'bu', 't ', 'he', ' w', 'as', ' s', 'tu', 'pi', 'd,', ' a', 'nd', ' c', 'ou', 'ld', ' l', 'ea', 'rn', ' n', 'ot']\n",
      "\n",
      "processing file stories\\026.txt\n",
      "Data size (chars) (document 25) 3369\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' m', 'an', ' w', 'ho', ' h', 'ad', ' a', ' d', 'au', 'gh', 'te', 'r ', 'wh', 'o ', 'wa', 's ', 'ca', 'll', 'ed', ' c', 'le', 've', 'r\\n', 'el', 'si', 'e.', '  ', 'an', 'd ', 'wh', 'en', ' s', 'he', ' h', 'ad', ' g', 'ro', 'wn', ' u', 'p ', 'he', 'r ']\n",
      "\n",
      "processing file stories\\027.txt\n",
      "Data size (chars) (document 26) 10013\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' t', 'ai', 'lo', 'r ', 'wh', 'o ', 'ha', 'd ', 'th', 're', 'e ', 'so', 'ns', ', ', 'an', 'd\\n', 'on', 'ly', ' o', 'ne', ' g', 'oa', 't.', '  ', 'bu', 't ', 'as', ' t', 'he', ' g', 'oa', 't ', 'su', 'pp', 'or', 'te']\n",
      "\n",
      "processing file stories\\028.txt\n",
      "Data size (chars) (document 27) 5788\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' p', 'oo', 'r ', 'pe', 'as', 'an', 't ', 'wh', 'o ', 'sa', 't ', 'in', ' t', 'he', ' e', 've', 'ni', 'ng', ' b', 'y ', 'th', 'e\\n', 'he', 'ar', 'th', ' a', 'nd', ' p', 'ok', 'ed', ' t', 'he', ' f', 'ir', 'e,', ' a', 'nd', ' h', 'is', ' w', 'if', 'e ']\n",
      "\n",
      "processing file stories\\029.txt\n",
      "Data size (chars) (document 28) 1335\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' p', 'oo', 'r ', 'se', 'rv', 'an', 't-', 'gi', 'rl', ' w', 'ho', ' w', 'as', ' i', 'nd', 'us', 'tr', 'io', 'us', ' a', 'nd', ' c', 'le', 'an', 'ly', '\\na', 'nd', ' s', 'we', 'pt', ' t', 'he', ' h', 'ou', 'se', ' e', 've', 'ry', ' d', 'ay', ', ', 'an']\n",
      "\n",
      "processing file stories\\030.txt\n",
      "Data size (chars) (document 29) 3591\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' m', 'il', 'le', 'r,', ' w', 'ho', ' h', 'ad', ' a', ' b', 'ea', 'ut', 'if', 'ul', '\\nd', 'au', 'gh', 'te', 'r,', ' a', 'nd', ' a', 's ', 'sh', 'e ', 'wa', 's ', 'gr', 'ow', 'n ', 'up', ', ', 'he', ' w', 'is', 'he']\n",
      "\n",
      "processing file stories\\031.txt\n",
      "Data size (chars) (document 30) 1624\n",
      "Sample string ['a ', 'po', 'or', ' m', 'an', ' h', 'ad', ' s', 'o ', 'ma', 'ny', ' c', 'hi', 'ld', 're', 'n ', 'th', 'at', ' h', 'e ', 'ha', 'd ', 'al', 're', 'ad', 'y ', 'as', 'ke', 'd\\n', 'ev', 'er', 'yo', 'ne', ' i', 'n ', 'th', 'e ', 'wo', 'rl', 'd ', 'to', ' b', 'e ', 'go', 'df', 'at', 'he', 'r,', ' a', 'nd']\n",
      "\n",
      "processing file stories\\032.txt\n",
      "Data size (chars) (document 31) 758\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' l', 'it', 'tl', 'e ', 'gi', 'rl', ' w', 'ho', ' w', 'as', ' o', 'bs', 'ti', 'na', 'te', ' a', 'nd', ' i', 'nq', 'ui', 'si', 'ti', 've', ',\\n', 'an', 'd ', 'wh', 'en', ' h', 'er', ' p', 'ar', 'en', 'ts', ' t', 'ol', 'd ', 'he', 'r ', 'to', ' d', 'o ']\n",
      "\n",
      "processing file stories\\033.txt\n",
      "Data size (chars) (document 32) 3121\n",
      "Sample string ['a ', 'po', 'or', ' m', 'an', ' h', 'ad', ' t', 'we', 'lv', 'e ', 'ch', 'il', 'dr', 'en', ' a', 'nd', ' w', 'as', ' f', 'or', 'ce', 'd ', 'to', ' w', 'or', 'k ', 'ni', 'gh', 't ', 'an', 'd\\n', 'da', 'y ', 'to', ' g', 'iv', 'e ', 'th', 'em', ' e', 've', 'n ', 'br', 'ea', 'd.', '  ', 'wh', 'en', ' t']\n",
      "\n",
      "processing file stories\\034.txt\n",
      "Data size (chars) (document 33) 4192\n",
      "Sample string ['a ', 'ce', 'rt', 'ai', 'n ', 'ta', 'il', 'or', ' h', 'ad', ' a', ' s', 'on', ', ', 'wh', 'o ', 'ha', 'pp', 'en', 'ed', ' t', 'o ', 'be', ' s', 'ma', 'll', ', ', 'an', 'd\\n', 'no', ' b', 'ig', 'ge', 'r ', 'th', 'an', ' a', ' t', 'hu', 'mb', ', ', 'an', 'd ', 'on', ' t', 'hi', 's ', 'ac', 'co', 'un']\n",
      "\n",
      "processing file stories\\035.txt\n",
      "Data size (chars) (document 34) 3650\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' w', 'iz', 'ar', 'd ', 'wh', 'o ', 'us', 'ed', ' t', 'o ', 'ta', 'ke', ' t', 'he', ' f', 'or', 'm ', 'of', ' a', ' p', 'oo', 'r\\n', 'ma', 'n,', ' a', 'nd', ' w', 'en', 't ', 'to', ' h', 'ou', 'se', 's ', 'an', 'd ', 'be', 'gg', 'ed', ', ', 'an', 'd ']\n",
      "\n",
      "processing file stories\\036.txt\n",
      "Data size (chars) (document 35) 8219\n",
      "Sample string ['it', ' i', 's ', 'no', 'w ', 'lo', 'ng', ' a', 'go', ', ', 'qu', 'it', 'e ', 'tw', 'o ', 'th', 'ou', 'sa', 'nd', ' y', 'ea', 'rs', ', ', 'si', 'nc', 'e ', 'th', 'er', 'e ', 'wa', 's\\n', 'a ', 'ri', 'ch', ' m', 'an', ' w', 'ho', ' h', 'ad', ' a', ' b', 'ea', 'ut', 'if', 'ul', ' a', 'nd', ' p', 'io']\n",
      "\n",
      "processing file stories\\037.txt\n",
      "Data size (chars) (document 36) 2151\n",
      "Sample string ['a ', 'fa', 'rm', 'er', ' o', 'nc', 'e ', 'ha', 'd ', 'a ', 'fa', 'it', 'hf', 'ul', ' d', 'og', ' c', 'al', 'le', 'd ', 'su', 'lt', 'an', ', ', 'wh', 'o ', 'ha', 'd ', 'gr', 'ow', 'n\\n', 'ol', 'd,', ' a', 'nd', ' l', 'os', 't ', 'al', 'l ', 'hi', 's ', 'te', 'et', 'h,', ' s', 'o ', 'th', 'at', ' h']\n",
      "\n",
      "processing file stories\\038.txt\n",
      "Data size (chars) (document 37) 5129\n",
      "Sample string ['on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ', ', 'a ', 'ce', 'rt', 'ai', 'n ', 'ki', 'ng', ' w', 'as', ' h', 'un', 'ti', 'ng', ' i', 'n ', 'a ', 'gr', 'ea', 't ', 'fo', 're', 'st', ',\\n', 'an', 'd ', 'he', ' c', 'ha', 'se', 'd ', 'a ', 'wi', 'ld', ' b', 'ea', 'st', ' s', 'o ', 'ea', 'ge', 'rl']\n",
      "\n",
      "processing file stories\\039.txt\n",
      "Data size (chars) (document 38) 3472\n",
      "Sample string ['\\tb', 'ri', 'ar', '-r', 'os', 'e\\n', '\\na', ' l', 'on', 'g ', 'ti', 'me', ' a', 'go', ' t', 'he', 're', ' w', 'er', 'e ', 'a ', 'ki', 'ng', ' a', 'nd', ' q', 'ue', 'en', ' w', 'ho', ' s', 'ai', 'd ', 'ev', 'er', 'y\\n', 'da', 'y,', ' a', 'h,', ' i', 'f ', 'on', 'ly', ' w', 'e ', 'ha', 'd ', 'a ', 'ch']\n",
      "\n",
      "processing file stories\\040.txt\n",
      "Data size (chars) (document 39) 2490\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' f', 'or', 'es', 'te', 'r ', 'wh', 'o ', 'we', 'nt', ' i', 'nt', 'o ', 'th', 'e ', 'fo', 're', 'st', ' t', 'o ', 'hu', 'nt', ',\\n', 'an', 'd ', 'as', ' h', 'e ', 'en', 'te', 're', 'd ', 'it', ' h', 'e ', 'he', 'ar', 'd ', 'a ', 'so', 'un', 'd ', 'of']\n",
      "\n",
      "processing file stories\\041.txt\n",
      "Data size (chars) (document 40) 4273\n",
      "Sample string ['a ', 'ki', 'ng', ' h', 'ad', ' a', ' d', 'au', 'gh', 'te', 'r ', 'wh', 'o ', 'wa', 's ', 'be', 'au', 'ti', 'fu', 'l ', 'be', 'yo', 'nd', ' a', 'll', ' m', 'ea', 'su', 're', ',\\n', 'bu', 't ', 'so', ' p', 'ro', 'ud', ' a', 'nd', ' h', 'au', 'gh', 'ty', ' w', 'it', 'ha', 'l ', 'th', 'at', ' n', 'o ']\n",
      "\n",
      "processing file stories\\042.txt\n",
      "Data size (chars) (document 41) 8327\n",
      "Sample string ['\\ts', 'no', 'w ', 'wh', 'it', 'e ', 'an', 'd ', 'th', 'e ', 'se', 've', 'n ', 'dw', 'ar', 'fs', '\\n\\n', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' i', 'n ', 'th', 'e ', 'mi', 'dd', 'le', ' o', 'f ', 'wi', 'nt', 'er', ', ', 'wh', 'en', ' t', 'he', ' f', 'la', 'ke', 's ', 'of', '\\ns', 'no', 'w ']\n",
      "\n",
      "processing file stories\\043.txt\n",
      "Data size (chars) (document 42) 6128\n",
      "Sample string ['th', 'er', 'e ', 'we', 're', ' o', 'nc', 'e ', 'th', 're', 'e ', 'br', 'ot', 'he', 'rs', ' w', 'ho', ' h', 'ad', ' f', 'al', 'le', 'n ', 'de', 'ep', 'er', ' a', 'nd', ' d', 'ee', 'pe', 'r ', 'in', 'to', '\\np', 'ov', 'er', 'ty', ', ', 'an', 'd ', 'at', ' l', 'as', 't ', 'th', 'ei', 'r ', 'ne', 'ed']\n",
      "\n",
      "processing file stories\\044.txt\n",
      "Data size (chars) (document 43) 2819\n",
      "Sample string ['\\tr', 'um', 'pe', 'ls', 'ti', 'lt', 'sk', 'in', '\\n\\n', 'on', 'ce', ' t', 'he', 're', ' w', 'as', ' a', ' m', 'il', 'le', 'r ', 'wh', 'o ', 'wa', 's ', 'po', 'or', ', ', 'bu', 't ', 'wh', 'o ', 'ha', 'd ', 'a ', 'be', 'au', 'ti', 'fu', 'l\\n', 'da', 'ug', 'ht', 'er', '. ', ' n', 'ow', ' i', 't ', 'ha']\n",
      "\n",
      "processing file stories\\045.txt\n",
      "Data size (chars) (document 44) 3822\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' w', 'om', 'an', ' w', 'ho', ' w', 'as', ' a', ' r', 'ea', 'l ', 'wi', 'tc', 'h ', 'an', 'd ', 'ha', 'd ', 'tw', 'o\\n', 'da', 'ug', 'ht', 'er', 's,', ' o', 'ne', ' u', 'gl', 'y ', 'an', 'd ', 'wi', 'ck', 'ed', ', ']\n",
      "\n",
      "processing file stories\\046.txt\n",
      "Data size (chars) (document 45) 7772\n",
      "Sample string ['in', ' o', 'ld', 'en', ' t', 'im', 'es', ' t', 'he', 're', ' w', 'as', ' a', ' k', 'in', 'g,', ' w', 'ho', ' h', 'ad', ' b', 'eh', 'in', 'd ', 'hi', 's ', 'pa', 'la', 'ce', ' a', '\\nb', 'ea', 'ut', 'if', 'ul', ' p', 'le', 'as', 'ur', 'e-', 'ga', 'rd', 'en', ' i', 'n ', 'wh', 'ic', 'h ', 'th', 'er']\n",
      "\n",
      "processing file stories\\047.txt\n",
      "Data size (chars) (document 46) 22158\n",
      "Sample string ['th', 'er', 'e ', 'we', 're', ' o', 'nc', 'e ', 'up', 'on', ' a', ' t', 'im', 'e ', 'tw', 'o ', 'br', 'ot', 'he', 'rs', ', ', 'on', 'e ', 'ri', 'ch', ' a', 'nd', ' t', 'he', ' o', 'th', 'er', '\\np', 'oo', 'r.', '  ', 'th', 'e ', 'ri', 'ch', ' o', 'ne', ' w', 'as', ' a', ' g', 'ol', 'ds', 'mi', 'th']\n",
      "\n",
      "processing file stories\\048.txt\n",
      "Data size (chars) (document 47) 2169\n",
      "Sample string ['tw', 'o ', 'ki', 'ng', \"s'\", ' s', 'on', 's ', 'on', 'ce', ' w', 'en', 't ', 'ou', 't ', 'in', ' s', 'ea', 'rc', 'h ', 'of', ' a', 'dv', 'en', 'tu', 're', 's,', ' a', 'nd', ' f', 'el', 'l ', 'in', 'to', '\\na', ' w', 'il', 'd,', ' d', 'is', 'or', 'de', 'rl', 'y ', 'wa', 'y ', 'of', ' l', 'iv', 'in']\n",
      "\n",
      "processing file stories\\049.txt\n",
      "Data size (chars) (document 48) 2822\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' k', 'in', 'g ', 'wh', 'o ', 'ha', 'd ', 'th', 're', 'e ', 'so', 'ns', ', ', 'of', ' w', 'ho', 'm ', 'tw', 'o\\n', 'we', 're', ' c', 'le', 've', 'r ', 'an', 'd ', 'wi', 'se', ', ', 'bu', 't ', 'th', 'e ', 'th', 'ir']\n",
      "\n",
      "processing file stories\\050.txt\n",
      "Data size (chars) (document 49) 4034\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'a ', 'ma', 'n ', 'wh', 'o ', 'ha', 'd ', 'th', 're', 'e ', 'so', 'ns', ', ', 'th', 'e ', 'yo', 'un', 'ge', 'st', ' o', 'f ', 'wh', 'om', ' w', 'as', ' c', 'al', 'le', 'd\\n', 'du', 'mm', 'li', 'ng', ', ', 'an', 'd ', 'wa', 's ', 'de', 'sp', 'is', 'ed', ', ', 'mo', 'ck']\n",
      "\n",
      "processing file stories\\051.txt\n",
      "Data size (chars) (document 50) 5608\n",
      "Sample string ['\\ta', 'll', 'er', 'le', 'ir', 'au', 'h\\n', '\\nt', 'he', 're', ' w', 'as', ' o', 'nc', 'e ', 'up', 'on', ' a', ' t', 'im', 'e ', 'a ', 'ki', 'ng', ' w', 'ho', ' h', 'ad', ' a', ' w', 'if', 'e ', 'wi', 'th', ' g', 'ol', 'de', 'n ', 'ha', 'ir', ',\\n', 'an', 'd ', 'sh', 'e ', 'wa', 's ', 'so', ' b', 'ea']\n",
      "\n",
      "processing file stories\\052.txt\n",
      "Data size (chars) (document 51) 1287\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' w', 'om', 'an', ' a', 'nd', ' h', 'er', ' d', 'au', 'gh', 'te', 'r ', 'wh', 'o ', 'li', 've', 'd ', 'in', ' a', '\\np', 're', 'tt', 'y ', 'ga', 'rd', 'en', ' w', 'it', 'h ', 'ca', 'bb', 'ag', 'es', '. ', ' a', 'nd', ' a', ' l', 'it', 'tl', 'e ', 'ha']\n",
      "\n",
      "processing file stories\\053.txt\n",
      "Data size (chars) (document 52) 2841\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' k', 'in', \"g'\", 's ', 'so', 'n ', 'wh', 'o ', 'ha', 'd ', 'a ', 'br', 'id', 'e ', 'wh', 'om', ' h', 'e ', 'lo', 've', 'd ', 've', 'ry', ' m', 'uc', 'h.', '\\na', 'nd', ' w', 'he', 'n ', 'he', ' w', 'as', ' s', 'it', 'ti', 'ng', ' b', 'es', 'id', 'e ']\n",
      "\n",
      "processing file stories\\054.txt\n",
      "Data size (chars) (document 53) 1922\n",
      "Sample string ['ha', 'ns', ' w', 'is', 'he', 'd ', 'to', ' p', 'ut', ' h', 'is', ' s', 'on', ' t', 'o ', 'le', 'ar', 'n ', 'a ', 'tr', 'ad', 'e,', ' s', 'o ', 'he', ' w', 'en', 't ', 'in', 'to', ' t', 'he', '\\nc', 'hu', 'rc', 'h ', 'an', 'd ', 'pr', 'ay', 'ed', ' t', 'o ', 'ou', 'r ', 'lo', 'rd', ' g', 'od', ' t']\n",
      "\n",
      "processing file stories\\055.txt\n",
      "Data size (chars) (document 54) 2573\n",
      "Sample string ['a ', 'fa', 'th', 'er', ' o', 'nc', 'e ', 'ca', 'll', 'ed', ' h', 'is', ' t', 'hr', 'ee', ' s', 'on', 's ', 'be', 'fo', 're', ' h', 'im', ', ', 'an', 'd ', 'he', ' g', 'av', 'e ', 'to', ' t', 'he', '\\nf', 'ir', 'st', ' a', ' c', 'oc', 'k,', ' t', 'o ', 'th', 'e ', 'se', 'co', 'nd', ' a', ' s', 'cy']\n",
      "\n",
      "processing file stories\\056.txt\n",
      "Data size (chars) (document 55) 5285\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' m', 'an', ' w', 'ho', ' u', 'nd', 'er', 'st', 'oo', 'd ', 'al', 'l ', 'ki', 'nd', 's ', 'of', ' a', 'rt', 's.', '  ', 'he', ' s', 'er', 've', 'd ', 'in', '\\nw', 'ar', ', ', 'an', 'd ', 'be', 'ha', 've', 'd ', 'we', 'll', ' a', 'nd', ' b', 'ra', 've']\n",
      "\n",
      "processing file stories\\057.txt\n",
      "Data size (chars) (document 56) 971\n",
      "Sample string ['th', 'e ', 'sh', 'e-', 'wo', 'lf', ' b', 'ro', 'ug', 'ht', ' i', 'nt', 'o ', 'th', 'e ', 'wo', 'rl', 'd ', 'a ', 'yo', 'un', 'g ', 'on', 'e,', ' a', 'nd', ' i', 'nv', 'it', 'ed', ' t', 'he', ' f', 'ox', '\\nt', 'o ', 'be', ' g', 'od', 'fa', 'th', 'er', '. ', ' a', 'ft', 'er', ' a', 'll', ', ', 'he']\n",
      "\n",
      "processing file stories\\058.txt\n",
      "Data size (chars) (document 57) 4538\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' q', 'ue', 'en', ' t', 'o ', 'wh', 'om', ' g', 'od', ' h', 'ad', ' g', 'iv', 'en', ' n', 'o ', 'ch', 'il', 'dr', 'en', '.\\n', 'ev', 'er', 'y ', 'mo', 'rn', 'in', 'g ', 'sh', 'e ', 'we', 'nt', ' i', 'nt', 'o ', 'th']\n",
      "\n",
      "processing file stories\\059.txt\n",
      "Data size (chars) (document 58) 636\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' v', 'er', 'y ', 'ol', 'd ', 'ma', 'n,', ' w', 'ho', 'se', ' e', 'ye', 's ', 'ha', 'd ', 'be', 'co', 'me', ' d', 'im', ', ', 'hi', 's ', 'ea', 'rs', '\\nd', 'ul', 'l ', 'of', ' h', 'ea', 'ri', 'ng', ', ', 'hi', 's ', 'kn', 'ee', 's ', 'tr', 'em', 'bl']\n",
      "\n",
      "processing file stories\\060.txt\n",
      "Data size (chars) (document 59) 786\n",
      "Sample string ['a ', 'li', 'tt', 'le', ' b', 'ro', 'th', 'er', ' a', 'nd', ' s', 'is', 'te', 'r ', 'we', 're', ' o', 'nc', 'e ', 'pl', 'ay', 'in', 'g ', 'by', ' a', ' w', 'el', 'l,', ' a', 'nd', ' w', 'hi', 'le', '\\nt', 'he', 'y ', 'we', 're', ' t', 'hu', 's ', 'pl', 'ay', 'in', 'g,', ' t', 'he', 'y ', 'bo', 'th']\n",
      "\n",
      "processing file stories\\061.txt\n",
      "Data size (chars) (document 60) 10687\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'e ', 'up', 'on', ' a', ' t', 'im', 'e ', 'a ', 'gr', 'ea', 't ', 'wa', 'r,', ' a', 'nd', ' w', 'he', 'n ', 'it', ' c', 'am', 'e ', 'to', ' a', 'n ', 'en', 'd,', '\\nm', 'an', 'y ', 'so', 'ld', 'ie', 'rs', ' w', 'er', 'e ', 'di', 'sc', 'ha', 'rg', 'ed', '. ', ' t']\n",
      "\n",
      "processing file stories\\062.txt\n",
      "Data size (chars) (document 61) 5105\n",
      "Sample string ['ha', 'ns', ' h', 'ad', ' s', 'er', 've', 'd ', 'hi', 's ', 'ma', 'st', 'er', ' f', 'or', ' s', 'ev', 'en', ' y', 'ea', 'rs', ', ', 'so', ' h', 'e ', 'sa', 'id', ' t', 'o ', 'hi', 'm,', '\\nm', 'as', 'te', 'r,', ' m', 'y ', 'ti', 'me', ' i', 's ', 'up', ', ', 'no', 'w ', 'i ', 'sh', 'ou', 'ld', ' b']\n",
      "\n",
      "processing file stories\\063.txt\n",
      "Data size (chars) (document 62) 1127\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' y', 'ou', 'ng', ' p', 'ea', 'sa', 'nt', ' n', 'am', 'ed', ' h', 'an', 's,', ' w', 'ho', 'se', ' u', 'nc', 'le', '\\nw', 'an', 'te', 'd ', 'to', ' f', 'in', 'd ', 'hi', 'm ', 'a ', 'ri', 'ch', ' w', 'if', 'e.', '  ']\n",
      "\n",
      "processing file stories\\064.txt\n",
      "Data size (chars) (document 63) 4981\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' p', 'oo', 'r ', 'ma', 'n ', 'an', 'd ', 'a ', 'po', 'or', ' w', 'om', 'an', ' w', 'ho', ' h', 'ad', ' n', 'ot', 'hi', 'ng', ' b', 'ut', ' a', '\\nl', 'it', 'tl', 'e ', 'co', 'tt', 'ag', 'e,', ' a', 'nd', ' w', 'ho', ' e', 'ar', 'ne', 'd ', 'th', 'ei']\n",
      "\n",
      "processing file stories\\065.txt\n",
      "Data size (chars) (document 64) 6006\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' m', 'an', ' w', 'ho', ' w', 'as', ' a', 'bo', 'ut', ' t', 'o ', 'se', 't ', 'ou', 't ', 'on', ' a', ' l', 'on', 'g\\n', 'jo', 'ur', 'ne', 'y,', ' a', 'nd', ' o', 'n ', 'pa', 'rt', 'in', 'g ', 'he', ' a', 'sk', 'ed']\n",
      "\n",
      "processing file stories\\066.txt\n",
      "Data size (chars) (document 65) 5900\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', 'n ', 'ol', 'd ', 'qu', 'ee', 'n ', 'wh', 'os', 'e ', 'hu', 'sb', 'an', 'd ', 'ha', 'd ', 'be', 'en', ' d', 'ea', 'd\\n', 'fo', 'r ', 'ma', 'ny', ' y', 'ea', 'rs', ', ', 'an', 'd ', 'sh', 'e ', 'ha', 'd ', 'a ', 'be']\n",
      "\n",
      "processing file stories\\067.txt\n",
      "Data size (chars) (document 66) 7837\n",
      "Sample string ['on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' c', 'ou', 'nt', 'ry', 'ma', 'n ', 'ha', 'd ', 'a ', 'so', 'n ', 'wh', 'o ', 'wa', 's ', 'as', ' b', 'ig', ' a', 's ', 'a ', 'th', 'um', 'b,', '\\na', 'nd', ' d', 'id', ' n', 'ot', ' b', 'ec', 'om', 'e ', 'an', 'y ', 'bi', 'gg', 'er', ', ', 'an']\n",
      "\n",
      "processing file stories\\068.txt\n",
      "Data size (chars) (document 67) 4717\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' r', 'ic', 'h ', 'ki', 'ng', ' w', 'ho', ' h', 'ad', ' t', 'hr', 'ee', ' d', 'au', 'gh', 'te', 'rs', ', ', 'wh', 'o\\n', 'da', 'il', 'y ', 'we', 'nt', ' t', 'o ', 'wa', 'lk', ' i', 'n ', 'th', 'e ', 'pa', 'la', 'ce']\n",
      "\n",
      "processing file stories\\069.txt\n",
      "Data size (chars) (document 68) 6233\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'a ', 'ce', 'rt', 'ai', 'n ', 'me', 'rc', 'ha', 'nt', ' w', 'ho', ' h', 'ad', ' t', 'wo', ' c', 'hi', 'ld', 're', 'n,', ' a', ' b', 'oy', ' a', 'nd', ' a', ' g', 'ir', 'l,', '\\nt', 'he', 'y ', 'we', 're', ' b', 'ot', 'h ', 'yo', 'un', 'g,', ' a', 'nd', ' c', 'ou', 'ld']\n",
      "\n",
      "processing file stories\\070.txt\n",
      "Data size (chars) (document 69) 5664\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' q', 'ue', 'en', ' w', 'ho', ' h', 'ad', ' a', ' l', 'it', 'tl', 'e ', 'da', 'ug', 'ht', 'er', ' w', 'ho', '\\nw', 'as', ' s', 'ti', 'll', ' s', 'o ', 'yo', 'un', 'g ', 'th', 'at', ' s', 'he', ' h', 'ad', ' t', 'o ']\n",
      "\n",
      "processing file stories\\071.txt\n",
      "Data size (chars) (document 70) 3569\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' p', 'oo', 'r ', 'pe', 'as', 'an', 't ', 'wh', 'o ', 'ha', 'd ', 'no', ' l', 'an', 'd,', ' b', 'ut', ' o', 'nl', 'y ', 'a ', 'sm', 'al', 'l\\n', 'ho', 'us', 'e,', ' a', 'nd', ' o', 'ne', ' d', 'au', 'gh', 'te', 'r.', '  ', 'th', 'en', ' s', 'ai', 'd ']\n",
      "\n",
      "processing file stories\\072.txt\n",
      "Data size (chars) (document 71) 3793\n",
      "Sample string ['ab', 'ou', 't ', 'a ', 'th', 'ou', 'sa', 'nd', ' o', 'r ', 'mo', 're', ' y', 'ea', 'rs', ' a', 'go', ', ', 'th', 'er', 'e ', 'we', 're', ' i', 'n ', 'th', 'is', '\\nc', 'ou', 'nt', 'ry', ' n', 'ot', 'hi', 'ng', ' b', 'ut', ' s', 'ma', 'll', ' k', 'in', 'gs', ', ', 'an', 'd ', 'on', 'e ', 'of', ' t']\n",
      "\n",
      "processing file stories\\073.txt\n",
      "Data size (chars) (document 72) 5980\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' k', 'in', 'g ', 'wh', 'o ', 'ha', 'd ', 'an', ' i', 'll', 'ne', 'ss', ', ', 'an', 'd ', 'no', ' o', 'ne', ' b', 'el', 'ie', 've', 'd ', 'th', 'at', ' h', 'e\\n', 'wo', 'ul', 'd ', 'co', 'me', ' o', 'ut', ' o', 'f ', 'it', ' w', 'it', 'h ', 'hi', 's ']\n",
      "\n",
      "processing file stories\\074.txt\n",
      "Data size (chars) (document 73) 4518\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' p', 'oo', 'r ', 'wo', 'od', 'cu', 'tt', 'er', ' w', 'ho', ' t', 'oi', 'le', 'd ', 'fr', 'om', ' e', 'ar', 'ly', '\\nm', 'or', 'ni', 'ng', ' t', 'il', 'l ', 'la', 'te', ' a', 't ', 'ni', 'gh', 't.', '  ', 'wh', 'en', ' a', 't ', 'la', 'st', ' h', 'e ']\n",
      "\n",
      "processing file stories\\075.txt\n",
      "Data size (chars) (document 74) 3247\n",
      "Sample string ['a ', 'di', 'sc', 'ha', 'rg', 'ed', ' s', 'ol', 'di', 'er', ' h', 'ad', ' n', 'ot', 'hi', 'ng', ' t', 'o ', 'li', 've', ' o', 'n,', ' a', 'nd', ' d', 'id', ' n', 'ot', ' k', 'no', 'w ', 'ho', 'w ', 'to', '\\nm', 'ak', 'e ', 'hi', 's ', 'wa', 'y.', '  ', 'so', ' h', 'e ', 'we', 'nt', ' o', 'ut', ' i']\n",
      "\n",
      "processing file stories\\076.txt\n",
      "Data size (chars) (document 75) 5130\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' y', 'ou', 'ng', ' f', 'el', 'lo', 'w ', 'wh', 'o ', 'en', 'li', 'st', 'ed', ' a', 's ', 'a ', 'so', 'ld', 'ie', 'r,', ' c', 'on', 'du', 'ct', 'ed', '\\nh', 'im', 'se', 'lf', ' b', 'ra', 've', 'ly', ', ', 'an', 'd ', 'wa', 's ', 'al', 'wa', 'ys', ' t']\n",
      "\n",
      "processing file stories\\077.txt\n",
      "Data size (chars) (document 76) 2401\n",
      "Sample string ['on', 'ce', ' i', 'n ', 'su', 'mm', 'er', '-t', 'im', 'e ', 'th', 'e ', 'be', 'ar', ' a', 'nd', ' t', 'he', ' w', 'ol', 'f ', 'we', 're', ' w', 'al', 'ki', 'ng', ' i', 'n ', 'th', 'e ', 'fo', 're', 'st', ',\\n', 'an', 'd ', 'th', 'e ', 'be', 'ar', ' h', 'ea', 'rd', ' a', ' b', 'ir', 'd ', 'si', 'ng']\n",
      "\n",
      "processing file stories\\078.txt\n",
      "Data size (chars) (document 77) 624\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'a ', 'po', 'or', ' b', 'ut', ' g', 'oo', 'd ', 'li', 'tt', 'le', ' g', 'ir', 'l ', 'wh', 'o ', 'li', 've', 'd ', 'al', 'on', 'e ', 'wi', 'th', ' h', 'er', '\\nm', 'ot', 'he', 'r,', ' a', 'nd', ' t', 'he', 'y ', 'no', ' l', 'on', 'ge', 'r ', 'ha', 'd ', 'an', 'yt', 'hi']\n",
      "\n",
      "processing file stories\\079.txt\n",
      "Data size (chars) (document 78) 3991\n",
      "Sample string ['on', 'e ', 'da', 'y ', 'a ', 'pe', 'as', 'an', 't ', 'to', 'ok', ' h', 'is', ' g', 'oo', 'd ', 'ha', 'ze', 'l-', 'st', 'ic', 'k ', 'ou', 't ', 'of', ' t', 'he', ' c', 'or', 'ne', 'r\\n', 'an', 'd ', 'sa', 'id', ' t', 'o ', 'hi', 's ', 'wi', 'fe', ', ', 'tr', 'in', 'a,', ' i', ' a', 'm ', 'go', 'in']\n",
      "\n",
      "processing file stories\\080.txt\n",
      "Data size (chars) (document 79) 1426\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' l', 'it', 'tl', 'e ', 'ch', 'il', 'd ', 'wh', 'os', 'e ', 'mo', 'th', 'er', ' g', 'av', 'e ', 'he', 'r ', 'ev', 'er', 'y\\n', 'af', 'te', 'rn', 'oo', 'n ', 'a ', 'sm', 'al', 'l ', 'bo', 'wl', ' o', 'f ', 'mi', 'lk', ' a', 'nd', ' b', 're', 'ad', ', ']\n",
      "\n",
      "processing file stories\\081.txt\n",
      "Data size (chars) (document 80) 3574\n",
      "Sample string ['in', ' a', ' c', 'er', 'ta', 'in', ' m', 'il', 'l ', 'li', 've', 'd ', 'an', ' o', 'ld', ' m', 'il', 'le', 'r ', 'wh', 'o ', 'ha', 'd ', 'ne', 'it', 'he', 'r ', 'wi', 'fe', ' n', 'or', ' c', 'hi', 'ld', ',\\n', 'an', 'd ', 'th', 're', 'e ', 'ap', 'pr', 'en', 'ti', 'ce', 's ', 'se', 'rv', 'ed', ' u']\n",
      "\n",
      "processing file stories\\082.txt\n",
      "Data size (chars) (document 81) 10822\n",
      "Sample string ['hi', 'll', ' a', 'nd', ' v', 'al', 'e ', 'do', ' n', 'ot', ' m', 'ee', 't,', ' b', 'ut', ' t', 'he', ' c', 'hi', 'ld', 're', 'n ', 'of', ' m', 'en', ' d', 'o,', ' g', 'oo', 'd ', 'an', 'd ', 'ba', 'd.', '\\ni', 'n ', 'th', 'is', ' w', 'ay', ' a', ' s', 'ho', 'em', 'ak', 'er', ' a', 'nd', ' a', ' t']\n",
      "\n",
      "processing file stories\\083.txt\n",
      "Data size (chars) (document 82) 5480\n",
      "Sample string ['\\th', 'an', 's ', 'th', 'e ', 'he', 'dg', 'eh', 'og', '\\n\\n', 'th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' c', 'ou', 'nt', 'ry', ' m', 'an', ' w', 'ho', ' h', 'ad', ' m', 'on', 'ey', ' a', 'nd', ' l', 'an', 'd ', 'in', ' p', 'le', 'nt', 'y,', ' b', 'ut', '\\nh', 'ow', 'ev', 'er', ' r', 'ic', 'h ']\n",
      "\n",
      "processing file stories\\084.txt\n",
      "Data size (chars) (document 83) 658\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' m', 'ot', 'he', 'r ', 'wh', 'o ', 'ha', 'd ', 'a ', 'li', 'tt', 'le', ' b', 'oy', ' o', 'f ', 'se', 've', 'n ', 'ye', 'ar', 's ', 'ol', 'd,', ' w', 'ho', '\\nw', 'as', ' s', 'o ', 'ha', 'nd', 'so', 'me', ' a', 'nd', ' l', 'ov', 'ab', 'le', ' t', 'ha']\n",
      "\n",
      "processing file stories\\085.txt\n",
      "Data size (chars) (document 84) 5989\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' y', 'ou', 'ng', ' f', 'el', 'lo', 'w ', 'wh', 'o ', 'ha', 'd ', 'le', 'ar', 'nt', ' t', 'he', ' t', 'ra', 'de', ' o', 'f ', 'lo', 'ck', 'sm', 'it', 'h,', '\\na', 'nd', ' t', 'ol', 'd ', 'hi', 's ', 'fa', 'th', 'er', ' h', 'e ', 'wo', 'ul', 'd ', 'no']\n",
      "\n",
      "processing file stories\\086.txt\n",
      "Data size (chars) (document 85) 8758\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' k', 'in', 'g ', 'wh', 'o ', 'ha', 'd ', 'a ', 'li', 'tt', 'le', ' b', 'oy', ' i', 'n ', 'wh', 'os', 'e ', 'st', 'ar', 's\\n', 'it', ' h', 'ad', ' b', 'ee', 'n ', 'fo', 're', 'to', 'ld', ' t', 'ha', 't ', 'he', ' s']\n",
      "\n",
      "processing file stories\\087.txt\n",
      "Data size (chars) (document 86) 3109\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' p', 'ri', 'nc', 'es', 's ', 'wh', 'o ', 'wa', 's ', 'ex', 'tr', 'em', 'el', 'y ', 'pr', 'ou', 'd.', ' i', 'f ', 'a\\n', 'wo', 'oe', 'r ', 'ca', 'me', ' s', 'he', ' g', 'av', 'e ', 'hi', 'm ', 'so', 'me', ' r', 'id']\n",
      "\n",
      "processing file stories\\088.txt\n",
      "Data size (chars) (document 87) 1365\n",
      "Sample string ['a ', 'ta', 'il', 'or', \"'s\", ' a', 'pp', 're', 'nt', 'ic', 'e ', 'wa', 's ', 'tr', 'av', 'el', 'in', 'g ', 'ab', 'ou', 't ', 'th', 'e ', 'wo', 'rl', 'd ', 'in', ' s', 'ea', 'rc', 'h ', 'of', '\\nw', 'or', 'k,', ' a', 'nd', ' a', 't ', 'on', 'e ', 'ti', 'me', ' h', 'e ', 'co', 'ul', 'd ', 'fi', 'nd']\n",
      "\n",
      "processing file stories\\089.txt\n",
      "Data size (chars) (document 88) 4538\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' o', 'n ', 'a ', 'ti', 'me', ' a', ' s', 'ol', 'di', 'er', ' w', 'ho', ' f', 'or', ' m', 'an', 'y ', 'ye', 'ar', 's ', 'ha', 'd ', 'se', 'rv', 'ed', ' t', 'he', '\\nk', 'in', 'g ', 'fa', 'it', 'hf', 'ul', 'ly', ', ', 'bu', 't ', 'wh', 'en', ' t', 'he', ' w']\n",
      "\n",
      "processing file stories\\090.txt\n",
      "Data size (chars) (document 89) 345\n",
      "Sample string ['on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' t', 'he', 're', ' w', 'as', ' a', ' c', 'hi', 'ld', ' w', 'ho', ' w', 'as', ' w', 'il', 'lf', 'ul', ', ', 'an', 'd ', 'wo', 'ul', 'd ', 'no', 't ', 'do', '\\nw', 'ha', 't ', 'he', 'r ', 'mo', 'th', 'er', ' w', 'is', 'he', 'd.', '  ', 'fo', 'r ', 'th']\n",
      "\n",
      "processing file stories\\091.txt\n",
      "Data size (chars) (document 90) 5460\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' k', 'in', \"g'\", 's ', 'so', 'n,', ' w', 'ho', ' w', 'as', ' n', 'o ', 'lo', 'ng', 'er', ' c', 'on', 'te', 'nt', ' t', 'o ', 'st', 'ay', ' a', 't\\n', 'ho', 'me', ' i', 'n ', 'hi', 's ', 'fa', 'th', 'er', \"'s\", ' h', 'ou', 'se', ', ', 'an', 'd ', 'as']\n",
      "\n",
      "processing file stories\\092.txt\n",
      "Data size (chars) (document 91) 6854\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' y', 'ou', 'ng', ' h', 'un', 'ts', 'ma', 'n ', 'wh', 'o ', 'we', 'nt', ' i', 'nt', 'o ', 'th', 'e ', 'fo', 're', 'st', ' t', 'o ', 'li', 'e ', 'in', '\\nw', 'ai', 't.', '  ', 'he', ' h', 'ad', ' a', ' f', 're', 'sh', ' a', 'nd', ' j', 'oy', 'ou', 's ']\n",
      "\n",
      "processing file stories\\093.txt\n",
      "Data size (chars) (document 92) 2314\n",
      "Sample string ['a ', 'po', 'or', ' s', 'er', 'va', 'nt', '-g', 'ir', 'l ', 'wa', 's ', 'on', 'ce', ' t', 'ra', 've', 'li', 'ng', ' w', 'it', 'h ', 'th', 'e ', 'fa', 'mi', 'ly', ' w', 'it', 'h ', 'wh', 'ic', 'h ', 'sh', 'e\\n', 'wa', 's ', 'in', ' s', 'er', 'vi', 'ce', ', ', 'th', 'ro', 'ug', 'h ', 'a ', 'gr', 'ea']\n",
      "\n",
      "processing file stories\\094.txt\n",
      "Data size (chars) (document 93) 1706\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' m', 'an', ' w', 'ho', ' h', 'ad', ' t', 'hr', 'ee', ' s', 'on', 's,', ' a', 'nd', ' n', 'ot', 'hi', 'ng', ' e', 'ls', 'e ', 'in', ' t', 'he', '\\nw', 'or', 'ld', ' b', 'ut', ' t', 'he', ' h', 'ou', 'se', ' i', 'n ', 'wh', 'ic', 'h ', 'he', ' l', 'iv']\n",
      "\n",
      "processing file stories\\095.txt\n",
      "Data size (chars) (document 94) 3229\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'a ', 'gr', 'ea', 't ', 'wa', 'r,', ' a', 'nd', ' t', 'he', ' k', 'in', 'g ', 'ha', 'd ', 'ma', 'ny', ' s', 'ol', 'di', 'er', 's,', ' b', 'ut', ' g', 'av', 'e ', 'th', 'em', '\\ns', 'ma', 'll', ' p', 'ay', ', ', 'so', ' s', 'ma', 'll', ' t', 'ha', 't ', 'th', 'ey', ' c']\n",
      "\n",
      "processing file stories\\096.txt\n",
      "Data size (chars) (document 95) 4954\n",
      "Sample string ['on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' l', 'iv', 'ed', ' a', ' m', 'an', ' a', 'nd', ' a', ' w', 'om', 'an', ' w', 'ho', ' s', 'o ', 'lo', 'ng', ' a', 's ', 'th', 'ey', ' w', 'er', 'e\\n', 'ri', 'ch', ' h', 'ad', ' n', 'o ', 'ch', 'il', 'dr', 'en', ', ', 'bu', 't ', 'wh', 'en', ' t', 'he']\n",
      "\n",
      "processing file stories\\097.txt\n",
      "Data size (chars) (document 96) 5732\n",
      "Sample string ['in', ' t', 'he', ' d', 'ay', 's ', 'wh', 'en', ' w', 'is', 'hi', 'ng', ' w', 'as', ' s', 'ti', 'll', ' o', 'f ', 'so', 'me', ' u', 'se', ', ', 'a ', 'ki', 'ng', \"'s\", ' s', 'on', ' w', 'as', '\\nb', 'ew', 'it', 'ch', 'ed', ' b', 'y ', 'an', ' o', 'ld', ' w', 'it', 'ch', ', ', 'an', 'd ', 'sh', 'ut']\n",
      "\n",
      "processing file stories\\098.txt\n",
      "Data size (chars) (document 97) 4334\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' p', 'oo', 'r ', 'ma', 'n ', 'wh', 'o ', 'ha', 'd ', 'fo', 'ur', ' s', 'on', 's,', ' a', 'nd', ' w', 'he', 'n ', 'th', 'ey', ' w', 'er', 'e ', 'gr', 'ow', 'n\\n', 'up', ', ', 'he', ' s', 'ai', 'd ', 'to', ' t', 'he', 'm,', ' \"', 'my', ' d', 'ea', 'r ']\n",
      "\n",
      "processing file stories\\099.txt\n",
      "Data size (chars) (document 98) 7090\n",
      "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' w', 'om', 'an', ' w', 'ho', ' h', 'ad', ' t', 'hr', 'ee', ' d', 'au', 'gh', 'te', 'rs', ', ', 'th', 'e ', 'el', 'de', 'st', ' o', 'f ', 'wh', 'om', '\\nw', 'as', ' c', 'al', 'le', 'd ', 'on', 'e-', 'ey', 'e,', ' b', 'ec', 'au', 'se', ' s', 'he', ' h']\n",
      "\n",
      "processing file stories\\100.txt\n",
      "Data size (chars) (document 99) 1007\n",
      "Sample string ['\"g', 'oo', 'd-', 'da', 'y,', ' f', 'at', 'he', 'r ', 'ho', 'll', 'en', 'th', 'e.', '\" ', '\"m', 'an', 'y ', 'th', 'an', 'ks', ', ', 'pi', 'f-', 'pa', 'f-', 'po', 'lt', 'ri', 'e.', '\" ', '\"m', 'ay', ' i', '\\nb', 'e ', 'al', 'lo', 'we', 'd ', 'to', ' h', 'av', 'e ', 'yo', 'ur', ' d', 'au', 'gh', 'te']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def read_data(filename):\n",
    "    with open(filename) as f:\n",
    "        data =  tf.compat.as_str(f.read())\n",
    "        data = data.lower()\n",
    "        data = list(data)\n",
    "    return data\n",
    "\n",
    "global documents\n",
    "documents = []\n",
    "num_files = 100\n",
    "for i in range(num_files):\n",
    "    print(\"processing file %s\" % os.path.join(dir_name, filenames[i]))\n",
    "    chars = read_data(os.path.join(dir_name, filenames[i]))\n",
    "\n",
    "    # break into bigrams\n",
    "    two_grams = [''.join(chars[ch_i:ch_i+2]) for ch_i in range(0, len(chars)-2, 2)]\n",
    "    # Create document\n",
    "    documents.append(two_grams)\n",
    "    print(\"Data size (chars) (document %d) %d\" % (i, len(two_grams)))\n",
    "    print(\"Sample string %s\\n\" % (two_grams[:50]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "449177 character found.\n",
      "Most common words (+UNK) [('e ', 15229), ('he', 15164), (' t', 13443), ('th', 13076), ('d ', 10687)]\n",
      "Least common words (+UNK) [('bj', 1), ('ii', 1), ('i?', 1), ('z ', 1), ('c.', 1), ('\"k', 1), ('pw', 1), ('f?', 1), (' z', 1), ('xq', 1), ('nm', 1), ('m?', 1), ('\\t\"', 1), ('\\tw', 1), ('tz', 1)]\n",
      "Sample data [15, 28, 86, 23, 3, 95, 74, 11, 2, 16]\n",
      "Sample data [22, 156, 25, 37, 82, 185, 43, 9, 90, 19]\n",
      "Vocabulary:  544\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build dictionaries\n",
    "# dictionary: maps a string word to an ID (e.g. {I:0, like:1, to:2, go:3, school:4})\n",
    "# reverse_dictionary: maps an ID to a string word (e.g. {0:I, 1:like, 2:to, 3:go, 4:school}\n",
    "# count: List of list of (word, frequency) elements (e.g. [(I,1),(like,1),(to,2),(go,1),(school,1)]\n",
    "# data : Contain the string of text we read, where string words are replaced with word IDs (e.g. [0, 1, 2, 3, 2, 4])\n",
    "\n",
    "def build_dataset(documents):\n",
    "    chars = []\n",
    "    # list of lists\n",
    "    data_list = []\n",
    "\n",
    "    for d in documents:\n",
    "        chars.extend(d)\n",
    "    print('%d character found.' % len(chars))\n",
    "\n",
    "    count = []\n",
    "    # bigrams sorted by their frequency\n",
    "    count.extend(collections.Counter(chars).most_common())\n",
    "\n",
    "    # Create dict map word to id by given the current length of the dictionary\n",
    "    # UNK is for two rare word\n",
    "    dictionary = dict({'UNK': 0})\n",
    "    for char, c in count:\n",
    "        # Only add if its frequency is more than 10\n",
    "        if c > 10:\n",
    "            dictionary[char] = len(dictionary)\n",
    "    unk_count = 0\n",
    "    # replace word with id of word\n",
    "    for d in documents:\n",
    "        data = list()\n",
    "        for char in d:\n",
    "            # if word in dictionary use the id of word\n",
    "            # otherwise use id of UNK\n",
    "            if char in dictionary:\n",
    "                index = dictionary[char]\n",
    "            else:\n",
    "                index = dictionary['UNK']\n",
    "                unk_count += 1\n",
    "            data.append(index)\n",
    "        data_list.append(data)\n",
    "\n",
    "    # dict map id to word\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data_list, count, dictionary, reverse_dictionary\n",
    "\n",
    "data_list, count, dictionary, reverse_dictionary = build_dataset(documents)\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Least common words (+UNK)', count[-15:])\n",
    "print('Sample data', data_list[0][:10])\n",
    "print('Sample data', data_list[1][:10])\n",
    "print('Vocabulary: ',len(dictionary))\n",
    "vocabulary_size = len(dictionary)\n",
    "del documents  # To reduce memory."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "'e '"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2char = np.array([i for i in dictionary.keys()])\n",
    "idx2char[1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Unrolled index 0\n",
      "\tInputs:\n",
      "\te  (1), \tki (131), \t d (48), \t w (11), \tbe (70), \n",
      "\tOutput:\n",
      "\tli (98), \tng (33), \tau (195), \ter (14), \tau (195), \n",
      "\n",
      "Unrolled index 1\n",
      "\tInputs:\n",
      "\tli (98), \tng (33), \tau (195), \ter (14), \tau (195), \n",
      "\tOutput:\n",
      "\tve (41), \t\n",
      "w (169), \tgh (106), \te  (1), \tti (112), \n",
      "\n",
      "Unrolled index 2\n",
      "\tInputs:\n",
      "\tve (41), \t\n",
      "w (169), \tgh (106), \te  (1), \tti (112), \n",
      "\tOutput:\n",
      "\td  (5), \tho (62), \tte (61), \tal (84), \tfu (228), \n",
      "\n",
      "Unrolled index 3\n",
      "\tInputs:\n",
      "\td  (5), \tho (62), \tte (61), \tal (84), \tfu (228), \n",
      "\tOutput:\n",
      "\ta  (82), \tse (58), \trs (137), \tl  (57), \tl, (257), \n",
      "\n",
      "Unrolled index 4\n",
      "\tInputs:\n",
      "\ta  (82), \tse (58), \trs (137), \tl  (57), \tbe (70), \n",
      "\tOutput:\n",
      "\tki (131), \t d (48), \t w (11), \tbe (70), \tau (195), "
     ]
    }
   ],
   "source": [
    "\n",
    "class DataGeneratorOHE(object):\n",
    "    def __init__(self, text, batch_size, num_unroll):\n",
    "        # text bigrams by its id\n",
    "        self._text = text\n",
    "        # number of bigrams in text\n",
    "        self._text_size = len(self._text)\n",
    "        self._batch_size = batch_size\n",
    "        # Num unroll is the number of steps unroll the RNN\n",
    "        # in a single training step\n",
    "        self._num_unroll = num_unroll\n",
    "        # Break text into several segments and the batch data\n",
    "        # is sampled by sampling a single item from a single segment\n",
    "        self._segments = self._text_size // self._batch_size\n",
    "        self._cursor = [offset * self._segments for offset in range(self._batch_size)]\n",
    "\n",
    "    def next_batch(self):\n",
    "        \"\"\"\n",
    "\n",
    "        :return: next batch of data\n",
    "        \"\"\"\n",
    "        # train inputs (one hot encoded) and train outputs (one hot encoded)\n",
    "        batch_data = np.zeros((self._batch_size, vocabulary_size,), dtype=np.float32)\n",
    "        batch_label = np.zeros((self._batch_size, vocabulary_size), dtype=np.float32)\n",
    "\n",
    "        for b in range(self._batch_size):\n",
    "            # reset back to begin when exceed batch_size\n",
    "            if self._cursor[b] + 1 >= self._text_size:\n",
    "                self._cursor[b] = b * self._segments\n",
    "\n",
    "            # Add text at cursor as input\n",
    "            batch_data[b, self._text[self._cursor[b]]] = 1.0\n",
    "\n",
    "            # Add preceding bigrams as the label\n",
    "            batch_label[b, self._text[self._cursor[b] + 1]] = 1.0\n",
    "\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "\n",
    "        return batch_data, batch_label\n",
    "\n",
    "    def unroll_batches(self):\n",
    "        \"\"\"\n",
    "\n",
    "        :return: a list of num_unroll batches required by training of the RNN\n",
    "        \"\"\"\n",
    "        unroll_data, unroll_labels = [], []\n",
    "        for ui in range(self._num_unroll):\n",
    "            data, labels = self.next_batch()\n",
    "            unroll_data.append(data)\n",
    "            unroll_labels.append(labels)\n",
    "        return unroll_data, unroll_labels\n",
    "\n",
    "    def reset_indices(self):\n",
    "        \"\"\"\n",
    "        Reset indices\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self._cursor = [offset * self._segments for offset in range(self._batch_size)]\n",
    "\n",
    "data_gen = DataGeneratorOHE(data_list[0][25:50], 5, 5)\n",
    "u_data_unroll, u_label_unroll = data_gen.unroll_batches()\n",
    "\n",
    "for ui, (data, label) in enumerate(zip(u_data_unroll, u_label_unroll)):\n",
    "    print('\\n\\nUnrolled index %d'%ui)\n",
    "    dat_ind = np.argmax(data,axis=1)\n",
    "    lbl_ind = np.argmax(label,axis=1)\n",
    "    print('\\tInputs:')\n",
    "    for sing_dat in dat_ind:\n",
    "        print('\\t%s (%d)'%(reverse_dictionary[sing_dat],sing_dat),end=\", \")\n",
    "    print('\\n\\tOutput:')\n",
    "    for sing_lbl in lbl_ind:\n",
    "        print('\\t%s (%d)'%(reverse_dictionary[sing_lbl],sing_lbl),end=\", \")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           139264    \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (64, None, 1024)          3938304   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 544)           557600    \n",
      "=================================================================\n",
      "Total params: 4,635,168\n",
      "Trainable params: 4,635,168\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 256\n",
    "rnn_units = 1024\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=(batch_size, None)),\n",
    "        tf.keras.layers.GRU(units=rnn_units,\n",
    "                            return_sequences=True,\n",
    "                            stateful=True,\n",
    "                            recurrent_initializer='glorot_uniform'),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "\n",
    "model = build_model(vocabulary_size, embedding_dim, rnn_units, BATCH_SIZE)\n",
    "model.summary()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "data": {
      "text/plain": "<TensorSliceDataset shapes: (), types: tf.int32>"
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_list)\n",
    "x = []\n",
    "for i in data_list:\n",
    "    x.extend(i)\n",
    "    # print(len(x))\n",
    "dataset = tf.data.Dataset.from_tensor_slices(x)\n",
    "dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'in olden times when wishing still helped one, there lived a king\\nwhose daughters were all beautiful, but the youngest was so beautiful\\nthat the sun itself, which has seen so much, was astonished wheneve'\n",
      "\"r\\nit shone in her face.  close by the king's castle lay a great dark\\nforest, and under an old lime-tree in the forest was a well, and when\\nthe day was very warm, the king's child went out into the fores\"\n",
      "'t and\\nsat down by the side of the cool fountain, and when she was bored she\\ntook a golden ball, and threw it up on high and caught it, and this\\nball was her favorite plaything.\\n\\nnow it so happened that '\n",
      "\"on one occasion the princess's golden ball\\ndid not fall into the little hand which she was holding up for it,\\nbut on to the ground beyond, and rolled straight into the water.  the\\nking's daughter follow\"\n",
      "'ed it with her eyes, but it vanished, and the\\nwell was deep, so deep that the bottom could not be seen.  at this\\nshe began to cry, and cried louder and louder, and could not be\\ncomforted.  and as she th'\n"
     ]
    }
   ],
   "source": [
    "seq_length = 100\n",
    "sequences = dataset.batch(seq_length + 1, drop_remainder=True)\n",
    "\n",
    "# for item in sequences.take(5):\n",
    "#     for i in item.numpy():\n",
    "#         print(reverse_dictionary[i], end='')\n",
    "#\n",
    "for item in sequences.take(5):\n",
    "    # print(item)\n",
    "    # print(idx2char[item.numpy()])\n",
    "    print(repr(''.join(idx2char[item.numpy()])))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "data": {
      "text/plain": "<MapDataset shapes: ((100,), (100,)), types: (tf.int32, tf.int32)>"
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map text to input and target (both input and target have\n",
    "# the same seq_length but target is shifted to right one character)\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1] # take all except the last character\n",
    "    target_text = chunk[1:] # take all except the first character\n",
    "\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)\n",
    "dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data 'in olden times when wishing still helped one, there lived a king\\nwhose daughters were all beautiful, but the youngest was so beautiful\\nthat the sun itself, which has seen so much, was astonished whene'\n",
      "Target data ' olden times when wishing still helped one, there lived a king\\nwhose daughters were all beautiful, but the youngest was so beautiful\\nthat the sun itself, which has seen so much, was astonished wheneve'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for input_exp, target_exp in dataset.take(1):\n",
    "    print('Input data', repr(''.join(idx2char[input_exp.numpy()])))\n",
    "    print(\"Target data\", repr(''.join(idx2char[target_exp.numpy()])))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [],
   "source": [
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 100\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True).prefetch(1)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [
    {
     "data": {
      "text/plain": "<TakeDataset shapes: ((64, 100), (64, 100)), types: (tf.int32, tf.int32)>"
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.take(1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 544) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    For each character the model looks up the\n",
    "    embedding, runs the GRU one timestep with\n",
    "    the embedding as input, and applies the dense\n",
    "    layer to generate logits predicting the log-likelihood of the next character:\n",
    "\"\"\"\n",
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [
    {
     "data": {
      "text/plain": "array([  1, 113, 226, 419, 212, 118,   3, 463, 397, 452, 122, 391, 333,\n       322, 185, 454, 422, 328, 519, 514, 402, 301, 410,  23, 211, 135,\n       181, 303, 151, 204, 339, 397, 345, 174, 468, 202, 256,  36, 233,\n       325, 467, 338, 250, 235, 364, 372, 239, 494, 208, 532, 379, 356,\n        61, 155, 180, 399, 405,  95,  90, 318, 306, 139, 497, 105,  32,\n       196, 384, 182, 120,  48, 279,  41, 239, 414,  73, 149, 449, 165,\n       463, 219, 253, 326, 320, 211, 239, 255,  31, 171,  65, 535,  41,\n       251, 522, 167, 160, 402, 144, 448, 184, 343], dtype=int64)"
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
    "sampled_indices"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      " '\\nawhile, and looked at everything in amazement, then she touched the\\nlight a little with her finger, and her finger became quite golden.\\nimmediately a great fear fell on her.  she shut the door violen'\n",
      "Next Char Predictions: \n",
      " 'e ce\\nbccapbu t-glpekdoyaglocgrp-xeoexamng-mbk\\nenr.d\\nag\\nplymisclph.fat-pr.\" fosl.bslt\\nio\\ngu.\\'cr\"hdrmr-mrgteraawggf.imfoiajuolnrkeitivn\\'od k dclvecr\"tilam\\'ibr-g\" uc\\n a\\nr.crspatigidt?veop-attdig-k ebt\\nud'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Input: \\n\", repr(''.join(idx2char[input_example_batch[0]])))\n",
    "print(\"Next Char Predictions: \\n\", repr(''.join(idx2char[sampled_indices])))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [
    {
     "data": {
      "text/plain": "6.2987657"
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def loss_sparse(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "example_batch_loss = loss_sparse(target_example_batch, example_batch_predictions)\n",
    "example_batch_loss.numpy().mean()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [],
   "source": [
    "\n",
    "# model.compile(optimizer='adam', loss=loss)\n",
    "#\n",
    "# checkpoint_dir = './training_checkpoint'\n",
    "#\n",
    "# checkpoint_predix = os.path.join(checkpoint_dir, 'ckpt_{epoch}')\n",
    "#\n",
    "# checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "#     filepath=checkpoint_predix,\n",
    "#     save_weights_only=True\n",
    "# )\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "@tf.function\n",
    "def train_step(inp, target):\n",
    "    with tf.GradientTape() as g:\n",
    "        predictions = model(inp)\n",
    "        loss = tf.reduce_mean(loss_sparse(target, predictions))\n",
    "\n",
    "    gradients = g.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 6.29873514175415\n",
      "Epoch 1 Loss 4.2748\n",
      "Time taken for 1 epoch 175.19048285484314 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 4.160902500152588\n",
      "Epoch 2 Loss 3.5232\n",
      "Time taken for 1 epoch 164.40199971199036 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 3.5004918575286865\n",
      "Epoch 3 Loss 3.2167\n",
      "Time taken for 1 epoch 165.2790002822876 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 3.1772332191467285\n",
      "Epoch 4 Loss 3.0115\n",
      "Time taken for 1 epoch 186.88235330581665 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 2.9823694229125977\n",
      "Epoch 5 Loss 2.8274\n",
      "Time taken for 1 epoch 213.66966199874878 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 2.8156843185424805\n",
      "Epoch 6 Loss 2.7111\n",
      "Time taken for 1 epoch 216.92969584465027 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 2.7132556438446045\n",
      "Epoch 7 Loss 2.5780\n",
      "Time taken for 1 epoch 214.7644681930542 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 2.560864210128784\n",
      "Epoch 8 Loss 2.4978\n",
      "Time taken for 1 epoch 215.11507606506348 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 2.4635009765625\n",
      "Epoch 9 Loss 2.3251\n",
      "Time taken for 1 epoch 214.786940574646 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 2.3294990062713623\n",
      "Epoch 10 Loss 2.2317\n",
      "Time taken for 1 epoch 216.3779468536377 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "EPOCHS = 10\n",
    "checkpoint_dir = './training_checkpoint'\n",
    "\n",
    "checkpoint_predix = os.path.join(checkpoint_dir, 'ckpt_{epoch}')\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_predix,\n",
    "    save_weights_only=True\n",
    ")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    # reset hidden state\n",
    "    model.reset_states()\n",
    "\n",
    "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
    "        loss = train_step(inp, target)\n",
    "\n",
    "        if batch_n % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {}'.format(epoch + 1, batch_n, loss))\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        model.save_weights(checkpoint_predix.format(epoch=epoch))\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1, loss))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "\n",
    "\n",
    "model.save_weights(checkpoint_predix.format(epoch=epoch))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15, 28, 86, 23, 3, 95]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 is incompatible with layer gru: expected shape=(64, None, 256), found shape=(1, 6, 256)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-117-ee9572656ef6>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     46\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mstart_string\u001B[0m \u001B[1;33m+\u001B[0m \u001B[1;34m''\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtext_generated\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     47\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 48\u001B[1;33m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mgenerate_text\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstart_string\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m\"In olden times\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     49\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     50\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-117-ee9572656ef6>\u001B[0m in \u001B[0;36mgenerate_text\u001B[1;34m(model, start_string)\u001B[0m\n\u001B[0;32m     32\u001B[0m     \u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreset_states\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     33\u001B[0m     \u001B[1;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnum_generate\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 34\u001B[1;33m         \u001B[0mpredictions\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput_eval\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     35\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     36\u001B[0m         \u001B[1;31m# remove batch dimension\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\nguyen trung tam\\pycharmprojects\\deeplearning001\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1010\u001B[0m         with autocast_variable.enable_auto_cast_variables(\n\u001B[0;32m   1011\u001B[0m             self._compute_dtype_object):\n\u001B[1;32m-> 1012\u001B[1;33m           \u001B[0moutputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcall_fn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1013\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1014\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_activity_regularizer\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\nguyen trung tam\\pycharmprojects\\deeplearning001\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py\u001B[0m in \u001B[0;36mcall\u001B[1;34m(self, inputs, training, mask)\u001B[0m\n\u001B[0;32m    373\u001B[0m       \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbuilt\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    374\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_init_graph_network\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moutputs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 375\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0msuper\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mSequential\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcall\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtraining\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtraining\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmask\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mmask\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    376\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    377\u001B[0m     \u001B[0moutputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0minputs\u001B[0m  \u001B[1;31m# handle the corner case where self.layers is empty\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\nguyen trung tam\\pycharmprojects\\deeplearning001\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py\u001B[0m in \u001B[0;36mcall\u001B[1;34m(self, inputs, training, mask)\u001B[0m\n\u001B[0;32m    423\u001B[0m     \"\"\"\n\u001B[0;32m    424\u001B[0m     return self._run_internal_graph(\n\u001B[1;32m--> 425\u001B[1;33m         inputs, training=training, mask=mask)\n\u001B[0m\u001B[0;32m    426\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    427\u001B[0m   \u001B[1;32mdef\u001B[0m \u001B[0mcompute_output_shape\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput_shape\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\nguyen trung tam\\pycharmprojects\\deeplearning001\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py\u001B[0m in \u001B[0;36m_run_internal_graph\u001B[1;34m(self, inputs, training, mask)\u001B[0m\n\u001B[0;32m    558\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    559\u001B[0m         \u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwargs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnode\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmap_arguments\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtensor_dict\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 560\u001B[1;33m         \u001B[0moutputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnode\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlayer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    561\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    562\u001B[0m         \u001B[1;31m# Update tensor_dict.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\nguyen trung tam\\pycharmprojects\\deeplearning001\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, inputs, initial_state, constants, **kwargs)\u001B[0m\n\u001B[0;32m    658\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    659\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0minitial_state\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0mconstants\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 660\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0msuper\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mRNN\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__call__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    661\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    662\u001B[0m     \u001B[1;31m# If any of `initial_state` or `constants` are specified and are Keras\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\nguyen trung tam\\pycharmprojects\\deeplearning001\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    996\u001B[0m         \u001B[0minputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_maybe_cast_inputs\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput_list\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    997\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 998\u001B[1;33m       \u001B[0minput_spec\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0massert_input_compatibility\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0minput_spec\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    999\u001B[0m       \u001B[1;32mif\u001B[0m \u001B[0meager\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1000\u001B[0m         \u001B[0mcall_fn\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcall\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\nguyen trung tam\\pycharmprojects\\deeplearning001\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py\u001B[0m in \u001B[0;36massert_input_compatibility\u001B[1;34m(input_spec, inputs, layer_name)\u001B[0m\n\u001B[0;32m    272\u001B[0m                              \u001B[1;34m' is incompatible with layer '\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mlayer_name\u001B[0m \u001B[1;33m+\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    273\u001B[0m                              \u001B[1;34m': expected shape='\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mspec\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m+\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 274\u001B[1;33m                              ', found shape=' + display_shape(x.shape))\n\u001B[0m\u001B[0;32m    275\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    276\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mValueError\u001B[0m: Input 0 is incompatible with layer gru: expected shape=(64, None, 256), found shape=(1, 6, 256)"
     ]
    }
   ],
   "source": [
    "\n",
    "def generate_text(model, start_string):\n",
    "    \"\"\"\n",
    "    Chose a start string, init RNN state and set the number\n",
    "    of characters to generate\n",
    "    Get the prediction distribution of next character using the start string and RNN state\n",
    "    Use categorical distribution to calculate the index of predicted character\n",
    "    and use this predicted character as our next input\n",
    "    The RNN state returned by the model is fed back into the model so that it now has more context,\n",
    "    After predicting the next character, the modified RNN states are again fed back into the model\n",
    "    :param model:\n",
    "    :param start_string:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    num_generate = 1000\n",
    "    start_string = start_string.lower()\n",
    "    # two_grams = [''.join(chars[ch_i:ch_i+2]) for ch_i in range(0, len(chars)-2, 2)]\n",
    "    two_grams = []\n",
    "    for ch_i in range(0, len(start_string) - 2, 2):\n",
    "        two_grams.append(start_string[ch_i: ch_i+2])\n",
    "\n",
    "    # fuck = [ch_i for ch_i in range(0, len(start_string)-2, 2) for s in start_string[ch_i:ch_i+2]]\n",
    "    input_eval = [dictionary[s] for s in two_grams]\n",
    "    print(input_eval)\n",
    "    input_eval = tf.expand_dims(input_eval, 0) # convert to 2d tensor\n",
    "\n",
    "    text_generated = []\n",
    "\n",
    "    # Love results in more predictable text\n",
    "    # High otherwise\n",
    "    temperature = 1.0\n",
    "\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "\n",
    "        # remove batch dimension\n",
    "        predictions = tf.squeeze(predictions, axis=0)\n",
    "\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
    "\n",
    "        input_eval = tf.expand_dims([predicted_id], axis=0)\n",
    "\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return start_string + ''.join(text_generated)\n",
    "\n",
    "print(generate_text(model, start_string=\"In olden times\"))\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}